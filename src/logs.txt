2023-04-09 11:13:37,939 - INFO - main.experiment - deep = 1 - plot = True - sigma0 = 0.01
2023-04-09 11:13:37,973 - INFO - training.pre_train_full - empirical mean of x0: 3.0000319984329904
2023-04-09 11:13:38,091 - INFO - training.pre_train_full - initial loss: 8.76840820812841
2023-04-09 11:13:38,152 - INFO - training.closure0 - iteration 0: loss = 8.76840820812841
2023-04-09 11:13:38,178 - INFO - training.closure0 - iteration 1: loss = 5.608629947275766
2023-04-09 11:13:38,207 - INFO - training.closure0 - iteration 2: loss = 5.049537917449689
2023-04-09 11:13:38,234 - INFO - training.closure0 - iteration 3: loss = 4.607388634027895
2023-04-09 11:13:38,261 - INFO - training.closure0 - iteration 4: loss = 4.2576727114774435
2023-04-09 11:13:38,287 - INFO - training.closure0 - iteration 5: loss = 3.468023233768825
2023-04-09 11:13:38,312 - INFO - training.closure0 - iteration 6: loss = 4.746411218818106
2023-04-09 11:13:38,338 - INFO - training.closure0 - iteration 7: loss = 2.9390796745854564
2023-04-09 11:13:38,366 - INFO - training.closure0 - iteration 8: loss = 2.218830113974835
2023-04-09 11:13:38,392 - INFO - training.closure0 - iteration 9: loss = 2819400.49726801
2023-04-09 11:13:38,418 - INFO - training.closure0 - iteration 10: loss = 13749731.730799846
2023-04-09 11:13:38,443 - INFO - training.closure0 - iteration 11: loss = 2.218829689616695
2023-04-09 11:13:38,468 - INFO - training.closure0 - iteration 12: loss = 8.170449725254121
2023-04-09 11:13:38,493 - INFO - training.closure0 - iteration 13: loss = 2.1753550159355326
2023-04-09 11:13:38,518 - INFO - training.closure0 - iteration 14: loss = 2.092339783138394
2023-04-09 11:13:38,543 - INFO - training.closure0 - iteration 15: loss = 1.8873238928196316
2023-04-09 11:13:38,569 - INFO - training.closure0 - iteration 16: loss = 2.1827467678949963
2023-04-09 11:13:38,604 - INFO - training.closure0 - iteration 17: loss = 1.5531869564444563
2023-04-09 11:13:38,639 - INFO - training.closure0 - iteration 18: loss = 1440.6707861594323
2023-04-09 11:13:38,666 - INFO - training.closure0 - iteration 19: loss = 6.58143827800852
2023-04-09 11:13:38,691 - INFO - training.closure0 - iteration 20: loss = 0.46604698879689216
2023-04-09 11:13:38,714 - INFO - training.closure0 - iteration 21: loss = 8626494.432249565
2023-04-09 11:13:38,741 - INFO - training.closure0 - iteration 22: loss = 39838.02512943948
2023-04-09 11:13:38,770 - INFO - training.closure0 - iteration 23: loss = 122.74764944944002
2023-04-09 11:13:38,801 - INFO - training.closure0 - iteration 24: loss = 6.123124832410575
2023-04-09 11:13:38,835 - INFO - training.closure0 - iteration 25: loss = -0.12549837982358775
2023-04-09 11:13:38,863 - INFO - training.closure0 - iteration 26: loss = 3163.0098079852187
2023-04-09 11:13:38,887 - INFO - training.closure0 - iteration 27: loss = 70.55654891408344
2023-04-09 11:13:38,910 - INFO - training.closure0 - iteration 28: loss = 1.7737240315755796
2023-04-09 11:13:38,938 - INFO - training.closure0 - iteration 29: loss = -0.17317135078081425
2023-04-09 11:13:38,965 - INFO - training.closure0 - iteration 30: loss = -0.18944325572946533
2023-04-09 11:13:38,993 - INFO - training.closure0 - iteration 31: loss = -0.17646787864498542
2023-04-09 11:13:39,021 - INFO - training.closure0 - iteration 32: loss = -0.20064523527247208
2023-04-09 11:13:39,049 - INFO - training.closure0 - iteration 33: loss = -0.21726659759924397
2023-04-09 11:13:39,080 - INFO - training.closure0 - iteration 34: loss = 0.3428700076093144
2023-04-09 11:13:39,106 - INFO - training.closure0 - iteration 35: loss = -0.22528177554376144
2023-04-09 11:13:39,133 - INFO - training.closure0 - iteration 36: loss = -0.2384992456913081
2023-04-09 11:13:39,160 - INFO - training.closure0 - iteration 37: loss = -0.2640875239454519
2023-04-09 11:13:39,188 - INFO - training.closure0 - iteration 38: loss = -0.21424660124842115
2023-04-09 11:13:39,212 - INFO - training.closure0 - iteration 39: loss = -0.27386112429875
2023-04-09 11:13:39,241 - INFO - training.closure0 - iteration 40: loss = -0.29364010842155097
2023-04-09 11:13:39,277 - INFO - training.closure0 - iteration 41: loss = -0.3161666847655583
2023-04-09 11:13:39,310 - INFO - training.closure0 - iteration 42: loss = -0.1447617851941895
2023-04-09 11:13:39,335 - INFO - training.closure0 - iteration 43: loss = -0.3375291945130803
2023-04-09 11:13:39,362 - INFO - training.closure0 - iteration 44: loss = -0.35026301864964404
2023-04-09 11:13:39,391 - INFO - training.closure0 - iteration 45: loss = -0.37850140817507927
2023-04-09 11:13:39,418 - INFO - training.closure0 - iteration 46: loss = 1.8425425715111512
2023-04-09 11:13:39,445 - INFO - training.closure0 - iteration 47: loss = -0.4047836478853355
2023-04-09 11:13:39,474 - INFO - training.closure0 - iteration 48: loss = -0.41251952309613893
2023-04-09 11:13:39,503 - INFO - training.closure0 - iteration 49: loss = -0.44465644933406
2023-04-09 11:13:39,530 - INFO - training.closure0 - iteration 50: loss = -0.1785625649015394
2023-04-09 11:13:39,556 - INFO - training.closure0 - iteration 51: loss = -0.48457668205146415
2023-04-09 11:13:39,582 - INFO - training.closure0 - iteration 52: loss = -0.5201619158096382
2023-04-09 11:13:39,607 - INFO - training.closure0 - iteration 53: loss = -0.6258938207622987
2023-04-09 11:13:39,635 - INFO - training.closure0 - iteration 54: loss = 324.56413425173616
2023-04-09 11:13:39,665 - INFO - training.closure0 - iteration 55: loss = 2.4744725721035445
2023-04-09 11:13:39,693 - INFO - training.closure0 - iteration 56: loss = -0.7903893282873259
2023-04-09 11:13:39,722 - INFO - training.closure0 - iteration 57: loss = 13.29631009291344
2023-04-09 11:13:39,749 - INFO - training.closure0 - iteration 58: loss = -0.7429140246799499
2023-04-09 11:13:39,775 - INFO - training.closure0 - iteration 59: loss = -0.8302827843291246
2023-04-09 11:13:39,803 - INFO - training.closure0 - iteration 60: loss = -0.8571811000170089
2023-04-09 11:13:39,831 - INFO - training.closure0 - iteration 61: loss = -0.8914029689624674
2023-04-09 11:13:39,861 - INFO - training.closure0 - iteration 62: loss = -0.9246573911812055
2023-04-09 11:13:39,893 - INFO - training.closure0 - iteration 63: loss = 4.125266835586784
2023-04-09 11:13:39,929 - INFO - training.closure0 - iteration 64: loss = -0.9046519087444536
2023-04-09 11:13:39,961 - INFO - training.closure0 - iteration 65: loss = -0.9627516286699134
2023-04-09 11:13:39,989 - INFO - training.closure0 - iteration 66: loss = -0.9615051817276389
2023-04-09 11:13:40,015 - INFO - training.closure0 - iteration 67: loss = -0.9674664544475375
2023-04-09 11:13:40,039 - INFO - training.closure0 - iteration 68: loss = -0.9875479812809113
2023-04-09 11:13:40,070 - INFO - training.closure0 - iteration 69: loss = -1.0143849087504
2023-04-09 11:13:40,094 - INFO - training.closure0 - iteration 70: loss = -1.0260179831234226
2023-04-09 11:13:40,133 - INFO - training.closure0 - iteration 71: loss = -1.0787971572361787
2023-04-09 11:13:40,163 - INFO - training.closure0 - iteration 72: loss = -1.0878400935800903
2023-04-09 11:13:40,195 - INFO - training.closure0 - iteration 73: loss = -1.017712803232079
2023-04-09 11:13:40,224 - INFO - training.closure0 - iteration 74: loss = -1.1161846761672178
2023-04-09 11:13:40,253 - INFO - training.closure0 - iteration 75: loss = -1.1185545031373658
2023-04-09 11:13:40,281 - INFO - training.closure0 - iteration 76: loss = -0.9001375835825701
2023-04-09 11:13:40,316 - INFO - training.closure0 - iteration 77: loss = -1.134310182790517
2023-04-09 11:13:40,343 - INFO - training.closure0 - iteration 78: loss = -1.1355533762112102
2023-04-09 11:13:40,372 - INFO - training.closure0 - iteration 79: loss = -1.1411853317115699
2023-04-09 11:13:40,400 - INFO - training.closure0 - iteration 80: loss = -1.0444797534426191
2023-04-09 11:13:40,425 - INFO - training.closure0 - iteration 81: loss = -1.1436033013621756
2023-04-09 11:13:40,454 - INFO - training.closure0 - iteration 82: loss = -1.1475215963278185
2023-04-09 11:13:40,482 - INFO - training.closure0 - iteration 83: loss = -1.1543892535758458
2023-04-09 11:13:40,509 - INFO - training.closure0 - iteration 84: loss = -1.1380630107733332
2023-04-09 11:13:40,537 - INFO - training.closure0 - iteration 85: loss = -1.1584527789008694
2023-04-09 11:13:40,590 - INFO - training.closure0 - iteration 86: loss = -1.164295694808767
2023-04-09 11:13:40,623 - INFO - training.closure0 - iteration 87: loss = -1.1724268237351452
2023-04-09 11:13:40,653 - INFO - training.closure0 - iteration 88: loss = 1.0566272803952383
2023-04-09 11:13:40,682 - INFO - training.closure0 - iteration 89: loss = -1.1480205814382995
2023-04-09 11:13:40,709 - INFO - training.closure0 - iteration 90: loss = -1.1784559562944976
2023-04-09 11:13:40,735 - INFO - training.closure0 - iteration 91: loss = -1.1820547316904972
2023-04-09 11:13:40,769 - INFO - training.closure0 - iteration 92: loss = -1.1974140152356565
2023-04-09 11:13:40,802 - INFO - training.closure0 - iteration 93: loss = -1.209799256449445
2023-04-09 11:13:40,830 - INFO - training.closure0 - iteration 94: loss = -0.934362927050671
2023-04-09 11:13:40,857 - INFO - training.closure0 - iteration 95: loss = -1.21282821710196
2023-04-09 11:13:40,884 - INFO - training.closure0 - iteration 96: loss = -1.214513586427141
2023-04-09 11:13:40,912 - INFO - training.closure0 - iteration 97: loss = -0.8900320800018975
2023-04-09 11:13:40,941 - INFO - training.closure0 - iteration 98: loss = -1.2259570953992545
2023-04-09 11:13:40,969 - INFO - training.closure0 - iteration 99: loss = -1.2520565944651796
2023-04-09 11:13:41,001 - INFO - training.closure0 - iteration 100: loss = -1.2732697830657873
2023-04-09 11:13:41,035 - INFO - training.closure0 - iteration 101: loss = -1.2769648825776647
2023-04-09 11:13:41,068 - INFO - training.closure0 - iteration 102: loss = -1.2601240884499367
2023-04-09 11:13:41,094 - INFO - training.closure0 - iteration 103: loss = -1.28106941291601
2023-04-09 11:13:41,126 - INFO - training.closure0 - iteration 104: loss = -1.3041251044852435
2023-04-09 11:13:41,160 - INFO - training.closure0 - iteration 105: loss = -1.3595174742402638
2023-04-09 11:13:41,190 - INFO - training.closure0 - iteration 106: loss = -1.4644657508090606
2023-04-09 11:13:41,220 - INFO - training.closure0 - iteration 107: loss = -1.685375598673481
2023-04-09 11:13:41,249 - INFO - training.closure0 - iteration 108: loss = 88506031.06177475
2023-04-09 11:13:41,279 - INFO - training.closure0 - iteration 109: loss = 1017648.1778532258
2023-04-09 11:13:41,307 - INFO - training.closure0 - iteration 110: loss = 15676.204728996183
2023-04-09 11:13:41,333 - INFO - training.closure0 - iteration 111: loss = 275.7718914339926
2023-04-09 11:13:41,359 - INFO - training.closure0 - iteration 112: loss = 3.784884618696771
2023-04-09 11:13:41,384 - INFO - training.closure0 - iteration 113: loss = -1.6760266464908664
2023-04-09 11:13:41,408 - INFO - training.closure0 - iteration 114: loss = -1.7644765457770033
2023-04-09 11:13:41,439 - INFO - training.closure0 - iteration 115: loss = -2.0246529561555424
2023-04-09 11:13:41,472 - INFO - training.closure0 - iteration 116: loss = -1.2472270159230665
2023-04-09 11:13:41,497 - INFO - training.closure0 - iteration 117: loss = -2.080156683701399
2023-04-09 11:13:41,525 - INFO - training.closure0 - iteration 118: loss = 0.6946707333334885
2023-04-09 11:13:41,552 - INFO - training.closure0 - iteration 119: loss = -2.2143660975959616
2023-04-09 11:13:41,576 - INFO - training.closure0 - iteration 120: loss = -2.2271545424827077
2023-04-09 11:13:41,604 - INFO - training.closure0 - iteration 121: loss = -1.2479000171404118
2023-04-09 11:13:41,640 - INFO - training.closure0 - iteration 122: loss = -2.239994807823434
2023-04-09 11:13:41,670 - INFO - training.closure0 - iteration 123: loss = -2.2495492028984754
2023-04-09 11:13:41,701 - INFO - training.closure0 - iteration 124: loss = -2.396978034017021
2023-04-09 11:13:41,731 - INFO - training.closure0 - iteration 125: loss = -2.444140993268172
2023-04-09 11:13:41,759 - INFO - training.closure0 - iteration 126: loss = -0.9853708501945904
2023-04-09 11:13:41,784 - INFO - training.closure0 - iteration 127: loss = -2.442058070513978
2023-04-09 11:13:41,808 - INFO - training.closure0 - iteration 128: loss = -2.454620970018545
2023-04-09 11:13:41,837 - INFO - training.closure0 - iteration 129: loss = -2.4600253712972258
2023-04-09 11:13:41,858 - INFO - training.closure0 - iteration 130: loss = -2.4630347608523118
2023-04-09 11:13:41,881 - INFO - training.closure0 - iteration 131: loss = -2.4711132547556636
2023-04-09 11:13:41,915 - INFO - training.closure0 - iteration 132: loss = -2.4761576403727936
2023-04-09 11:13:41,944 - INFO - training.closure0 - iteration 133: loss = -2.4834450314193672
2023-04-09 11:13:41,975 - INFO - training.closure0 - iteration 134: loss = -2.4891911445313735
2023-04-09 11:13:42,002 - INFO - training.closure0 - iteration 135: loss = -2.4896327314194417
2023-04-09 11:13:42,027 - INFO - training.closure0 - iteration 136: loss = -2.492092433468821
2023-04-09 11:13:42,050 - INFO - training.closure0 - iteration 137: loss = -2.498363733228749
2023-04-09 11:13:42,077 - INFO - training.closure0 - iteration 138: loss = -2.505443392206946
2023-04-09 11:13:42,100 - INFO - training.closure0 - iteration 139: loss = -2.514335493271295
2023-04-09 11:13:42,122 - INFO - training.closure0 - iteration 140: loss = -2.5253705832258575
2023-04-09 11:13:42,144 - INFO - training.closure0 - iteration 141: loss = -2.5809981551296595
2023-04-09 11:13:42,163 - INFO - training.closure0 - iteration 142: loss = -2.611628755620078
2023-04-09 11:13:42,185 - INFO - training.closure0 - iteration 143: loss = 3.732852540377662
2023-04-09 11:13:42,205 - INFO - training.closure0 - iteration 144: loss = -2.7424970728001012
2023-04-09 11:13:42,225 - INFO - training.closure0 - iteration 145: loss = -2.8720732508066993
2023-04-09 11:13:42,247 - INFO - training.closure0 - iteration 146: loss = 7.370070973122781
2023-04-09 11:13:42,266 - INFO - training.closure0 - iteration 147: loss = -0.7377048074990677
2023-04-09 11:13:42,286 - INFO - training.closure0 - iteration 148: loss = -2.9114614540427315
2023-04-09 11:13:42,314 - INFO - training.closure0 - iteration 149: loss = -2.9885960988868296
2023-04-09 11:13:42,335 - INFO - training.closure0 - iteration 150: loss = -3.183824066099093
2023-04-09 11:13:42,360 - INFO - training.closure0 - iteration 151: loss = -3.168953204215767
2023-04-09 11:13:42,381 - INFO - training.closure0 - iteration 152: loss = -3.198904950386282
2023-04-09 11:13:42,399 - INFO - training.closure0 - iteration 153: loss = -3.1828309934309003
2023-04-09 11:13:42,417 - INFO - training.closure0 - iteration 154: loss = -3.2019440546616025
2023-04-09 11:13:42,436 - INFO - training.closure0 - iteration 155: loss = -3.2038583921128527
2023-04-09 11:13:42,455 - INFO - training.closure0 - iteration 156: loss = -3.2047066888260396
2023-04-09 11:13:42,473 - INFO - training.closure0 - iteration 157: loss = -3.2081141098782338
2023-04-09 11:13:42,489 - INFO - training.closure0 - iteration 158: loss = -3.2157092213294325
2023-04-09 11:13:42,511 - INFO - training.closure0 - iteration 159: loss = -3.2390950600683714
2023-04-09 11:13:42,537 - INFO - training.closure0 - iteration 160: loss = -3.316162424130482
2023-04-09 11:13:42,560 - INFO - training.closure0 - iteration 161: loss = -0.8134754919446971
2023-04-09 11:13:42,583 - INFO - training.closure0 - iteration 162: loss = -3.391608187143466
2023-04-09 11:13:42,606 - INFO - training.closure0 - iteration 163: loss = -4.0400840137406995
2023-04-09 11:13:42,625 - INFO - training.closure0 - iteration 164: loss = -4.056444068595463
2023-04-09 11:13:42,644 - INFO - training.closure0 - iteration 165: loss = 90567190.9348777
2023-04-09 11:13:42,660 - INFO - training.closure0 - iteration 166: loss = -4.0564441248318905
2023-04-09 11:13:42,677 - INFO - training.closure0 - iteration 167: loss = 30.283175776306418
2023-04-09 11:13:42,693 - INFO - training.closure0 - iteration 168: loss = -3.260661280381518
2023-04-09 11:13:42,713 - INFO - training.closure0 - iteration 169: loss = -4.091354910370048
2023-04-09 11:13:42,734 - INFO - training.closure0 - iteration 170: loss = -4.267385763183722
2023-04-09 11:13:42,759 - INFO - training.closure0 - iteration 171: loss = -4.053578922551705
2023-04-09 11:13:42,779 - INFO - training.closure0 - iteration 172: loss = -4.471337424282829
2023-04-09 11:13:42,799 - INFO - training.closure0 - iteration 173: loss = -4.686589647891051
2023-04-09 11:13:42,819 - INFO - training.closure0 - iteration 174: loss = -4.725909232433915
2023-04-09 11:13:42,840 - INFO - training.closure0 - iteration 175: loss = 368.17019219599206
2023-04-09 11:13:42,860 - INFO - training.closure0 - iteration 176: loss = 6.345746073245852
2023-04-09 11:13:42,880 - INFO - training.closure0 - iteration 177: loss = -4.577272426358042
2023-04-09 11:13:42,899 - INFO - training.closure0 - iteration 178: loss = -4.757357732852402
2023-04-09 11:13:42,922 - INFO - training.closure0 - iteration 179: loss = -4.808750683573069
2023-04-09 11:13:42,950 - INFO - training.closure0 - iteration 180: loss = -4.814085843795475
2023-04-09 11:13:42,978 - INFO - training.closure0 - iteration 181: loss = -4.887401944651864
2023-04-09 11:13:43,006 - INFO - training.closure0 - iteration 182: loss = -5.076425410581129
2023-04-09 11:13:43,028 - INFO - training.closure0 - iteration 183: loss = -4.772523298596817
2023-04-09 11:13:43,049 - INFO - training.closure0 - iteration 184: loss = -5.209077790222402
2023-04-09 11:13:43,070 - INFO - training.closure0 - iteration 185: loss = -5.030399958805701
2023-04-09 11:13:43,089 - INFO - training.closure0 - iteration 186: loss = -5.287197890707814
2023-04-09 11:13:43,106 - INFO - training.closure0 - iteration 187: loss = -5.280671598340966
2023-04-09 11:13:43,124 - INFO - training.closure0 - iteration 188: loss = -5.295786643106657
2023-04-09 11:13:43,153 - INFO - training.closure0 - iteration 189: loss = -5.301992391151256
2023-04-09 11:13:43,176 - INFO - training.closure0 - iteration 190: loss = -5.321841342644242
2023-04-09 11:13:43,199 - INFO - training.closure0 - iteration 191: loss = -5.458645795581739
2023-04-09 11:13:43,219 - INFO - training.closure0 - iteration 192: loss = -5.541812303924408
2023-04-09 11:13:43,237 - INFO - training.closure0 - iteration 193: loss = -5.610808537910282
2023-04-09 11:13:43,258 - INFO - training.closure0 - iteration 194: loss = -5.698840725124456
2023-04-09 11:13:43,276 - INFO - training.closure0 - iteration 195: loss = -5.793011905632329
2023-04-09 11:13:43,294 - INFO - training.closure0 - iteration 196: loss = 188.64869231676596
2023-04-09 11:13:43,313 - INFO - training.closure0 - iteration 197: loss = -2.0178768308702524
2023-04-09 11:13:43,335 - INFO - training.closure0 - iteration 198: loss = -5.976519763271954
2023-04-09 11:13:43,356 - INFO - training.closure0 - iteration 199: loss = -6.0083963198048425
2023-04-09 11:13:43,383 - INFO - training.closure0 - iteration 200: loss = -6.0822333601515
2023-04-09 11:13:43,410 - INFO - training.closure0 - iteration 201: loss = -6.18931223403527
2023-04-09 11:13:43,435 - INFO - training.closure0 - iteration 202: loss = -6.321100749882381
2023-04-09 11:13:43,454 - INFO - training.closure0 - iteration 203: loss = -6.413859987731175
2023-04-09 11:13:43,472 - INFO - training.closure0 - iteration 204: loss = -6.396900065932975
2023-04-09 11:13:43,491 - INFO - training.closure0 - iteration 205: loss = -6.444414632257114
2023-04-09 11:13:43,513 - INFO - training.closure0 - iteration 206: loss = -6.446945970772212
2023-04-09 11:13:43,536 - INFO - training.closure0 - iteration 207: loss = -6.452894931851288
2023-04-09 11:13:43,568 - INFO - training.closure0 - iteration 208: loss = -6.456058058342654
2023-04-09 11:13:43,594 - INFO - training.closure0 - iteration 209: loss = -6.4568838621080715
2023-04-09 11:13:43,623 - INFO - training.closure0 - iteration 210: loss = -6.456907231185164
2023-04-09 11:13:43,648 - INFO - training.closure0 - iteration 211: loss = -6.456911640479307
2023-04-09 11:13:43,671 - INFO - training.closure0 - iteration 212: loss = -6.456911675365854
2023-04-09 11:13:43,691 - INFO - training.closure0 - iteration 213: loss = -6.456911675910607
2023-04-09 11:13:43,711 - INFO - training.closure0 - iteration 214: loss = -6.456911675911378
2023-04-09 11:13:43,721 - INFO - training.pre_train_full - a0 mean: [2.99944253 3.00062146]
2023-04-09 11:13:43,723 - INFO - training.pre_train_full - a0 var: [1.00409418e-04 8.44174487e-05]
2023-04-09 11:13:43,736 - INFO - training.pre_train_full - a0 covar: [[0.00010040941772195457, -5.508688032141373e-06], [-5.508688032141373e-06, 8.441744871557383e-05]]
2023-04-09 11:15:04,529 - INFO - training.closure - iteration 0: loss = 57053613.93347549
2023-04-09 11:15:07,525 - INFO - training.closure - iteration 1: loss = 11618739.69868468
2023-04-09 11:15:10,483 - INFO - training.closure - iteration 2: loss = 6938813.083233328
2023-04-09 11:15:12,629 - INFO - training.closure - iteration 3: loss = 2041610.3153740752
2023-04-09 11:15:17,211 - INFO - training.closure - iteration 4: loss = 979269.2477477221
2023-04-09 11:15:20,829 - INFO - training.closure - iteration 5: loss = 617468.23236647
2023-04-09 11:15:24,130 - INFO - training.closure - iteration 6: loss = 104321.48454139182
2023-04-09 11:15:26,571 - INFO - training.closure - iteration 7: loss = 35676.79958728365
2023-04-09 11:15:28,956 - INFO - training.closure - iteration 8: loss = 32666.928953839895
2023-04-09 11:15:31,383 - INFO - training.closure - iteration 9: loss = 21426.90842063445
2023-04-09 11:15:33,689 - INFO - training.closure - iteration 10: loss = 19514.965569014978
2023-04-09 11:15:36,118 - INFO - training.closure - iteration 11: loss = 16437.12938694589
2023-04-09 11:15:38,488 - INFO - training.closure - iteration 12: loss = 15855.934921295246
2023-04-09 11:15:40,914 - INFO - training.closure - iteration 13: loss = 15355.831297060833
2023-04-09 11:15:43,331 - INFO - training.closure - iteration 14: loss = 12861.160217687053
2023-04-09 11:15:45,771 - INFO - training.closure - iteration 15: loss = 10832.050678062978
2023-04-09 11:15:48,376 - INFO - training.closure - iteration 16: loss = 9498.535978282333
2023-04-09 11:15:50,804 - INFO - training.closure - iteration 17: loss = 8451.963498613248
2023-04-09 11:15:53,202 - INFO - training.closure - iteration 18: loss = 7285.584789518558
2023-04-09 11:15:55,588 - INFO - training.closure - iteration 19: loss = 4188.717026003065
2023-04-09 11:15:57,972 - INFO - training.closure - iteration 20: loss = 2947.5542057956495
2023-04-09 11:16:00,335 - INFO - training.closure - iteration 21: loss = 1529.4277476448706
2023-04-09 11:16:02,671 - INFO - training.closure - iteration 22: loss = 1288.466231422819
2023-04-09 11:16:05,100 - INFO - training.closure - iteration 23: loss = 349.13964027843343
2023-04-09 11:16:07,423 - INFO - training.closure - iteration 24: loss = 186.31646326428145
2023-04-09 11:16:09,807 - INFO - training.closure - iteration 25: loss = 80.34713220887903
2023-04-09 11:16:12,322 - INFO - training.closure - iteration 26: loss = 38.92111559666279
2023-04-09 11:16:14,723 - INFO - training.closure - iteration 27: loss = 18.872842958694157
2023-04-09 11:16:17,120 - INFO - training.closure - iteration 28: loss = 9.97986718804048
2023-04-09 11:16:19,489 - INFO - training.closure - iteration 29: loss = 6.038476208791694
2023-04-09 11:16:21,948 - INFO - training.closure - iteration 30: loss = 4.272901319304775
2023-04-09 11:16:24,318 - INFO - training.closure - iteration 31: loss = 3.402532421991573
2023-04-09 11:16:26,684 - INFO - training.closure - iteration 32: loss = 2.7982622042035086
2023-04-09 11:16:29,037 - INFO - training.closure - iteration 33: loss = 1.6638818200767709
2023-04-09 11:16:31,385 - INFO - training.closure - iteration 34: loss = 1.1345947200150066
2023-04-09 11:16:33,691 - INFO - training.closure - iteration 35: loss = 0.3715091001185691
2023-04-09 11:16:36,151 - INFO - training.closure - iteration 36: loss = 0.09762041359337115
2023-04-09 11:16:38,536 - INFO - training.closure - iteration 37: loss = -0.2870220546583546
2023-04-09 11:16:40,853 - INFO - training.closure - iteration 38: loss = -0.7229997892320599
2023-04-09 11:16:43,207 - INFO - training.closure - iteration 39: loss = -0.9610323785751609
2023-04-09 11:16:45,544 - INFO - training.closure - iteration 40: loss = -1.1202374387131009
2023-04-09 11:16:47,897 - INFO - training.closure - iteration 41: loss = -1.1734798052968882
2023-04-09 11:16:50,265 - INFO - training.closure - iteration 42: loss = -1.2817646007354853
2023-04-09 11:16:52,673 - INFO - training.closure - iteration 43: loss = -1.4486744012666168
2023-04-09 11:16:55,081 - INFO - training.closure - iteration 44: loss = -1.6581005789253433
2023-04-09 11:16:57,420 - INFO - training.closure - iteration 45: loss = -1.8112621300908023
2023-04-09 11:16:59,820 - INFO - training.closure - iteration 46: loss = -1.8659830002527036
2023-04-09 11:17:02,216 - INFO - training.closure - iteration 47: loss = -1.9372181325637294
2023-04-09 11:17:04,554 - INFO - training.closure - iteration 48: loss = -2.0794214212282114
2023-04-09 11:17:06,919 - INFO - training.closure - iteration 49: loss = -2.188416458076775
2023-04-09 11:17:09,256 - INFO - training.closure - iteration 50: loss = -2.2863446871205007
2023-04-09 11:17:11,576 - INFO - training.closure - iteration 51: loss = -2.354975994289365
2023-04-09 11:17:13,929 - INFO - training.closure - iteration 52: loss = -2.4150655900385227
2023-04-09 11:17:16,283 - INFO - training.closure - iteration 53: loss = -2.472785753658169
2023-04-09 11:17:18,667 - INFO - training.closure - iteration 54: loss = -2.8112281756088233
2023-04-09 11:17:21,036 - INFO - training.closure - iteration 55: loss = -2.3468883022144404
2023-04-09 11:17:23,529 - INFO - training.closure - iteration 56: loss = -3.1432199222917205
2023-04-09 11:17:25,882 - INFO - training.closure - iteration 57: loss = -3.7467185235205482
2023-04-09 11:17:28,267 - INFO - training.closure - iteration 58: loss = 4.1622106157248195
2023-04-09 11:17:30,585 - INFO - training.closure - iteration 59: loss = -3.853349172821525
2023-04-09 11:17:32,967 - INFO - training.closure - iteration 60: loss = -3.9655873140250693
2023-04-09 11:17:35,319 - INFO - training.closure - iteration 61: loss = 1.5681827773582535
2023-04-09 11:17:37,624 - INFO - training.closure - iteration 62: loss = -4.210986135666005
2023-04-09 11:17:39,946 - INFO - training.closure - iteration 63: loss = -4.411944464656498
2023-04-09 11:17:42,533 - INFO - training.closure - iteration 64: loss = -4.641357415404296
2023-04-09 11:17:45,169 - INFO - training.closure - iteration 65: loss = -4.756813182983993
2023-04-09 11:17:47,788 - INFO - training.closure - iteration 66: loss = -4.8682782295292375
2023-04-09 11:17:50,393 - INFO - training.closure - iteration 67: loss = -5.082027568672318
2023-04-09 11:17:53,124 - INFO - training.closure - iteration 68: loss = -5.2284629612193125
2023-04-09 11:17:59,257 - INFO - training.closure - iteration 69: loss = -5.451015628397458
2023-04-09 11:18:02,326 - INFO - training.closure - iteration 70: loss = -5.656185531825713
2023-04-09 11:18:04,977 - INFO - training.closure - iteration 71: loss = -5.094272739575121
2023-04-09 11:18:07,734 - INFO - training.closure - iteration 72: loss = -5.739627475577512
2023-04-09 11:18:10,208 - INFO - training.closure - iteration 73: loss = -5.743654097527416
2023-04-09 11:18:12,687 - INFO - training.closure - iteration 74: loss = -5.820175949938072
2023-04-09 11:18:15,720 - INFO - training.closure - iteration 75: loss = -5.832898648287749
2023-04-09 11:18:18,780 - INFO - training.closure - iteration 76: loss = -5.852197589359025
2023-04-09 11:18:21,269 - INFO - training.closure - iteration 77: loss = -5.88002833771812
2023-04-09 11:18:23,680 - INFO - training.closure - iteration 78: loss = -5.92444294810428
2023-04-09 11:18:26,086 - INFO - training.closure - iteration 79: loss = -5.978660459399742
2023-04-09 11:18:28,517 - INFO - training.closure - iteration 80: loss = -6.048569867981281
2023-04-09 11:18:30,820 - INFO - training.closure - iteration 81: loss = -6.178018970519672
2023-04-09 11:18:33,142 - INFO - training.closure - iteration 82: loss = -6.199309227609854
2023-04-09 11:18:35,413 - INFO - training.closure - iteration 83: loss = -6.2784696296931255
2023-04-09 11:18:37,906 - INFO - training.closure - iteration 84: loss = -6.3330788519613535
2023-04-09 11:18:40,225 - INFO - training.closure - iteration 85: loss = -6.374093396478111
2023-04-09 11:18:42,563 - INFO - training.closure - iteration 86: loss = -6.371274719872078
2023-04-09 11:18:44,933 - INFO - training.closure - iteration 87: loss = -6.3888958039774675
2023-04-09 11:18:47,254 - INFO - training.closure - iteration 88: loss = -6.405907321196983
2023-04-09 11:18:49,611 - INFO - training.closure - iteration 89: loss = -6.410811739473348
2023-04-09 11:18:51,943 - INFO - training.closure - iteration 90: loss = -6.421555103709989
2023-04-09 11:18:54,266 - INFO - training.closure - iteration 91: loss = -6.432776540220448
2023-04-09 11:18:56,697 - INFO - training.closure - iteration 92: loss = -6.473736368383973
2023-04-09 11:18:59,035 - INFO - training.closure - iteration 93: loss = -6.5089024324696325
2023-04-09 11:19:01,533 - INFO - training.closure - iteration 94: loss = -6.525127046176523
2023-04-09 11:19:03,915 - INFO - training.closure - iteration 95: loss = -6.5332355960811865
2023-04-09 11:19:06,293 - INFO - training.closure - iteration 96: loss = -6.544184786795806
2023-04-09 11:19:08,599 - INFO - training.closure - iteration 97: loss = -6.554257378849174
2023-04-09 11:19:10,889 - INFO - training.closure - iteration 98: loss = -6.57362793839466
2023-04-09 11:19:13,259 - INFO - training.closure - iteration 99: loss = -6.596743427637835
2023-04-09 11:19:15,611 - INFO - training.closure - iteration 100: loss = -6.624586050831677
2023-04-09 11:19:18,028 - INFO - training.closure - iteration 101: loss = -6.640069474160066
2023-04-09 11:19:20,611 - INFO - training.closure - iteration 102: loss = -6.672409995929641
2023-04-09 11:19:23,012 - INFO - training.closure - iteration 103: loss = -6.713902362978499
2023-04-09 11:19:25,329 - INFO - training.closure - iteration 104: loss = -6.789936769313911
2023-04-09 11:19:27,620 - INFO - training.closure - iteration 105: loss = -6.946214354459252
2023-04-09 11:19:29,879 - INFO - training.closure - iteration 106: loss = 9.026312085999969e+23
2023-04-09 11:19:32,170 - INFO - training.closure - iteration 107: loss = 217926850.40402818
2023-04-09 11:19:34,413 - INFO - training.closure - iteration 108: loss = 12.707260070248832
2023-04-09 11:19:36,763 - INFO - training.closure - iteration 109: loss = -6.9739367891847746
2023-04-09 11:19:39,148 - INFO - training.closure - iteration 110: loss = 206.90336781001318
2023-04-09 11:19:41,487 - INFO - training.closure - iteration 111: loss = -0.48515355690379636
2023-04-09 11:19:43,715 - INFO - training.closure - iteration 112: loss = -6.902335913088996
2023-04-09 11:19:46,114 - INFO - training.closure - iteration 113: loss = -7.056534840891487
2023-04-09 11:19:48,342 - INFO - training.closure - iteration 114: loss = -6.834585078936835
2023-04-09 11:19:50,600 - INFO - training.closure - iteration 115: loss = -7.07371471652317
2023-04-09 11:19:52,890 - INFO - training.closure - iteration 116: loss = -7.080792743649056
2023-04-09 11:19:55,241 - INFO - training.closure - iteration 117: loss = -7.0979680982894395
2023-04-09 11:19:57,515 - INFO - training.closure - iteration 118: loss = -7.100570028265519
2023-04-09 11:19:59,868 - INFO - training.closure - iteration 119: loss = -7.106303834509541
2023-04-09 11:20:02,156 - INFO - training.closure - iteration 120: loss = -7.109966724442113
2023-04-09 11:20:04,447 - INFO - training.closure - iteration 121: loss = -7.110146374664257
2023-04-09 11:20:06,785 - INFO - training.closure - iteration 122: loss = -7.113561976785393
2023-04-09 11:20:09,076 - INFO - training.closure - iteration 123: loss = -7.115925330112258
2023-04-09 11:20:11,367 - INFO - training.closure - iteration 124: loss = -7.127691918742102
2023-04-09 11:20:13,688 - INFO - training.closure - iteration 125: loss = -7.1439753664163375
2023-04-09 11:20:15,976 - INFO - training.closure - iteration 126: loss = -7.1737370554616104
2023-04-09 11:20:18,236 - INFO - training.closure - iteration 127: loss = -7.152055397992118
2023-04-09 11:20:20,521 - INFO - training.closure - iteration 128: loss = -7.191041353554718
2023-04-09 11:20:22,780 - INFO - training.closure - iteration 129: loss = -7.206264953247224
2023-04-09 11:20:25,101 - INFO - training.closure - iteration 130: loss = -7.209936073853915
2023-04-09 11:20:27,453 - INFO - training.closure - iteration 131: loss = -7.219057303519808
2023-04-09 11:20:29,948 - INFO - training.closure - iteration 132: loss = -7.219105251046854
2023-04-09 11:20:32,285 - INFO - training.closure - iteration 133: loss = -7.222462971402228
2023-04-09 11:20:34,600 - INFO - training.closure - iteration 134: loss = -7.22388218891912
2023-04-09 11:20:36,961 - INFO - training.closure - iteration 135: loss = -7.22548953104296
2023-04-09 11:20:39,214 - INFO - training.closure - iteration 136: loss = -7.23074011853115
2023-04-09 11:20:41,507 - INFO - training.closure - iteration 137: loss = -7.2365806220876125
2023-04-09 11:20:43,823 - INFO - training.closure - iteration 138: loss = -7.241960832570332
2023-04-09 11:20:46,147 - INFO - training.closure - iteration 139: loss = -7.246919897447826
2023-04-09 11:20:48,486 - INFO - training.closure - iteration 140: loss = -7.250491898596647
2023-04-09 11:20:50,843 - INFO - training.closure - iteration 141: loss = -7.252526120354014
2023-04-09 11:20:53,109 - INFO - training.closure - iteration 142: loss = -7.255803581082162
2023-04-09 11:20:55,400 - INFO - training.closure - iteration 143: loss = -7.262088225168542
2023-04-09 11:20:57,750 - INFO - training.closure - iteration 144: loss = -7.269703741784495
2023-04-09 11:21:00,107 - INFO - training.closure - iteration 145: loss = -7.249714171979451
2023-04-09 11:21:02,428 - INFO - training.closure - iteration 146: loss = -7.271684067156956
2023-04-09 11:21:04,735 - INFO - training.closure - iteration 147: loss = -7.2761413696359245
2023-04-09 11:21:07,042 - INFO - training.closure - iteration 148: loss = -7.278758252118081
2023-04-09 11:21:09,489 - INFO - training.closure - iteration 149: loss = -7.2799351361961655
2023-04-09 11:21:11,807 - INFO - training.closure - iteration 150: loss = -7.2812247223420545
2023-04-09 11:21:14,256 - INFO - training.closure - iteration 151: loss = -7.285812343012218
2023-04-09 11:21:16,546 - INFO - training.closure - iteration 152: loss = -7.288464979961678
2023-04-09 11:21:18,899 - INFO - training.closure - iteration 153: loss = -7.28647123065057
2023-04-09 11:21:21,205 - INFO - training.closure - iteration 154: loss = -7.290194558862982
2023-04-09 11:21:23,500 - INFO - training.closure - iteration 155: loss = -7.292514480792086
2023-04-09 11:21:25,849 - INFO - training.closure - iteration 156: loss = -7.293256834175365
2023-04-09 11:21:28,218 - INFO - training.closure - iteration 157: loss = -7.29357003211398
2023-04-09 11:21:30,615 - INFO - training.closure - iteration 158: loss = -7.293804871259588
2023-04-09 11:21:32,984 - INFO - training.closure - iteration 159: loss = -7.294313166686071
2023-04-09 11:21:35,427 - INFO - training.closure - iteration 160: loss = -7.294780100970671
2023-04-09 11:21:37,844 - INFO - training.closure - iteration 161: loss = -7.2962323369181
2023-04-09 11:21:40,231 - INFO - training.closure - iteration 162: loss = -7.300511890402125
2023-04-09 11:21:42,861 - INFO - training.closure - iteration 163: loss = -7.304328993401595
2023-04-09 11:21:45,343 - INFO - training.closure - iteration 164: loss = -7.298862678981359
2023-04-09 11:21:47,709 - INFO - training.closure - iteration 165: loss = -7.308067199994555
2023-04-09 11:21:50,125 - INFO - training.closure - iteration 166: loss = -7.317606292286739
2023-04-09 11:21:52,587 - INFO - training.closure - iteration 167: loss = -7.326409040454526
2023-04-09 11:21:55,034 - INFO - training.closure - iteration 168: loss = -7.336293227997045
2023-04-09 11:21:57,449 - INFO - training.closure - iteration 169: loss = -7.344004970861347
2023-04-09 11:21:59,928 - INFO - training.closure - iteration 170: loss = -7.3506013467815805
2023-04-09 11:22:02,325 - INFO - training.closure - iteration 171: loss = -7.364071026833784
2023-04-09 11:22:04,726 - INFO - training.closure - iteration 172: loss = -7.3714140921563835
2023-04-09 11:22:07,168 - INFO - training.closure - iteration 173: loss = -7.39199452439183
2023-04-09 11:22:09,631 - INFO - training.closure - iteration 174: loss = -5.0601298343066095
2023-04-09 11:22:12,075 - INFO - training.closure - iteration 175: loss = -7.36176844771176
2023-04-09 11:22:14,491 - INFO - training.closure - iteration 176: loss = -7.397805669346702
2023-04-09 11:22:16,861 - INFO - training.closure - iteration 177: loss = -7.405606946833463
2023-04-09 11:22:19,339 - INFO - training.closure - iteration 178: loss = -7.41726439676522
2023-04-09 11:22:21,850 - INFO - training.closure - iteration 179: loss = -7.422404542496643
2023-04-09 11:22:24,281 - INFO - training.closure - iteration 180: loss = -7.416038027105459
2023-04-09 11:22:26,729 - INFO - training.closure - iteration 181: loss = -7.4233298948135955
2023-04-09 11:22:29,175 - INFO - training.closure - iteration 182: loss = -7.425158753421158
2023-04-09 11:22:31,588 - INFO - training.closure - iteration 183: loss = -7.426446235938819
2023-04-09 11:22:34,068 - INFO - training.closure - iteration 184: loss = -7.427430029259255
2023-04-09 11:22:36,479 - INFO - training.closure - iteration 185: loss = -7.428961315819157
2023-04-09 11:22:38,949 - INFO - training.closure - iteration 186: loss = -7.430415939826675
2023-04-09 11:22:41,375 - INFO - training.closure - iteration 187: loss = -7.433852529042123
2023-04-09 11:22:43,822 - INFO - training.closure - iteration 188: loss = -7.437628957502864
2023-04-09 11:22:46,301 - INFO - training.closure - iteration 189: loss = -7.44133678546282
2023-04-09 11:22:48,702 - INFO - training.closure - iteration 190: loss = -7.44263515331941
2023-04-09 11:22:51,150 - INFO - training.closure - iteration 191: loss = -7.444935373957318
2023-04-09 11:22:53,659 - INFO - training.closure - iteration 192: loss = -7.447462243452013
2023-04-09 11:22:56,383 - INFO - training.closure - iteration 193: loss = -7.447723483670849
2023-04-09 11:22:58,803 - INFO - training.closure - iteration 194: loss = -7.4485503266550275
2023-04-09 11:23:01,235 - INFO - training.closure - iteration 195: loss = -7.448832625210988
2023-04-09 11:23:03,651 - INFO - training.closure - iteration 196: loss = -7.44925503171018
2023-04-09 11:23:06,193 - INFO - training.closure - iteration 197: loss = -7.450024016116846
2023-04-09 11:23:08,703 - INFO - training.closure - iteration 198: loss = -7.451071284310935
2023-04-09 11:23:11,083 - INFO - training.closure - iteration 199: loss = -7.452463475233109
2023-04-09 11:23:13,546 - INFO - training.closure - iteration 200: loss = -7.453037451104493
2023-04-09 11:23:16,053 - INFO - training.closure - iteration 201: loss = -7.45334369205202
2023-04-09 11:23:18,737 - INFO - training.closure - iteration 202: loss = -7.453384516308968
2023-04-09 11:23:21,667 - INFO - training.closure - iteration 203: loss = -7.453569103660167
2023-04-09 11:23:24,246 - INFO - training.closure - iteration 204: loss = -7.4543349434789326
2023-04-09 11:23:26,910 - INFO - training.closure - iteration 205: loss = -7.454724662425939
2023-04-09 11:23:29,486 - INFO - training.closure - iteration 206: loss = -7.455496532879854
2023-04-09 11:23:32,084 - INFO - training.closure - iteration 207: loss = -7.456167277223365
2023-04-09 11:23:34,798 - INFO - training.closure - iteration 208: loss = -7.45701407502408
2023-04-09 11:23:37,362 - INFO - training.closure - iteration 209: loss = -7.458586659619912
2023-04-09 11:23:39,914 - INFO - training.closure - iteration 210: loss = -7.460966266144218
2023-04-09 11:23:42,498 - INFO - training.closure - iteration 211: loss = -7.46573342168988
2023-04-09 11:23:45,054 - INFO - training.closure - iteration 212: loss = -7.473814409571845
2023-04-09 11:23:47,711 - INFO - training.closure - iteration 213: loss = -7.478967203659023
2023-04-09 11:23:50,279 - INFO - training.closure - iteration 214: loss = -7.485721938740371
2023-04-09 11:23:52,884 - INFO - training.closure - iteration 215: loss = -7.49338935785145
2023-04-09 11:23:55,491 - INFO - training.closure - iteration 216: loss = -7.500345107921748
2023-04-09 11:23:58,165 - INFO - training.closure - iteration 217: loss = -7.503607503221291
2023-04-09 11:24:00,684 - INFO - training.closure - iteration 218: loss = -7.5063285742802615
2023-04-09 11:24:03,293 - INFO - training.closure - iteration 219: loss = -7.5093412747954975
2023-04-09 11:24:05,811 - INFO - training.closure - iteration 220: loss = -7.51211644969831
2023-04-09 11:24:08,399 - INFO - training.closure - iteration 221: loss = -6.966695985298889
2023-04-09 11:24:10,952 - INFO - training.closure - iteration 222: loss = -7.5128826525509105
2023-04-09 11:24:13,535 - INFO - training.closure - iteration 223: loss = -7.514579324724549
2023-04-09 11:24:16,083 - INFO - training.closure - iteration 224: loss = -7.51551894501622
2023-04-09 11:24:18,671 - INFO - training.closure - iteration 225: loss = -7.517914480956883
2023-04-09 11:24:21,269 - INFO - training.closure - iteration 226: loss = -7.519088189449054
2023-04-09 11:24:23,936 - INFO - training.closure - iteration 227: loss = -7.5202358176669515
2023-04-09 11:24:26,560 - INFO - training.closure - iteration 228: loss = -7.521852173271459
2023-04-09 11:24:29,225 - INFO - training.closure - iteration 229: loss = -7.5236674769698375
2023-04-09 11:24:31,823 - INFO - training.closure - iteration 230: loss = -7.521492308188141
2023-04-09 11:24:34,367 - INFO - training.closure - iteration 231: loss = -7.525891467355159
2023-04-09 11:24:36,926 - INFO - training.closure - iteration 232: loss = -7.530534460182775
2023-04-09 11:24:39,505 - INFO - training.closure - iteration 233: loss = -7.534664536538754
2023-04-09 11:24:42,000 - INFO - training.closure - iteration 234: loss = -7.538482987786331
2023-04-09 11:24:44,576 - INFO - training.closure - iteration 235: loss = -7.539607840122716
2023-04-09 11:24:47,241 - INFO - training.closure - iteration 236: loss = -7.540628354881445
2023-04-09 11:24:49,869 - INFO - training.closure - iteration 237: loss = -7.541711171725155
2023-04-09 11:24:52,447 - INFO - training.closure - iteration 238: loss = -7.542629967727885
2023-04-09 11:24:55,075 - INFO - training.closure - iteration 239: loss = -7.543596989800479
2023-04-09 11:24:57,623 - INFO - training.closure - iteration 240: loss = -7.545748738792369
2023-04-09 11:25:00,256 - INFO - training.closure - iteration 241: loss = -7.548888401711583
2023-04-09 11:25:02,790 - INFO - training.closure - iteration 242: loss = -7.552364458415688
2023-04-09 11:25:05,323 - INFO - training.closure - iteration 243: loss = -7.551830393547458
2023-04-09 11:25:07,915 - INFO - training.closure - iteration 244: loss = -7.555336059959885
2023-04-09 11:25:10,450 - INFO - training.closure - iteration 245: loss = -7.559227495892989
2023-04-09 11:25:13,030 - INFO - training.closure - iteration 246: loss = -7.560621476914479
2023-04-09 11:25:15,576 - INFO - training.closure - iteration 247: loss = -7.562586009225255
2023-04-09 11:25:18,119 - INFO - training.closure - iteration 248: loss = -7.564431026356212
2023-04-09 11:25:20,639 - INFO - training.closure - iteration 249: loss = -7.5664059802946415
2023-04-09 11:25:23,218 - INFO - training.closure - iteration 250: loss = -7.567907256852212
2023-04-09 11:25:25,695 - INFO - training.closure - iteration 251: loss = -7.569053221529057
2023-04-09 11:25:28,231 - INFO - training.closure - iteration 252: loss = -7.506016081534697
2023-04-09 11:25:30,768 - INFO - training.closure - iteration 253: loss = -7.5695651959077015
2023-04-09 11:25:33,302 - INFO - training.closure - iteration 254: loss = -7.570882769186882
2023-04-09 11:25:35,988 - INFO - training.closure - iteration 255: loss = -7.5733918712383
2023-04-09 11:25:38,505 - INFO - training.closure - iteration 256: loss = -7.576343627507602
2023-04-09 11:25:41,163 - INFO - training.closure - iteration 257: loss = -7.580333582762426
2023-04-09 11:25:43,667 - INFO - training.closure - iteration 258: loss = -7.585950538424077
2023-04-09 11:25:46,284 - INFO - training.closure - iteration 259: loss = -7.591826195357902
2023-04-09 11:25:48,897 - INFO - training.closure - iteration 260: loss = -7.543487549746138
2023-04-09 11:25:51,443 - INFO - training.closure - iteration 261: loss = -7.595450143187771
2023-04-09 11:25:54,087 - INFO - training.closure - iteration 262: loss = -7.6007484121717
2023-04-09 11:25:56,634 - INFO - training.closure - iteration 263: loss = -7.604634254238058
2023-04-09 11:25:59,241 - INFO - training.closure - iteration 264: loss = -7.6063219480393505
2023-04-09 11:26:01,921 - INFO - training.closure - iteration 265: loss = -7.607871833578998
2023-04-09 11:26:04,495 - INFO - training.closure - iteration 266: loss = -7.608630795667421
2023-04-09 11:26:07,023 - INFO - training.closure - iteration 267: loss = -7.610696622615436
2023-04-09 11:26:09,612 - INFO - training.closure - iteration 268: loss = -7.615548415955987
2023-04-09 11:26:12,195 - INFO - training.closure - iteration 269: loss = -7.606578460203707
2023-04-09 11:26:14,765 - INFO - training.closure - iteration 270: loss = -7.617916884947441
2023-04-09 11:26:17,531 - INFO - training.closure - iteration 271: loss = -7.625547980087507
2023-04-09 11:26:20,207 - INFO - training.closure - iteration 272: loss = -7.63466166789879
2023-04-09 11:26:22,786 - INFO - training.closure - iteration 273: loss = -7.51809081467149
2023-04-09 11:26:25,511 - INFO - training.closure - iteration 274: loss = -7.637993785602797
2023-04-09 11:26:28,043 - INFO - training.closure - iteration 275: loss = -7.643161932635024
2023-04-09 11:26:30,686 - INFO - training.closure - iteration 276: loss = -7.6516246044995055
2023-04-09 11:26:33,230 - INFO - training.closure - iteration 277: loss = -7.643881228394983
2023-04-09 11:26:35,746 - INFO - training.closure - iteration 278: loss = -7.655081335831571
2023-04-09 11:26:38,314 - INFO - training.closure - iteration 279: loss = -7.659325670908067
2023-04-09 11:26:40,884 - INFO - training.closure - iteration 280: loss = -7.567732290771341
2023-04-09 11:26:43,372 - INFO - training.closure - iteration 281: loss = -7.6611607354183535
2023-04-09 11:26:45,949 - INFO - training.closure - iteration 282: loss = -7.6672032898931635
2023-04-09 11:26:48,500 - INFO - training.closure - iteration 283: loss = -7.658764947927889
2023-04-09 11:26:51,131 - INFO - training.closure - iteration 284: loss = -7.669049780449769
2023-04-09 11:26:53,655 - INFO - training.closure - iteration 285: loss = -7.670156250451492
2023-04-09 11:26:56,190 - INFO - training.closure - iteration 286: loss = -7.670793168609552
2023-04-09 11:26:58,812 - INFO - training.closure - iteration 287: loss = -7.671921022731608
2023-04-09 11:27:01,468 - INFO - training.closure - iteration 288: loss = -7.67486935704701
2023-04-09 11:27:04,033 - INFO - training.closure - iteration 289: loss = -7.674116128066557
2023-04-09 11:27:06,617 - INFO - training.closure - iteration 290: loss = -7.677000667365208
2023-04-09 11:27:09,195 - INFO - training.closure - iteration 291: loss = -7.68007139889796
2023-04-09 11:27:11,777 - INFO - training.closure - iteration 292: loss = -7.684105758939939
2023-04-09 11:27:14,354 - INFO - training.closure - iteration 293: loss = -7.686884952444855
2023-04-09 11:27:16,879 - INFO - training.closure - iteration 294: loss = -7.692620222776571
2023-04-09 11:27:19,430 - INFO - training.closure - iteration 295: loss = -7.696042108360302
2023-04-09 11:27:21,968 - INFO - training.closure - iteration 296: loss = -7.697607043354056
2023-04-09 11:27:24,526 - INFO - training.closure - iteration 297: loss = -7.698044013949075
2023-04-09 11:27:27,078 - INFO - training.closure - iteration 298: loss = -7.698506342964018
2023-04-09 11:27:29,581 - INFO - training.closure - iteration 299: loss = -7.698939150855426
2023-04-09 11:27:32,086 - INFO - training.closure - iteration 300: loss = -7.699338168562786
2023-04-09 11:27:34,632 - INFO - training.closure - iteration 301: loss = -7.700298858425576
2023-04-09 11:27:37,190 - INFO - training.closure - iteration 302: loss = -7.701439982012753
2023-04-09 11:27:39,832 - INFO - training.closure - iteration 303: loss = -7.695033933465151
2023-04-09 11:27:42,369 - INFO - training.closure - iteration 304: loss = -7.702721075816494
2023-04-09 11:27:44,926 - INFO - training.closure - iteration 305: loss = -7.704637397467907
2023-04-09 11:27:47,454 - INFO - training.closure - iteration 306: loss = -7.706068761494389
2023-04-09 11:27:50,120 - INFO - training.closure - iteration 307: loss = -7.707741775771867
2023-04-09 11:27:52,688 - INFO - training.closure - iteration 308: loss = -7.70989800990631
2023-04-09 11:27:55,271 - INFO - training.closure - iteration 309: loss = -7.7130244297277955
2023-04-09 11:27:57,857 - INFO - training.closure - iteration 310: loss = -7.719246468024809
2023-04-09 11:28:00,374 - INFO - training.closure - iteration 311: loss = -7.723815039602756
2023-04-09 11:28:02,983 - INFO - training.closure - iteration 312: loss = -7.727999658560978
2023-04-09 11:28:05,509 - INFO - training.closure - iteration 313: loss = -7.7274603417684515
2023-04-09 11:28:08,125 - INFO - training.closure - iteration 314: loss = -7.728918115388019
2023-04-09 11:28:10,757 - INFO - training.closure - iteration 315: loss = -7.730077874057755
2023-04-09 11:28:13,330 - INFO - training.closure - iteration 316: loss = -7.731657958672076
2023-04-09 11:28:16,006 - INFO - training.closure - iteration 317: loss = -7.732211839195189
2023-04-09 11:28:18,552 - INFO - training.closure - iteration 318: loss = -7.733826128300015
2023-04-09 11:28:21,104 - INFO - training.closure - iteration 319: loss = -7.734764340764392
2023-04-09 11:28:23,623 - INFO - training.closure - iteration 320: loss = -7.737186244460243
2023-04-09 11:28:26,153 - INFO - training.closure - iteration 321: loss = -7.641570743491251
2023-04-09 11:28:28,844 - INFO - training.closure - iteration 322: loss = -7.737822353485462
2023-04-09 11:28:31,417 - INFO - training.closure - iteration 323: loss = -7.740544726487478
2023-04-09 11:28:33,981 - INFO - training.closure - iteration 324: loss = -7.7388602713210055
2023-04-09 11:28:36,484 - INFO - training.closure - iteration 325: loss = -7.745294893118608
2023-04-09 11:28:39,048 - INFO - training.closure - iteration 326: loss = -7.747189002188968
2023-04-09 11:28:41,573 - INFO - training.closure - iteration 327: loss = -7.752056257000275
2023-04-09 11:28:44,095 - INFO - training.closure - iteration 328: loss = -7.7530447316515385
2023-04-09 11:28:46,643 - INFO - training.closure - iteration 329: loss = -7.753761898476971
2023-04-09 11:28:49,293 - INFO - training.closure - iteration 330: loss = -7.75446823937401
2023-04-09 11:28:51,931 - INFO - training.closure - iteration 331: loss = -7.754763705223295
2023-04-09 11:28:54,505 - INFO - training.closure - iteration 332: loss = -7.756724199446887
2023-04-09 11:28:57,072 - INFO - training.closure - iteration 333: loss = -7.760076672354687
2023-04-09 11:28:59,696 - INFO - training.closure - iteration 334: loss = -7.761836083589747
2023-04-09 11:29:02,218 - INFO - training.closure - iteration 335: loss = -7.76315590479297
2023-04-09 11:29:04,817 - INFO - training.closure - iteration 336: loss = -7.76449911342997
2023-04-09 11:29:07,435 - INFO - training.closure - iteration 337: loss = -7.765460823610607
2023-04-09 11:29:10,025 - INFO - training.closure - iteration 338: loss = -7.768017924586584
2023-04-09 11:29:12,581 - INFO - training.closure - iteration 339: loss = -7.769259756081766
2023-04-09 11:29:15,307 - INFO - training.closure - iteration 340: loss = -7.769985659194108
2023-04-09 11:29:18,127 - INFO - training.closure - iteration 341: loss = -7.770618228273973
2023-04-09 11:29:20,769 - INFO - training.closure - iteration 342: loss = -7.771015188758355
2023-04-09 11:29:23,326 - INFO - training.closure - iteration 343: loss = -7.771427850209081
2023-04-09 11:29:25,939 - INFO - training.closure - iteration 344: loss = -7.7720623972439515
2023-04-09 11:29:28,552 - INFO - training.closure - iteration 345: loss = -7.773056159558452
2023-04-09 11:29:31,112 - INFO - training.closure - iteration 346: loss = -7.774476224385064
2023-04-09 11:29:33,693 - INFO - training.closure - iteration 347: loss = -7.773326593809787
2023-04-09 11:29:36,250 - INFO - training.closure - iteration 348: loss = -7.775103614841589
2023-04-09 11:29:38,829 - INFO - training.closure - iteration 349: loss = -7.776047139308968
2023-04-09 11:29:41,385 - INFO - training.closure - iteration 350: loss = -7.776811625697553
2023-04-09 11:29:43,905 - INFO - training.closure - iteration 351: loss = -7.499646839281837
2023-04-09 11:29:46,401 - INFO - training.closure - iteration 352: loss = -7.777029868814225
2023-04-09 11:29:48,906 - INFO - training.closure - iteration 353: loss = -7.777889486683332
2023-04-09 11:29:51,465 - INFO - training.closure - iteration 354: loss = -7.778998962950447
2023-04-09 11:29:54,026 - INFO - training.closure - iteration 355: loss = -7.780551048265037
2023-04-09 11:29:56,575 - INFO - training.closure - iteration 356: loss = -7.782986652471167
2023-04-09 11:29:59,167 - INFO - training.closure - iteration 357: loss = -7.783860896083498
2023-04-09 11:30:01,734 - INFO - training.closure - iteration 358: loss = -7.785442042958697
2023-04-09 11:30:04,181 - INFO - training.closure - iteration 359: loss = -7.7869142307438315
2023-04-09 11:30:06,808 - INFO - training.closure - iteration 360: loss = -7.788387587513058
2023-04-09 11:30:09,360 - INFO - training.closure - iteration 361: loss = -7.791028683515387
2023-04-09 11:30:11,924 - INFO - training.closure - iteration 362: loss = -7.796054434269102
2023-04-09 11:30:14,401 - INFO - training.closure - iteration 363: loss = -7.796616752492021
2023-04-09 11:30:16,917 - INFO - training.closure - iteration 364: loss = -7.799724230702763
2023-04-09 11:30:19,485 - INFO - training.closure - iteration 365: loss = -7.802747125566215
2023-04-09 11:30:22,101 - INFO - training.closure - iteration 366: loss = -7.811901292251312
2023-04-09 11:30:24,736 - INFO - training.closure - iteration 367: loss = -7.809588684291599
2023-04-09 11:30:27,217 - INFO - training.closure - iteration 368: loss = -7.81753831648941
2023-04-09 11:30:29,934 - INFO - training.closure - iteration 369: loss = -7.82205820413477
2023-04-09 11:30:32,487 - INFO - training.closure - iteration 370: loss = -7.830704582734542
2023-04-09 11:30:35,061 - INFO - training.closure - iteration 371: loss = -7.837588344201158
2023-04-09 11:30:37,644 - INFO - training.closure - iteration 372: loss = -7.839864277981874
2023-04-09 11:30:40,257 - INFO - training.closure - iteration 373: loss = -7.847214763767696
2023-04-09 11:30:42,860 - INFO - training.closure - iteration 374: loss = -7.8589896769924374
2023-04-09 11:30:45,376 - INFO - training.closure - iteration 375: loss = -7.870260171991401
2023-04-09 11:30:47,885 - INFO - training.closure - iteration 376: loss = -7.850708221790013
2023-04-09 11:30:50,447 - INFO - training.closure - iteration 377: loss = -7.892627815094496
2023-04-09 11:30:53,001 - INFO - training.closure - iteration 378: loss = -7.920339413464301
2023-04-09 11:30:55,630 - INFO - training.closure - iteration 379: loss = -7.945303582208551
2023-04-09 11:30:58,174 - INFO - training.closure - iteration 380: loss = -7.935718000537658
2023-04-09 11:31:00,749 - INFO - training.closure - iteration 381: loss = -7.959490594329298
2023-04-09 11:31:03,353 - INFO - training.closure - iteration 382: loss = -7.962893839043411
2023-04-09 11:31:05,921 - INFO - training.closure - iteration 383: loss = -7.973483833255074
2023-04-09 11:31:08,520 - INFO - training.closure - iteration 384: loss = -7.994229258900732
2023-04-09 11:31:11,092 - INFO - training.closure - iteration 385: loss = -8.007495012263238
2023-04-09 11:31:13,674 - INFO - training.closure - iteration 386: loss = -8.02034994259792
2023-04-09 11:31:16,211 - INFO - training.closure - iteration 387: loss = -7.969443202814526
2023-04-09 11:31:18,863 - INFO - training.closure - iteration 388: loss = -8.030038594656245
2023-04-09 11:31:21,420 - INFO - training.closure - iteration 389: loss = -8.014965723903607
2023-04-09 11:31:24,004 - INFO - training.closure - iteration 390: loss = -8.033324093025916
2023-04-09 11:31:26,551 - INFO - training.closure - iteration 391: loss = -8.036357423985367
2023-04-09 11:31:29,191 - INFO - training.closure - iteration 392: loss = -8.043156513673724
2023-04-09 11:31:31,766 - INFO - training.closure - iteration 393: loss = -8.050396611618803
2023-04-09 11:31:34,300 - INFO - training.closure - iteration 394: loss = -8.051337792057241
2023-04-09 11:31:36,898 - INFO - training.closure - iteration 395: loss = -7.84812615699231
2023-04-09 11:31:39,400 - INFO - training.closure - iteration 396: loss = -8.05476131577098
2023-04-09 11:31:41,932 - INFO - training.closure - iteration 397: loss = -8.06124837722586
2023-04-09 11:31:44,551 - INFO - training.closure - iteration 398: loss = -8.080137805393273
2023-04-09 11:31:47,143 - INFO - training.closure - iteration 399: loss = -8.094125117468296
2023-04-09 11:31:49,731 - INFO - training.closure - iteration 400: loss = -8.109646549539416
2023-04-09 11:31:52,292 - INFO - training.closure - iteration 401: loss = -8.139867784129734
2023-04-09 11:31:54,850 - INFO - training.closure - iteration 402: loss = -8.144477357041165
2023-04-09 11:31:57,390 - INFO - training.closure - iteration 403: loss = -8.167122852679942
2023-04-09 11:31:59,921 - INFO - training.closure - iteration 404: loss = -8.181525648560942
2023-04-09 11:32:02,493 - INFO - training.closure - iteration 405: loss = -8.189398445290319
2023-04-09 11:32:05,145 - INFO - training.closure - iteration 406: loss = -8.20349241331494
2023-04-09 11:32:07,789 - INFO - training.closure - iteration 407: loss = -8.203865157597273
2023-04-09 11:32:10,402 - INFO - training.closure - iteration 408: loss = -8.21330596124667
2023-04-09 11:32:12,977 - INFO - training.closure - iteration 409: loss = -8.221482587954423
2023-04-09 11:32:15,489 - INFO - training.closure - iteration 410: loss = -8.217044964752631
2023-04-09 11:32:18,043 - INFO - training.closure - iteration 411: loss = -8.226927320843581
2023-04-09 11:32:20,551 - INFO - training.closure - iteration 412: loss = -8.237256697481467
2023-04-09 11:32:23,196 - INFO - training.closure - iteration 413: loss = -8.249541440748969
2023-04-09 11:32:25,753 - INFO - training.closure - iteration 414: loss = -8.25270936292018
2023-04-09 11:32:28,280 - INFO - training.closure - iteration 415: loss = -8.255937877813706
2023-04-09 11:32:30,744 - INFO - training.closure - iteration 416: loss = -8.258105512610326
2023-04-09 11:32:33,446 - INFO - training.closure - iteration 417: loss = -8.260668199512551
2023-04-09 11:32:36,034 - INFO - training.closure - iteration 418: loss = -8.269035870889107
2023-04-09 11:32:38,630 - INFO - training.closure - iteration 419: loss = -8.27961703646885
2023-04-09 11:32:41,201 - INFO - training.closure - iteration 420: loss = -8.2965591972801
2023-04-09 11:32:43,698 - INFO - training.closure - iteration 421: loss = -8.311745093464491
2023-04-09 11:32:46,233 - INFO - training.closure - iteration 422: loss = -8.328513946017255
2023-04-09 11:32:48,758 - INFO - training.closure - iteration 423: loss = -8.3428356577835
2023-04-09 11:32:51,304 - INFO - training.closure - iteration 424: loss = -8.358996511883408
2023-04-09 11:32:53,880 - INFO - training.closure - iteration 425: loss = -8.369538745587393
2023-04-09 11:32:56,523 - INFO - training.closure - iteration 426: loss = -8.379422152272426
2023-04-09 11:32:59,132 - INFO - training.closure - iteration 427: loss = -8.35752407939561
2023-04-09 11:33:01,764 - INFO - training.closure - iteration 428: loss = -8.391131129668072
2023-04-09 11:33:04,268 - INFO - training.closure - iteration 429: loss = -8.419628878121905
2023-04-09 11:33:06,796 - INFO - training.closure - iteration 430: loss = -8.416809791756945
2023-04-09 11:33:09,307 - INFO - training.closure - iteration 431: loss = -8.43237679375591
2023-04-09 11:33:11,806 - INFO - training.closure - iteration 432: loss = -8.440362162900353
2023-04-09 11:33:14,371 - INFO - training.closure - iteration 433: loss = -8.427096673501392
2023-04-09 11:33:16,904 - INFO - training.closure - iteration 434: loss = -8.445999653443216
2023-04-09 11:33:19,432 - INFO - training.closure - iteration 435: loss = -8.452999846665554
2023-04-09 11:33:22,059 - INFO - training.closure - iteration 436: loss = -8.455443305374462
2023-04-09 11:33:24,593 - INFO - training.closure - iteration 437: loss = -8.466932681479712
2023-04-09 11:33:27,145 - INFO - training.closure - iteration 438: loss = -8.475940482856828
2023-04-09 11:33:29,814 - INFO - training.closure - iteration 439: loss = -8.475210802153352
2023-04-09 11:33:32,416 - INFO - training.closure - iteration 440: loss = -8.487262545654776
2023-04-09 11:33:34,939 - INFO - training.closure - iteration 441: loss = -8.491316122706689
2023-04-09 11:33:37,472 - INFO - training.closure - iteration 442: loss = -8.501220233491582
2023-04-09 11:33:40,035 - INFO - training.closure - iteration 443: loss = -8.505319935780417
2023-04-09 11:33:42,537 - INFO - training.closure - iteration 444: loss = -8.51187079521966
2023-04-09 11:33:45,208 - INFO - training.closure - iteration 445: loss = -8.516970059863674
2023-04-09 11:33:47,790 - INFO - training.closure - iteration 446: loss = -8.51845170754229
2023-04-09 11:33:50,334 - INFO - training.closure - iteration 447: loss = -8.506278872726469
2023-04-09 11:33:52,868 - INFO - training.closure - iteration 448: loss = -8.52453553768122
2023-04-09 11:33:55,428 - INFO - training.closure - iteration 449: loss = -8.529980708625475
2023-04-09 11:33:58,020 - INFO - training.closure - iteration 450: loss = -8.536517716068774
2023-04-09 11:34:00,515 - INFO - training.closure - iteration 451: loss = -8.541226672892059
2023-04-09 11:34:03,175 - INFO - training.closure - iteration 452: loss = -8.54453604879343
2023-04-09 11:34:05,770 - INFO - training.closure - iteration 453: loss = -8.521339665500975
2023-04-09 11:34:08,365 - INFO - training.closure - iteration 454: loss = -8.546300504589631
2023-04-09 11:34:11,012 - INFO - training.closure - iteration 455: loss = -8.548994739162051
2023-04-09 11:34:13,653 - INFO - training.closure - iteration 456: loss = -8.550779320896932
2023-04-09 11:34:16,145 - INFO - training.closure - iteration 457: loss = -8.550281041752806
2023-04-09 11:34:18,660 - INFO - training.closure - iteration 458: loss = -8.557244322590545
2023-04-09 11:34:21,243 - INFO - training.closure - iteration 459: loss = -8.565679014850208
2023-04-09 11:34:23,923 - INFO - training.closure - iteration 460: loss = -8.573606780403127
2023-04-09 11:34:26,474 - INFO - training.closure - iteration 461: loss = -8.591723576318712
2023-04-09 11:34:29,000 - INFO - training.closure - iteration 462: loss = -8.600760397780899
2023-04-09 11:34:31,581 - INFO - training.closure - iteration 463: loss = -8.615629500606778
2023-04-09 11:34:34,314 - INFO - training.closure - iteration 464: loss = -8.619403877420162
2023-04-09 11:34:36,862 - INFO - training.closure - iteration 465: loss = -8.629559680584649
2023-04-09 11:34:39,353 - INFO - training.closure - iteration 466: loss = -8.639233854589733
2023-04-09 11:34:41,912 - INFO - training.closure - iteration 467: loss = -8.65312302098459
2023-04-09 11:34:44,609 - INFO - training.closure - iteration 468: loss = -8.66559120686422
2023-04-09 11:34:47,272 - INFO - training.closure - iteration 469: loss = -8.571111295533656
2023-04-09 11:34:49,910 - INFO - training.closure - iteration 470: loss = -8.673692237125332
2023-04-09 11:34:52,430 - INFO - training.closure - iteration 471: loss = -8.684290891572942
2023-04-09 11:34:55,146 - INFO - training.closure - iteration 472: loss = -8.70264647319923
2023-04-09 11:34:57,784 - INFO - training.closure - iteration 473: loss = -8.530990257224467
2023-04-09 11:35:00,495 - INFO - training.closure - iteration 474: loss = -8.708862057628606
2023-04-09 11:35:03,103 - INFO - training.closure - iteration 475: loss = -8.715722843820593
2023-04-09 11:35:05,671 - INFO - training.closure - iteration 476: loss = -8.720726147359084
2023-04-09 11:35:08,212 - INFO - training.closure - iteration 477: loss = -8.730090486291898
2023-04-09 11:35:10,819 - INFO - training.closure - iteration 478: loss = -8.739196899425433
2023-04-09 11:35:13,397 - INFO - training.closure - iteration 479: loss = -8.747560007576542
2023-04-09 11:35:15,924 - INFO - training.closure - iteration 480: loss = -8.760281479627045
2023-04-09 11:35:18,470 - INFO - training.closure - iteration 481: loss = -8.765709495654638
2023-04-09 11:35:20,995 - INFO - training.closure - iteration 482: loss = -8.772533770886062
2023-04-09 11:35:23,624 - INFO - training.closure - iteration 483: loss = -8.769377913084513
2023-04-09 11:35:26,178 - INFO - training.closure - iteration 484: loss = -8.77781898651862
2023-04-09 11:35:28,735 - INFO - training.closure - iteration 485: loss = -8.784740415489157
2023-04-09 11:35:31,539 - INFO - training.closure - iteration 486: loss = -8.789064253221703
2023-04-09 11:35:34,089 - INFO - training.closure - iteration 487: loss = -8.79457081238453
2023-04-09 11:35:36,685 - INFO - training.closure - iteration 488: loss = -8.803484899080221
2023-04-09 11:35:39,183 - INFO - training.closure - iteration 489: loss = -8.806405158064253
2023-04-09 11:35:41,721 - INFO - training.closure - iteration 490: loss = -8.812815915475818
2023-04-09 11:35:44,259 - INFO - training.closure - iteration 491: loss = -8.819700501188214
2023-04-09 11:35:46,885 - INFO - training.closure - iteration 492: loss = -8.833323424760984
2023-04-09 11:35:49,497 - INFO - training.closure - iteration 493: loss = -8.850427239576204
2023-04-09 11:35:52,031 - INFO - training.closure - iteration 494: loss = -8.865390708169016
2023-04-09 11:35:54,655 - INFO - training.closure - iteration 495: loss = -8.889108717029924
2023-04-09 11:35:57,287 - INFO - training.closure - iteration 496: loss = -8.927498776884889
2023-04-09 11:35:59,864 - INFO - training.closure - iteration 497: loss = -0.8239626327837195
2023-04-09 11:36:02,466 - INFO - training.closure - iteration 498: loss = -8.610989840662041
2023-04-09 11:36:05,087 - INFO - training.closure - iteration 499: loss = -8.93999105721441
2023-04-09 11:36:07,635 - INFO - training.closure - iteration 500: loss = -9.029259980084479
2023-04-09 11:36:10,152 - INFO - training.closure - iteration 501: loss = 0.4487799691961012
2023-04-09 11:36:12,772 - INFO - training.closure - iteration 502: loss = -8.819564375728016
2023-04-09 11:36:15,360 - INFO - training.closure - iteration 503: loss = -9.0891807735793
2023-04-09 11:36:17,901 - INFO - training.closure - iteration 504: loss = -9.155430483327274
2023-04-09 11:36:20,418 - INFO - training.closure - iteration 505: loss = 90.1132358197248
2023-04-09 11:36:22,958 - INFO - training.closure - iteration 506: loss = -4.960832591793962
2023-04-09 11:36:25,542 - INFO - training.closure - iteration 507: loss = -9.182697147037155
2023-04-09 11:36:28,109 - INFO - training.closure - iteration 508: loss = -9.160448895618327
2023-04-09 11:36:30,704 - INFO - training.closure - iteration 509: loss = -9.210206447486037
2023-04-09 11:36:33,211 - INFO - training.closure - iteration 510: loss = -9.228959370056847
2023-04-09 11:36:35,736 - INFO - training.closure - iteration 511: loss = -9.253918202065861
2023-04-09 11:36:38,378 - INFO - training.closure - iteration 512: loss = -9.286290962897679
2023-04-09 11:36:40,952 - INFO - training.closure - iteration 513: loss = -9.318334007493045
2023-04-09 11:36:43,494 - INFO - training.closure - iteration 514: loss = -9.336042302026629
2023-04-09 11:36:46,064 - INFO - training.closure - iteration 515: loss = -9.354358785805402
2023-04-09 11:36:48,694 - INFO - training.closure - iteration 516: loss = -9.372870133952112
2023-04-09 11:36:51,248 - INFO - training.closure - iteration 517: loss = -9.366696436499398
2023-04-09 11:36:53,846 - INFO - training.closure - iteration 518: loss = -9.384692213046787
2023-04-09 11:36:56,431 - INFO - training.closure - iteration 519: loss = -9.394939718668883
2023-04-09 11:36:58,968 - INFO - training.closure - iteration 520: loss = -9.404266114943141
2023-04-09 11:37:01,638 - INFO - training.closure - iteration 521: loss = -9.422076745749063
2023-04-09 11:37:04,247 - INFO - training.closure - iteration 522: loss = -9.430579562880522
2023-04-09 11:37:06,866 - INFO - training.closure - iteration 523: loss = -9.435722069433977
2023-04-09 11:37:09,414 - INFO - training.closure - iteration 524: loss = -9.444355721713974
2023-04-09 11:37:11,979 - INFO - training.closure - iteration 525: loss = -9.452781305914353
2023-04-09 11:37:14,541 - INFO - training.closure - iteration 526: loss = -9.457520964687044
2023-04-09 11:37:17,121 - INFO - training.closure - iteration 527: loss = -9.459120206060966
2023-04-09 11:37:19,658 - INFO - training.closure - iteration 528: loss = -9.461445205298403
2023-04-09 11:37:22,200 - INFO - training.closure - iteration 529: loss = -9.467474846875096
2023-04-09 11:37:24,781 - INFO - training.closure - iteration 530: loss = -9.485852503391838
2023-04-09 11:37:27,477 - INFO - training.closure - iteration 531: loss = -9.497639895364134
2023-04-09 11:37:30,067 - INFO - training.closure - iteration 532: loss = -9.498444472584897
2023-04-09 11:37:32,776 - INFO - training.closure - iteration 533: loss = -9.508370675425887
2023-04-09 11:37:35,348 - INFO - training.closure - iteration 534: loss = -9.512751517855023
2023-04-09 11:37:37,934 - INFO - training.closure - iteration 535: loss = -9.519972877538944
2023-04-09 11:37:40,509 - INFO - training.closure - iteration 536: loss = -9.525194013069017
2023-04-09 11:37:43,171 - INFO - training.closure - iteration 537: loss = -9.532280373113721
2023-04-09 11:37:45,788 - INFO - training.closure - iteration 538: loss = -9.541791053003157
2023-04-09 11:37:48,468 - INFO - training.closure - iteration 539: loss = -9.559115948611204
2023-04-09 11:37:51,111 - INFO - training.closure - iteration 540: loss = -9.592131167817383
2023-04-09 11:37:53,668 - INFO - training.closure - iteration 541: loss = -9.616793541574529
2023-04-09 11:37:56,246 - INFO - training.closure - iteration 542: loss = -8.164511731441024
2023-04-09 11:37:58,854 - INFO - training.closure - iteration 543: loss = -9.626660801559293
2023-04-09 11:38:01,433 - INFO - training.closure - iteration 544: loss = -9.64207156179561
2023-04-09 11:38:03,961 - INFO - training.closure - iteration 545: loss = -9.651397714645856
2023-04-09 11:38:06,449 - INFO - training.closure - iteration 546: loss = -9.658824252502427
2023-04-09 11:38:09,013 - INFO - training.closure - iteration 547: loss = -9.663780694553763
2023-04-09 11:38:11,577 - INFO - training.closure - iteration 548: loss = -9.671121711380666
2023-04-09 11:38:14,156 - INFO - training.closure - iteration 549: loss = -9.67863061642096
2023-04-09 11:38:16,768 - INFO - training.closure - iteration 550: loss = -9.646349560142438
2023-04-09 11:38:19,407 - INFO - training.closure - iteration 551: loss = -9.685678106034572
2023-04-09 11:38:21,938 - INFO - training.closure - iteration 552: loss = -9.693634811960568
2023-04-09 11:38:24,470 - INFO - training.closure - iteration 553: loss = -9.697643669753624
2023-04-09 11:38:27,041 - INFO - training.closure - iteration 554: loss = -9.702233753915639
2023-04-09 11:38:29,597 - INFO - training.closure - iteration 555: loss = -9.7071736211302
2023-04-09 11:38:32,152 - INFO - training.closure - iteration 556: loss = -9.711219441596768
2023-04-09 11:38:34,765 - INFO - training.closure - iteration 557: loss = -9.718052572088908
2023-04-09 11:38:37,289 - INFO - training.closure - iteration 558: loss = -9.725024755039367
2023-04-09 11:38:39,967 - INFO - training.closure - iteration 559: loss = -9.736773320002982
2023-04-09 11:38:42,512 - INFO - training.closure - iteration 560: loss = -9.344761433075936
2023-04-09 11:38:45,076 - INFO - training.closure - iteration 561: loss = -9.739274750393257
2023-04-09 11:38:47,761 - INFO - training.closure - iteration 562: loss = -9.755689991234101
2023-04-09 11:38:50,269 - INFO - training.closure - iteration 563: loss = -9.762307421361376
2023-04-09 11:38:52,803 - INFO - training.closure - iteration 564: loss = -9.766712721452677
2023-04-09 11:38:55,316 - INFO - training.closure - iteration 565: loss = -9.768296548813005
2023-04-09 11:38:57,848 - INFO - training.closure - iteration 566: loss = -9.770333259275006
2023-04-09 11:39:00,428 - INFO - training.closure - iteration 567: loss = -9.783485918521587
2023-04-09 11:39:02,956 - INFO - training.closure - iteration 568: loss = -9.791875122758793
2023-04-09 11:39:05,585 - INFO - training.closure - iteration 569: loss = -9.80008908436989
2023-04-09 11:39:08,100 - INFO - training.closure - iteration 570: loss = -9.778585752383496
2023-04-09 11:39:10,606 - INFO - training.closure - iteration 571: loss = -9.80466599052798
2023-04-09 11:39:13,137 - INFO - training.closure - iteration 572: loss = -9.806697644798902
2023-04-09 11:39:15,662 - INFO - training.closure - iteration 573: loss = -9.808359515473295
2023-04-09 11:39:18,223 - INFO - training.closure - iteration 574: loss = -9.802106803754452
2023-04-09 11:39:20,795 - INFO - training.closure - iteration 575: loss = -9.810066027057601
2023-04-09 11:39:23,360 - INFO - training.closure - iteration 576: loss = -9.812591357399224
2023-04-09 11:39:25,905 - INFO - training.closure - iteration 577: loss = -9.81886946753755
2023-04-09 11:39:28,518 - INFO - training.closure - iteration 578: loss = -9.828929319633765
2023-04-09 11:39:31,052 - INFO - training.closure - iteration 579: loss = -9.840623833512733
2023-04-09 11:39:33,628 - INFO - training.closure - iteration 580: loss = -9.850039737287757
2023-04-09 11:39:36,159 - INFO - training.closure - iteration 581: loss = -9.855936423772192
2023-04-09 11:39:38,705 - INFO - training.closure - iteration 582: loss = -9.8619340198604
2023-04-09 11:39:41,275 - INFO - training.closure - iteration 583: loss = -9.870754531375198
2023-04-09 11:39:43,774 - INFO - training.closure - iteration 584: loss = -9.89360219017453
2023-04-09 11:39:46,306 - INFO - training.closure - iteration 585: loss = -9.562017278179534
2023-04-09 11:39:48,874 - INFO - training.closure - iteration 586: loss = -9.898911556168304
2023-04-09 11:39:51,511 - INFO - training.closure - iteration 587: loss = -9.905957588784858
2023-04-09 11:39:54,152 - INFO - training.closure - iteration 588: loss = -9.917813954317205
2023-04-09 11:39:56,680 - INFO - training.closure - iteration 589: loss = -9.927361236774242
2023-04-09 11:39:59,257 - INFO - training.closure - iteration 590: loss = -9.938153404417864
2023-04-09 11:40:01,811 - INFO - training.closure - iteration 591: loss = -9.949022525411678
2023-04-09 11:40:04,420 - INFO - training.closure - iteration 592: loss = -9.97120420611539
2023-04-09 11:40:06,988 - INFO - training.closure - iteration 593: loss = -10.032557061759995
2023-04-09 11:40:09,646 - INFO - training.closure - iteration 594: loss = -9.633332306992042
2023-04-09 11:40:12,201 - INFO - training.closure - iteration 595: loss = -10.049038629290454
2023-04-09 11:40:14,739 - INFO - training.closure - iteration 596: loss = -10.137087930410392
2023-04-09 11:40:17,346 - INFO - training.closure - iteration 597: loss = -10.192162676869778
2023-04-09 11:40:19,853 - INFO - training.closure - iteration 598: loss = -10.164558164415421
2023-04-09 11:40:22,456 - INFO - training.closure - iteration 599: loss = -10.225157849373762
2023-04-09 11:40:25,057 - INFO - training.closure - iteration 600: loss = -10.238199077559837
2023-04-09 11:40:27,670 - INFO - training.closure - iteration 601: loss = -10.251902356806886
2023-04-09 11:40:30,201 - INFO - training.closure - iteration 602: loss = -10.266346590720513
2023-04-09 11:40:32,750 - INFO - training.closure - iteration 603: loss = -10.227184859202438
2023-04-09 11:40:35,298 - INFO - training.closure - iteration 604: loss = -10.27326655495228
2023-04-09 11:40:37,813 - INFO - training.closure - iteration 605: loss = -10.286864204321523
2023-04-09 11:40:40,312 - INFO - training.closure - iteration 606: loss = -10.304829017924197
2023-04-09 11:40:42,937 - INFO - training.closure - iteration 607: loss = -10.29813545365402
2023-04-09 11:40:45,514 - INFO - training.closure - iteration 608: loss = -10.32424064773267
2023-04-09 11:40:48,097 - INFO - training.closure - iteration 609: loss = -10.342012244844724
2023-04-09 11:40:50,659 - INFO - training.closure - iteration 610: loss = -10.282782898240772
2023-04-09 11:40:53,207 - INFO - training.closure - iteration 611: loss = -10.358907355723229
2023-04-09 11:40:55,736 - INFO - training.closure - iteration 612: loss = -10.367007182596954
2023-04-09 11:40:58,313 - INFO - training.closure - iteration 613: loss = -10.372382522243207
2023-04-09 11:41:00,913 - INFO - training.closure - iteration 614: loss = -10.378631184261323
2023-04-09 11:41:03,422 - INFO - training.closure - iteration 615: loss = -10.387542087742514
2023-04-09 11:41:06,119 - INFO - training.closure - iteration 616: loss = -10.401696465561997
2023-04-09 11:41:08,650 - INFO - training.closure - iteration 617: loss = -10.414862013254968
2023-04-09 11:41:11,211 - INFO - training.closure - iteration 618: loss = -10.444683865213086
2023-04-09 11:41:13,711 - INFO - training.closure - iteration 619: loss = -10.489658914368707
2023-04-09 11:41:16,232 - INFO - training.closure - iteration 620: loss = -10.54720871120275
2023-04-09 11:41:18,762 - INFO - training.closure - iteration 621: loss = -9.064767943022035
2023-04-09 11:41:21,306 - INFO - training.closure - iteration 622: loss = -10.557225295798006
2023-04-09 11:41:23,908 - INFO - training.closure - iteration 623: loss = -10.564906282942676
2023-04-09 11:41:26,536 - INFO - training.closure - iteration 624: loss = -10.592417223347518
2023-04-09 11:41:29,052 - INFO - training.closure - iteration 625: loss = -10.608991498480492
2023-04-09 11:41:31,691 - INFO - training.closure - iteration 626: loss = -10.618641035780549
2023-04-09 11:41:34,300 - INFO - training.closure - iteration 627: loss = -10.634387402116243
2023-04-09 11:41:36,835 - INFO - training.closure - iteration 628: loss = -10.66349737656908
2023-04-09 11:41:39,351 - INFO - training.closure - iteration 629: loss = -10.677437817481216
2023-04-09 11:41:42,002 - INFO - training.closure - iteration 630: loss = -10.700173740916519
2023-04-09 11:41:44,499 - INFO - training.closure - iteration 631: loss = -10.721714930622984
2023-04-09 11:41:47,073 - INFO - training.closure - iteration 632: loss = -10.749280612876564
2023-04-09 11:41:49,628 - INFO - training.closure - iteration 633: loss = -10.78353392857153
2023-04-09 11:41:52,226 - INFO - training.closure - iteration 634: loss = -7.031479034020938
2023-04-09 11:41:54,793 - INFO - training.closure - iteration 635: loss = -10.793786549751797
2023-04-09 11:41:57,339 - INFO - training.closure - iteration 636: loss = -10.822746386827689
2023-04-09 11:41:59,844 - INFO - training.closure - iteration 637: loss = -10.75261185958141
2023-04-09 11:42:02,388 - INFO - training.closure - iteration 638: loss = -10.848449886078612
2023-04-09 11:42:04,897 - INFO - training.closure - iteration 639: loss = -10.862070920260438
2023-04-09 11:42:07,410 - INFO - training.closure - iteration 640: loss = -10.883941259265384
2023-04-09 11:42:09,975 - INFO - training.closure - iteration 641: loss = -10.937604294568427
2023-04-09 11:42:12,479 - INFO - training.closure - iteration 642: loss = -10.972427295716923
2023-04-09 11:42:15,056 - INFO - training.closure - iteration 643: loss = -11.00362521612993
2023-04-09 11:42:17,687 - INFO - training.closure - iteration 644: loss = -11.035579031544822
2023-04-09 11:42:20,344 - INFO - training.closure - iteration 645: loss = -11.073481517895823
2023-04-09 11:42:22,855 - INFO - training.closure - iteration 646: loss = -11.10562238799814
2023-04-09 11:42:25,408 - INFO - training.closure - iteration 647: loss = -11.114895721214276
2023-04-09 11:42:28,013 - INFO - training.closure - iteration 648: loss = -11.125498223028439
2023-04-09 11:42:30,588 - INFO - training.closure - iteration 649: loss = -11.138178210167705
2023-04-09 11:42:33,217 - INFO - training.closure - iteration 650: loss = -11.16513582330387
2023-04-09 11:42:35,817 - INFO - training.closure - iteration 651: loss = -11.173590472716352
2023-04-09 11:42:38,429 - INFO - training.closure - iteration 652: loss = -11.1872801555479
2023-04-09 11:42:41,027 - INFO - training.closure - iteration 653: loss = -11.205705941612099
2023-04-09 11:42:43,728 - INFO - training.closure - iteration 654: loss = -11.236741190255461
2023-04-09 11:42:46,329 - INFO - training.closure - iteration 655: loss = -11.281973194258967
2023-04-09 11:42:49,048 - INFO - training.closure - iteration 656: loss = -11.32151146440955
2023-04-09 11:42:51,770 - INFO - training.closure - iteration 657: loss = -11.261399365835617
2023-04-09 11:42:54,469 - INFO - training.closure - iteration 658: loss = -11.339035047011333
2023-04-09 11:42:57,102 - INFO - training.closure - iteration 659: loss = -11.35977790526669
2023-04-09 11:42:59,707 - INFO - training.closure - iteration 660: loss = -10.944570789705047
2023-04-09 11:43:02,218 - INFO - training.closure - iteration 661: loss = -11.373062241880529
2023-04-09 11:43:04,744 - INFO - training.closure - iteration 662: loss = -11.368078722282629
2023-04-09 11:43:07,298 - INFO - training.closure - iteration 663: loss = -11.38323312666595
2023-04-09 11:43:09,898 - INFO - training.closure - iteration 664: loss = -11.385593565106188
2023-04-09 11:43:12,466 - INFO - training.closure - iteration 665: loss = -11.39236512035475
2023-04-09 11:43:14,962 - INFO - training.closure - iteration 666: loss = -11.396034310687206
2023-04-09 11:43:17,496 - INFO - training.closure - iteration 667: loss = -11.400909392957823
2023-04-09 11:43:20,121 - INFO - training.closure - iteration 668: loss = -11.402855590424533
2023-04-09 11:43:22,664 - INFO - training.closure - iteration 669: loss = -11.40428578258743
2023-04-09 11:43:25,222 - INFO - training.closure - iteration 670: loss = -11.405730457395995
2023-04-09 11:43:27,820 - INFO - training.closure - iteration 671: loss = -11.411795299552967
2023-04-09 11:43:30,392 - INFO - training.closure - iteration 672: loss = -11.420710849789552
2023-04-09 11:43:33,043 - INFO - training.closure - iteration 673: loss = -10.890529857468147
2023-04-09 11:43:35,557 - INFO - training.closure - iteration 674: loss = -11.423638668867852
2023-04-09 11:43:38,160 - INFO - training.closure - iteration 675: loss = -11.43309970189748
2023-04-09 11:43:40,741 - INFO - training.closure - iteration 676: loss = -11.441884499228824
2023-04-09 11:43:43,301 - INFO - training.closure - iteration 677: loss = -11.44566169997842
2023-04-09 11:43:45,805 - INFO - training.closure - iteration 678: loss = -11.447777864894167
2023-04-09 11:43:48,380 - INFO - training.closure - iteration 679: loss = -11.44951477304647
2023-04-09 11:43:50,998 - INFO - training.closure - iteration 680: loss = -11.451908763239452
2023-04-09 11:43:53,477 - INFO - training.closure - iteration 681: loss = -11.453924160341424
2023-04-09 11:43:56,000 - INFO - training.closure - iteration 682: loss = -11.456139506596772
2023-04-09 11:43:58,597 - INFO - training.closure - iteration 683: loss = -11.461742615821118
2023-04-09 11:44:01,162 - INFO - training.closure - iteration 684: loss = -11.468694116815982
2023-04-09 11:44:03,684 - INFO - training.closure - iteration 685: loss = -11.47715804718954
2023-04-09 11:44:06,274 - INFO - training.closure - iteration 686: loss = -11.479368110615003
2023-04-09 11:44:08,848 - INFO - training.closure - iteration 687: loss = -11.492792515118884
2023-04-09 11:44:11,463 - INFO - training.closure - iteration 688: loss = -11.496777605321977
2023-04-09 11:44:14,086 - INFO - training.closure - iteration 689: loss = -11.498955314344958
2023-04-09 11:44:16,634 - INFO - training.closure - iteration 690: loss = -11.5000386117322
2023-04-09 11:44:19,202 - INFO - training.closure - iteration 691: loss = -11.500912357749183
2023-04-09 11:44:21,834 - INFO - training.closure - iteration 692: loss = -11.501215861119839
2023-04-09 11:44:24,408 - INFO - training.closure - iteration 693: loss = -11.503267498121845
2023-04-09 11:44:26,923 - INFO - training.closure - iteration 694: loss = -11.505073642994923
2023-04-09 11:44:29,489 - INFO - training.closure - iteration 695: loss = -11.506528925803284
2023-04-09 11:44:31,976 - INFO - training.closure - iteration 696: loss = -11.50798851348712
2023-04-09 11:44:34,523 - INFO - training.closure - iteration 697: loss = -11.510173970096382
2023-04-09 11:44:37,001 - INFO - training.closure - iteration 698: loss = -11.507797501779827
2023-04-09 11:44:39,567 - INFO - training.closure - iteration 699: loss = -11.511290539884104
2023-04-09 11:44:42,149 - INFO - training.closure - iteration 700: loss = -11.513980504608508
2023-04-09 11:44:44,640 - INFO - training.closure - iteration 701: loss = -11.517404560013434
2023-04-09 11:44:47,257 - INFO - training.closure - iteration 702: loss = -11.5199071998892
2023-04-09 11:44:49,790 - INFO - training.closure - iteration 703: loss = -11.521227502698743
2023-04-09 11:44:52,300 - INFO - training.closure - iteration 704: loss = -11.522777306607303
2023-04-09 11:44:54,930 - INFO - training.closure - iteration 705: loss = -11.527730060518355
2023-04-09 11:44:57,479 - INFO - training.closure - iteration 706: loss = -11.53385034144697
2023-04-09 11:45:00,005 - INFO - training.closure - iteration 707: loss = -11.527943158235072
2023-04-09 11:45:02,507 - INFO - training.closure - iteration 708: loss = -11.536573391048961
2023-04-09 11:45:05,104 - INFO - training.closure - iteration 709: loss = -11.542208937354465
2023-04-09 11:45:07,663 - INFO - training.closure - iteration 710: loss = -11.545720263448231
2023-04-09 11:45:10,291 - INFO - training.closure - iteration 711: loss = -11.548398132307819
2023-04-09 11:45:12,833 - INFO - training.closure - iteration 712: loss = -11.550688399718545
2023-04-09 11:45:15,421 - INFO - training.closure - iteration 713: loss = -11.555418818685325
2023-04-09 11:45:17,975 - INFO - training.closure - iteration 714: loss = -11.549522842786068
2023-04-09 11:45:20,532 - INFO - training.closure - iteration 715: loss = -11.558159296853654
2023-04-09 11:45:23,092 - INFO - training.closure - iteration 716: loss = -11.542578208693566
2023-04-09 11:45:25,682 - INFO - training.closure - iteration 717: loss = -11.561469228100744
2023-04-09 11:45:28,165 - INFO - training.closure - iteration 718: loss = -11.569465075648392
2023-04-09 11:45:30,793 - INFO - training.closure - iteration 719: loss = -11.576037455889331
2023-04-09 11:45:33,372 - INFO - training.closure - iteration 720: loss = -11.57801497447331
2023-04-09 11:45:36,000 - INFO - training.closure - iteration 721: loss = -11.57923749753662
2023-04-09 11:45:38,553 - INFO - training.closure - iteration 722: loss = -11.57989289994822
2023-04-09 11:45:41,116 - INFO - training.closure - iteration 723: loss = -11.580688384584015
2023-04-09 11:45:43,689 - INFO - training.closure - iteration 724: loss = -11.535258888074543
2023-04-09 11:45:46,289 - INFO - training.closure - iteration 725: loss = -11.581223678656418
2023-04-09 11:45:48,817 - INFO - training.closure - iteration 726: loss = -11.582511788422881
2023-04-09 11:45:51,479 - INFO - training.closure - iteration 727: loss = -11.582570738344984
2023-04-09 11:45:54,112 - INFO - training.closure - iteration 728: loss = -11.582895652327348
2023-04-09 11:45:56,703 - INFO - training.closure - iteration 729: loss = -11.58383303679593
2023-04-09 11:45:59,400 - INFO - training.closure - iteration 730: loss = -11.586021575212737
2023-04-09 11:46:02,019 - INFO - training.closure - iteration 731: loss = -11.588077459666717
2023-04-09 11:46:04,587 - INFO - training.closure - iteration 732: loss = -11.591503997095256
2023-04-09 11:46:07,062 - INFO - training.closure - iteration 733: loss = -11.594588298613612
2023-04-09 11:46:09,628 - INFO - training.closure - iteration 734: loss = -11.551456444012205
2023-04-09 11:46:12,157 - INFO - training.closure - iteration 735: loss = -11.595561449009896
2023-04-09 11:46:14,732 - INFO - training.closure - iteration 736: loss = -11.59716010418306
2023-04-09 11:46:17,307 - INFO - training.closure - iteration 737: loss = -11.598613233704473
2023-04-09 11:46:19,941 - INFO - training.closure - iteration 738: loss = -11.600179431670867
2023-04-09 11:46:22,548 - INFO - training.closure - iteration 739: loss = -11.602022719887989
2023-04-09 11:46:25,242 - INFO - training.closure - iteration 740: loss = -11.603568552980999
2023-04-09 11:46:27,791 - INFO - training.closure - iteration 741: loss = -11.604372025045155
2023-04-09 11:46:30,429 - INFO - training.closure - iteration 742: loss = -11.604665273649879
2023-04-09 11:46:33,020 - INFO - training.closure - iteration 743: loss = -11.604815186433065
2023-04-09 11:46:35,571 - INFO - training.closure - iteration 744: loss = -11.60510413341343
2023-04-09 11:46:38,023 - INFO - training.closure - iteration 745: loss = -11.581693796706453
2023-04-09 11:46:40,611 - INFO - training.closure - iteration 746: loss = -11.605335692400631
2023-04-09 11:46:43,169 - INFO - training.closure - iteration 747: loss = -11.606118082889612
2023-04-09 11:46:45,683 - INFO - training.closure - iteration 748: loss = -11.60784291093862
2023-04-09 11:46:48,306 - INFO - training.closure - iteration 749: loss = -11.610007265531847
2023-04-09 11:46:50,854 - INFO - training.closure - iteration 750: loss = -11.61244281154479
2023-04-09 11:46:53,403 - INFO - training.closure - iteration 751: loss = -11.615143537893093
2023-04-09 11:46:56,034 - INFO - training.closure - iteration 752: loss = -11.616892764765964
2023-04-09 11:46:58,656 - INFO - training.closure - iteration 753: loss = -11.61771583034799
2023-04-09 11:47:01,194 - INFO - training.closure - iteration 754: loss = -11.618300872847144
2023-04-09 11:47:03,783 - INFO - training.closure - iteration 755: loss = -11.618828797718995
2023-04-09 11:47:06,354 - INFO - training.closure - iteration 756: loss = -11.61914183687608
2023-04-09 11:47:08,824 - INFO - training.closure - iteration 757: loss = -11.619707089739032
2023-04-09 11:47:11,390 - INFO - training.closure - iteration 758: loss = -11.61854789159391
2023-04-09 11:47:14,007 - INFO - training.closure - iteration 759: loss = -11.620017962527118
2023-04-09 11:47:16,533 - INFO - training.closure - iteration 760: loss = -11.621130158770832
2023-04-09 11:47:19,059 - INFO - training.closure - iteration 761: loss = -11.622657732464397
2023-04-09 11:47:21,877 - INFO - training.closure - iteration 762: loss = -11.62473156526934
2023-04-09 11:47:24,434 - INFO - training.closure - iteration 763: loss = -11.627565231165875
2023-04-09 11:47:26,929 - INFO - training.closure - iteration 764: loss = -11.620632755984634
2023-04-09 11:47:29,510 - INFO - training.closure - iteration 765: loss = -11.62953107766737
2023-04-09 11:47:32,124 - INFO - training.closure - iteration 766: loss = -11.632511499678603
2023-04-09 11:47:34,650 - INFO - training.closure - iteration 767: loss = -11.634582286510774
2023-04-09 11:47:37,315 - INFO - training.closure - iteration 768: loss = -11.6361437600888
2023-04-09 11:47:39,828 - INFO - training.closure - iteration 769: loss = -11.638739349332674
2023-04-09 11:47:42,363 - INFO - training.closure - iteration 770: loss = -11.63903890911985
2023-04-09 11:47:44,863 - INFO - training.closure - iteration 771: loss = -11.642334452221245
2023-04-09 11:47:47,344 - INFO - training.closure - iteration 772: loss = -11.644516947636184
2023-04-09 11:47:49,883 - INFO - training.closure - iteration 773: loss = -11.646854556208112
2023-04-09 11:47:52,340 - INFO - training.closure - iteration 774: loss = -11.64839909238404
2023-04-09 11:47:54,934 - INFO - training.closure - iteration 775: loss = -11.65009376216678
2023-04-09 11:47:57,536 - INFO - training.closure - iteration 776: loss = -11.651330270344463
2023-04-09 11:48:00,099 - INFO - training.closure - iteration 777: loss = -11.653191961653377
2023-04-09 11:48:02,666 - INFO - training.closure - iteration 778: loss = -11.655234924680634
2023-04-09 11:48:05,266 - INFO - training.closure - iteration 779: loss = -11.660255906521394
2023-04-09 11:48:07,731 - INFO - training.closure - iteration 780: loss = -11.665083392422298
2023-04-09 11:48:10,265 - INFO - training.closure - iteration 781: loss = -11.680497808359299
2023-04-09 11:48:12,740 - INFO - training.closure - iteration 782: loss = -11.693272099901783
2023-04-09 11:48:15,313 - INFO - training.closure - iteration 783: loss = -11.690623195891357
2023-04-09 11:48:17,870 - INFO - training.closure - iteration 784: loss = -11.696006045527671
2023-04-09 11:48:20,428 - INFO - training.closure - iteration 785: loss = -11.701507715667569
2023-04-09 11:48:22,982 - INFO - training.closure - iteration 786: loss = -11.708515931229753
2023-04-09 11:48:25,532 - INFO - training.closure - iteration 787: loss = -11.715529558217089
2023-04-09 11:48:28,084 - INFO - training.closure - iteration 788: loss = -11.722890046798538
2023-04-09 11:48:30,661 - INFO - training.closure - iteration 789: loss = -11.725970156817347
2023-04-09 11:48:33,195 - INFO - training.closure - iteration 790: loss = -11.737420841571456
2023-04-09 11:48:35,740 - INFO - training.closure - iteration 791: loss = -11.743757433152965
2023-04-09 11:48:38,299 - INFO - training.closure - iteration 792: loss = -0.12248049028864205
2023-04-09 11:48:40,790 - INFO - training.closure - iteration 793: loss = -11.340119524996965
2023-04-09 11:48:43,321 - INFO - training.closure - iteration 794: loss = -11.743709097564587
2023-04-09 11:48:45,882 - INFO - training.closure - iteration 795: loss = -11.747705423135908
2023-04-09 11:48:48,356 - INFO - training.closure - iteration 796: loss = -11.751861956832927
2023-04-09 11:48:50,973 - INFO - training.closure - iteration 797: loss = -11.754106987643203
2023-04-09 11:48:53,537 - INFO - training.closure - iteration 798: loss = -11.755080484263928
2023-04-09 11:48:56,058 - INFO - training.closure - iteration 799: loss = -11.755843325847675
2023-04-09 11:48:58,681 - INFO - training.closure - iteration 800: loss = -11.757392686391594
2023-04-09 11:49:01,196 - INFO - training.closure - iteration 801: loss = -11.7585203320427
2023-04-09 11:49:03,723 - INFO - training.closure - iteration 802: loss = -11.760411103924858
2023-04-09 11:49:06,338 - INFO - training.closure - iteration 803: loss = -11.764818187247874
2023-04-09 11:49:08,919 - INFO - training.closure - iteration 804: loss = -11.769695900982253
2023-04-09 11:49:11,411 - INFO - training.closure - iteration 805: loss = -11.773103805683697
2023-04-09 11:49:14,003 - INFO - training.closure - iteration 806: loss = -11.776154737111863
2023-04-09 11:49:16,598 - INFO - training.closure - iteration 807: loss = -11.777733656455586
2023-04-09 11:49:19,109 - INFO - training.closure - iteration 808: loss = -11.780406325432033
2023-04-09 11:49:21,670 - INFO - training.closure - iteration 809: loss = -11.783786379025802
2023-04-09 11:49:24,273 - INFO - training.closure - iteration 810: loss = -11.785551733984551
2023-04-09 11:49:26,885 - INFO - training.closure - iteration 811: loss = -11.785838404343089
2023-04-09 11:49:29,384 - INFO - training.closure - iteration 812: loss = -11.786246360484267
2023-04-09 11:49:31,881 - INFO - training.closure - iteration 813: loss = -11.787251317112212
2023-04-09 11:49:34,415 - INFO - training.closure - iteration 814: loss = -11.788776424966985
2023-04-09 11:49:36,918 - INFO - training.closure - iteration 815: loss = -11.78936058988237
2023-04-09 11:49:39,535 - INFO - training.closure - iteration 816: loss = -11.789576785506139
2023-04-09 11:49:42,051 - INFO - training.closure - iteration 817: loss = -11.790058115873446
2023-04-09 11:49:44,566 - INFO - training.closure - iteration 818: loss = -11.79047724601322
2023-04-09 11:49:47,083 - INFO - training.closure - iteration 819: loss = -11.791078593596492
2023-04-09 11:49:49,625 - INFO - training.closure - iteration 820: loss = -11.792203972017342
2023-04-09 11:49:52,156 - INFO - training.closure - iteration 821: loss = -11.792786862186077
2023-04-09 11:49:54,719 - INFO - training.closure - iteration 822: loss = -11.793368051859515
2023-04-09 11:49:57,246 - INFO - training.closure - iteration 823: loss = -11.793831017756098
2023-04-09 11:49:59,794 - INFO - training.closure - iteration 824: loss = -11.794266372294263
2023-04-09 11:50:02,344 - INFO - training.closure - iteration 825: loss = -11.794536472818002
2023-04-09 11:50:04,892 - INFO - training.closure - iteration 826: loss = -11.790107057703029
2023-04-09 11:50:07,444 - INFO - training.closure - iteration 827: loss = -11.794780344884005
2023-04-09 11:50:09,945 - INFO - training.closure - iteration 828: loss = -11.795000888855533
2023-04-09 11:50:12,483 - INFO - training.closure - iteration 829: loss = -11.795007340394065
2023-04-09 11:50:15,064 - INFO - training.closure - iteration 830: loss = -11.795053715298767
2023-04-09 11:50:17,593 - INFO - training.closure - iteration 831: loss = -11.795080445269186
2023-04-09 11:50:20,151 - INFO - training.closure - iteration 832: loss = -11.795262938355942
2023-04-09 11:50:22,701 - INFO - training.closure - iteration 833: loss = -11.795459413634994
2023-04-09 11:50:25,201 - INFO - training.closure - iteration 834: loss = -11.7959318367375
2023-04-09 11:50:27,812 - INFO - training.closure - iteration 835: loss = -11.796162126004969
2023-04-09 11:50:30,398 - INFO - training.closure - iteration 836: loss = -11.796735648272232
2023-04-09 11:50:32,874 - INFO - training.closure - iteration 837: loss = -11.797280843456377
2023-04-09 11:50:35,489 - INFO - training.closure - iteration 838: loss = -11.797633383224596
2023-04-09 11:50:38,036 - INFO - training.closure - iteration 839: loss = -11.79785366150999
2023-04-09 11:50:40,585 - INFO - training.closure - iteration 840: loss = -11.798086690357684
2023-04-09 11:50:43,127 - INFO - training.closure - iteration 841: loss = -11.798224606283377
2023-04-09 11:50:45,658 - INFO - training.closure - iteration 842: loss = -11.798318704560831
2023-04-09 11:50:48,188 - INFO - training.closure - iteration 843: loss = -11.798785441934466
2023-04-09 11:50:50,851 - INFO - training.closure - iteration 844: loss = -11.799235570050818
2023-04-09 11:50:53,415 - INFO - training.closure - iteration 845: loss = -11.799875905289849
2023-04-09 11:50:56,047 - INFO - training.closure - iteration 846: loss = -11.800154504637668
2023-04-09 11:50:58,629 - INFO - training.closure - iteration 847: loss = -11.800742289575018
2023-04-09 11:51:01,202 - INFO - training.closure - iteration 848: loss = -11.79995793651294
2023-04-09 11:51:03,759 - INFO - training.closure - iteration 849: loss = -11.800863782560562
2023-04-09 11:51:06,409 - INFO - training.closure - iteration 850: loss = -11.800982195891525
2023-04-09 11:51:09,022 - INFO - training.closure - iteration 851: loss = -11.801451728225732
2023-04-09 11:51:11,585 - INFO - training.closure - iteration 852: loss = -11.801895004740572
2023-04-09 11:51:14,182 - INFO - training.closure - iteration 853: loss = -11.756528364203
2023-04-09 11:51:16,754 - INFO - training.closure - iteration 854: loss = -11.802441945497026
2023-04-09 11:51:19,303 - INFO - training.closure - iteration 855: loss = -11.804542299018753
2023-04-09 11:51:21,936 - INFO - training.closure - iteration 856: loss = -11.80765058554807
2023-04-09 11:51:24,450 - INFO - training.closure - iteration 857: loss = -11.81095812883212
2023-04-09 11:51:27,017 - INFO - training.closure - iteration 858: loss = -11.808926353681947
2023-04-09 11:51:29,611 - INFO - training.closure - iteration 859: loss = -11.813609821440064
2023-04-09 11:51:32,161 - INFO - training.closure - iteration 860: loss = -11.816722298578426
2023-04-09 11:51:34,668 - INFO - training.closure - iteration 861: loss = -11.818445822474132
2023-04-09 11:51:37,319 - INFO - training.closure - iteration 862: loss = -11.816179389509632
2023-04-09 11:51:39,964 - INFO - training.closure - iteration 863: loss = -11.819519009876196
2023-04-09 11:51:42,606 - INFO - training.closure - iteration 864: loss = -11.821128807102173
2023-04-09 11:51:45,235 - INFO - training.closure - iteration 865: loss = -11.823740594205177
2023-04-09 11:51:47,865 - INFO - training.closure - iteration 866: loss = -11.825328540910151
2023-04-09 11:51:50,438 - INFO - training.closure - iteration 867: loss = -11.827621088101681
2023-04-09 11:51:52,946 - INFO - training.closure - iteration 868: loss = -11.830328414236558
2023-04-09 11:51:55,583 - INFO - training.closure - iteration 869: loss = -11.834942906498817
2023-04-09 11:51:58,138 - INFO - training.closure - iteration 870: loss = -11.83789097393308
2023-04-09 11:52:00,789 - INFO - training.closure - iteration 871: loss = -11.833077660426195
2023-04-09 11:52:03,421 - INFO - training.closure - iteration 872: loss = -11.840044551485015
2023-04-09 11:52:06,102 - INFO - training.closure - iteration 873: loss = -11.842689438755343
2023-04-09 11:52:08,652 - INFO - training.closure - iteration 874: loss = -11.84530602766455
2023-04-09 11:52:11,307 - INFO - training.closure - iteration 875: loss = -11.847240341708943
2023-04-09 11:52:13,846 - INFO - training.closure - iteration 876: loss = -11.84831690085203
2023-04-09 11:52:16,321 - INFO - training.closure - iteration 877: loss = -11.846940628960684
2023-04-09 11:52:18,842 - INFO - training.closure - iteration 878: loss = -11.849680666029993
2023-04-09 11:52:21,418 - INFO - training.closure - iteration 879: loss = -11.851055384878258
2023-04-09 11:52:23,936 - INFO - training.closure - iteration 880: loss = -11.855773357269037
2023-04-09 11:52:26,564 - INFO - training.closure - iteration 881: loss = -11.859585348220476
2023-04-09 11:52:29,227 - INFO - training.closure - iteration 882: loss = -11.86472830109623
2023-04-09 11:52:31,764 - INFO - training.closure - iteration 883: loss = -11.868205138329436
2023-04-09 11:52:34,315 - INFO - training.closure - iteration 884: loss = -11.852718085323598
2023-04-09 11:52:36,981 - INFO - training.closure - iteration 885: loss = -11.870882866617887
2023-04-09 11:52:39,611 - INFO - training.closure - iteration 886: loss = -11.874662606660998
2023-04-09 11:52:42,102 - INFO - training.closure - iteration 887: loss = -11.878120744651445
2023-04-09 11:52:44,699 - INFO - training.closure - iteration 888: loss = -11.880379575853752
2023-04-09 11:52:47,224 - INFO - training.closure - iteration 889: loss = -11.881323757106504
2023-04-09 11:52:49,743 - INFO - training.closure - iteration 890: loss = -11.881743970070563
2023-04-09 11:52:52,353 - INFO - training.closure - iteration 891: loss = -11.882707556146254
2023-04-09 11:52:55,062 - INFO - training.closure - iteration 892: loss = -11.883079835483558
2023-04-09 11:52:57,687 - INFO - training.closure - iteration 893: loss = -11.883954414121611
2023-04-09 11:53:00,235 - INFO - training.closure - iteration 894: loss = -11.884718065848407
2023-04-09 11:53:02,836 - INFO - training.closure - iteration 895: loss = -11.864133028480243
2023-04-09 11:53:05,392 - INFO - training.closure - iteration 896: loss = -11.885060328513362
2023-04-09 11:53:07,971 - INFO - training.closure - iteration 897: loss = -11.886233105799583
2023-04-09 11:53:10,542 - INFO - training.closure - iteration 898: loss = -11.888400832882844
2023-04-09 11:53:13,059 - INFO - training.closure - iteration 899: loss = -11.890991731752873
2023-04-09 11:53:15,655 - INFO - training.closure - iteration 900: loss = -11.893270611616849
2023-04-09 11:53:18,287 - INFO - training.closure - iteration 901: loss = -11.896426567564397
2023-04-09 11:53:20,888 - INFO - training.closure - iteration 902: loss = -11.899186357098444
2023-04-09 11:53:23,542 - INFO - training.closure - iteration 903: loss = -11.902249053794371
2023-04-09 11:53:26,174 - INFO - training.closure - iteration 904: loss = -11.89737433808764
2023-04-09 11:53:28,715 - INFO - training.closure - iteration 905: loss = -11.90470478657852
2023-04-09 11:53:31,248 - INFO - training.closure - iteration 906: loss = -11.90836495812897
2023-04-09 11:53:33,835 - INFO - training.closure - iteration 907: loss = -11.909884346105708
2023-04-09 11:53:36,365 - INFO - training.closure - iteration 908: loss = -11.912971548262503
2023-04-09 11:53:38,927 - INFO - training.closure - iteration 909: loss = -11.9141076082916
2023-04-09 11:53:41,536 - INFO - training.closure - iteration 910: loss = -11.918632821540438
2023-04-09 11:53:44,162 - INFO - training.closure - iteration 911: loss = -11.92247911505201
2023-04-09 11:53:46,688 - INFO - training.closure - iteration 912: loss = -11.905288105735815
2023-04-09 11:53:49,232 - INFO - training.closure - iteration 913: loss = -11.923106827160492
2023-04-09 11:53:51,785 - INFO - training.closure - iteration 914: loss = -11.878569432435542
2023-04-09 11:53:54,328 - INFO - training.closure - iteration 915: loss = -11.927821629415021
2023-04-09 11:53:56,900 - INFO - training.closure - iteration 916: loss = -11.927156762509863
2023-04-09 11:53:59,480 - INFO - training.closure - iteration 917: loss = -11.929605805141922
2023-04-09 11:54:02,025 - INFO - training.closure - iteration 918: loss = -11.931758472027395
2023-04-09 11:54:04,514 - INFO - training.closure - iteration 919: loss = -11.932583106918749
2023-04-09 11:54:07,160 - INFO - training.closure - iteration 920: loss = -11.93334383298777
2023-04-09 11:54:09,726 - INFO - training.closure - iteration 921: loss = -11.934092752761256
2023-04-09 11:54:12,236 - INFO - training.closure - iteration 922: loss = -11.935596646683113
2023-04-09 11:54:14,763 - INFO - training.closure - iteration 923: loss = -11.937179818029627
2023-04-09 11:54:17,296 - INFO - training.closure - iteration 924: loss = -11.939220841726627
2023-04-09 11:54:19,780 - INFO - training.closure - iteration 925: loss = -11.938284716988663
2023-04-09 11:54:22,377 - INFO - training.closure - iteration 926: loss = -11.940011331197383
2023-04-09 11:54:24,943 - INFO - training.closure - iteration 927: loss = -11.941524715309525
2023-04-09 11:54:27,472 - INFO - training.closure - iteration 928: loss = -11.942664162754905
2023-04-09 11:54:30,024 - INFO - training.closure - iteration 929: loss = -11.942344976863197
2023-04-09 11:54:32,711 - INFO - training.closure - iteration 930: loss = -11.943200386564332
2023-04-09 11:54:35,319 - INFO - training.closure - iteration 931: loss = -11.944327862564766
2023-04-09 11:54:37,874 - INFO - training.closure - iteration 932: loss = -11.945089688108768
2023-04-09 11:54:40,476 - INFO - training.closure - iteration 933: loss = -11.946694421050122
2023-04-09 11:54:43,100 - INFO - training.closure - iteration 934: loss = -11.948346027979365
2023-04-09 11:54:45,677 - INFO - training.closure - iteration 935: loss = -11.951515126264216
2023-04-09 11:54:48,285 - INFO - training.closure - iteration 936: loss = -11.957468987120585
2023-04-09 11:54:50,852 - INFO - training.closure - iteration 937: loss = -11.94722010914981
2023-04-09 11:54:53,310 - INFO - training.closure - iteration 938: loss = -11.958169897121472
2023-04-09 11:54:56,065 - INFO - training.closure - iteration 939: loss = -11.959976266163405
2023-04-09 11:54:58,835 - INFO - training.closure - iteration 940: loss = -11.960569106066078
2023-04-09 11:55:01,351 - INFO - training.closure - iteration 941: loss = -11.9608930264301
2023-04-09 11:55:03,926 - INFO - training.closure - iteration 942: loss = -11.961305801989267
2023-04-09 11:55:06,432 - INFO - training.closure - iteration 943: loss = -11.957260270075063
2023-04-09 11:55:09,062 - INFO - training.closure - iteration 944: loss = -11.961854782063579
2023-04-09 11:55:11,625 - INFO - training.closure - iteration 945: loss = -11.96286450680147
2023-04-09 11:55:14,118 - INFO - training.closure - iteration 946: loss = -11.963532177731821
2023-04-09 11:55:16,651 - INFO - training.closure - iteration 947: loss = -11.962635214196288
2023-04-09 11:55:19,122 - INFO - training.closure - iteration 948: loss = -11.964076521207296
2023-04-09 11:55:21,726 - INFO - training.closure - iteration 949: loss = -11.964350420992375
2023-04-09 11:55:24,290 - INFO - training.closure - iteration 950: loss = -11.964446448760743
2023-04-09 11:55:26,789 - INFO - training.closure - iteration 951: loss = -11.964533056058134
2023-04-09 11:55:29,321 - INFO - training.closure - iteration 952: loss = -11.96460728877987
2023-04-09 11:55:31,907 - INFO - training.closure - iteration 953: loss = -11.96333186875244
2023-04-09 11:55:34,494 - INFO - training.closure - iteration 954: loss = -11.964651221480514
2023-04-09 11:55:37,073 - INFO - training.closure - iteration 955: loss = -11.96289588423295
2023-04-09 11:55:39,575 - INFO - training.closure - iteration 956: loss = -11.964755511407457
2023-04-09 11:55:42,093 - INFO - training.closure - iteration 957: loss = -11.964899670953379
2023-04-09 11:55:44,739 - INFO - training.closure - iteration 958: loss = -11.965751270026365
2023-04-09 11:55:47,252 - INFO - training.closure - iteration 959: loss = -11.966863880064185
2023-04-09 11:55:49,880 - INFO - training.closure - iteration 960: loss = -11.968126279164116
2023-04-09 11:55:52,432 - INFO - training.closure - iteration 961: loss = -11.969319079449555
2023-04-09 11:55:54,959 - INFO - training.closure - iteration 962: loss = -11.969970257493658
2023-04-09 11:55:57,429 - INFO - training.closure - iteration 963: loss = -11.9700777959601
2023-04-09 11:55:59,999 - INFO - training.closure - iteration 964: loss = -11.97021227737366
2023-04-09 11:56:02,582 - INFO - training.closure - iteration 965: loss = -11.970284209912547
2023-04-09 11:56:05,137 - INFO - training.closure - iteration 966: loss = -11.970338974823061
2023-04-09 11:56:07,605 - INFO - training.closure - iteration 967: loss = -11.970330827950534
2023-04-09 11:56:10,255 - INFO - training.closure - iteration 968: loss = -11.970361021492987
2023-04-09 11:56:12,781 - INFO - training.closure - iteration 969: loss = -11.97039617154943
2023-04-09 11:56:15,353 - INFO - training.closure - iteration 970: loss = -11.970457508015466
2023-04-09 11:56:17,925 - INFO - training.closure - iteration 971: loss = -11.970589247220651
2023-04-09 11:56:20,474 - INFO - training.closure - iteration 972: loss = -11.970774324244378
2023-04-09 11:56:22,955 - INFO - training.closure - iteration 973: loss = -11.970927012829767
2023-04-09 11:56:25,503 - INFO - training.closure - iteration 974: loss = -11.971094735465854
2023-04-09 11:56:28,059 - INFO - training.closure - iteration 975: loss = -11.971468957873421
2023-04-09 11:56:30,696 - INFO - training.closure - iteration 976: loss = -11.97136610749473
2023-04-09 11:56:33,320 - INFO - training.closure - iteration 977: loss = -11.971658045295042
2023-04-09 11:56:35,863 - INFO - training.closure - iteration 978: loss = -11.971977690086193
2023-04-09 11:56:38,423 - INFO - training.closure - iteration 979: loss = -11.972409537760406
2023-04-09 11:56:40,927 - INFO - training.closure - iteration 980: loss = -11.972477028211152
2023-04-09 11:56:43,469 - INFO - training.closure - iteration 981: loss = -11.97251185441695
2023-04-09 11:56:45,958 - INFO - training.closure - iteration 982: loss = -11.972559891109121
2023-04-09 11:56:48,429 - INFO - training.closure - iteration 983: loss = -11.97257886831674
2023-04-09 11:56:51,015 - INFO - training.closure - iteration 984: loss = -11.972622671931246
2023-04-09 11:56:53,577 - INFO - training.closure - iteration 985: loss = -11.972661256541942
2023-04-09 11:56:56,182 - INFO - training.closure - iteration 986: loss = -11.972537976858822
2023-04-09 11:56:58,834 - INFO - training.closure - iteration 987: loss = -11.97267170323962
2023-04-09 11:57:01,319 - INFO - training.closure - iteration 988: loss = -11.972708553257771
2023-04-09 11:57:03,864 - INFO - training.closure - iteration 989: loss = -11.972738137174556
2023-04-09 11:57:06,415 - INFO - training.closure - iteration 990: loss = -11.972770534093012
2023-04-09 11:57:08,975 - INFO - training.closure - iteration 991: loss = -11.972836635290456
2023-04-09 11:57:11,485 - INFO - training.closure - iteration 992: loss = -11.972885054501743
2023-04-09 11:57:13,940 - INFO - training.closure - iteration 993: loss = -11.972935260672092
2023-04-09 11:57:16,522 - INFO - training.closure - iteration 994: loss = -11.972997279780452
2023-04-09 11:57:19,054 - INFO - training.closure - iteration 995: loss = -11.973072352366678
2023-04-09 11:57:21,600 - INFO - training.closure - iteration 996: loss = -11.973176828404332
2023-04-09 11:57:24,169 - INFO - training.closure - iteration 997: loss = -11.97204601685015
2023-04-09 11:57:26,692 - INFO - training.closure - iteration 998: loss = -11.973192435470757
2023-04-09 11:57:29,234 - INFO - training.closure - iteration 999: loss = -11.973299674263963
2023-04-09 11:57:31,838 - INFO - training.closure - iteration 1000: loss = -11.97332934848168
2023-04-09 11:57:34,340 - INFO - training.closure - iteration 1001: loss = -11.973475215947738
2023-04-09 11:57:36,931 - INFO - training.closure - iteration 1002: loss = -11.973529520903845
2023-04-09 11:57:39,500 - INFO - training.closure - iteration 1003: loss = -11.973609900292168
2023-04-09 11:57:42,025 - INFO - training.closure - iteration 1004: loss = -11.973748321533881
2023-04-09 11:57:44,547 - INFO - training.closure - iteration 1005: loss = -11.974021459251485
2023-04-09 11:57:47,208 - INFO - training.closure - iteration 1006: loss = -11.974506056810252
2023-04-09 11:57:49,878 - INFO - training.closure - iteration 1007: loss = -11.974018220559989
2023-04-09 11:57:52,489 - INFO - training.closure - iteration 1008: loss = -11.974848020348924
2023-04-09 11:57:55,035 - INFO - training.closure - iteration 1009: loss = -11.975462525691793
2023-04-09 11:57:57,618 - INFO - training.closure - iteration 1010: loss = -11.976591180693871
2023-04-09 11:58:00,112 - INFO - training.closure - iteration 1011: loss = -11.978055815696163
2023-04-09 11:58:02,705 - INFO - training.closure - iteration 1012: loss = -11.978934699716396
2023-04-09 11:58:05,217 - INFO - training.closure - iteration 1013: loss = -11.979448065236465
2023-04-09 11:58:07,780 - INFO - training.closure - iteration 1014: loss = -11.979763012387465
2023-04-09 11:58:10,434 - INFO - training.closure - iteration 1015: loss = -11.98001667893468
2023-04-09 11:58:13,013 - INFO - training.closure - iteration 1016: loss = -11.98020968058401
2023-04-09 11:58:15,622 - INFO - training.closure - iteration 1017: loss = -11.980585702035118
2023-04-09 11:58:18,205 - INFO - training.closure - iteration 1018: loss = -11.981061221106966
2023-04-09 11:58:20,804 - INFO - training.closure - iteration 1019: loss = -11.981545222180216
2023-04-09 11:58:23,311 - INFO - training.closure - iteration 1020: loss = -11.981774768165064
2023-04-09 11:58:25,841 - INFO - training.closure - iteration 1021: loss = -11.981044094923657
2023-04-09 11:58:28,443 - INFO - training.closure - iteration 1022: loss = -11.982120177360965
2023-04-09 11:58:31,026 - INFO - training.closure - iteration 1023: loss = -11.982604235029502
2023-04-09 11:58:33,480 - INFO - training.closure - iteration 1024: loss = -11.98313790269712
2023-04-09 11:58:36,101 - INFO - training.closure - iteration 1025: loss = -11.921539833464564
2023-04-09 11:58:38,766 - INFO - training.closure - iteration 1026: loss = -11.983249855668038
2023-04-09 11:58:41,303 - INFO - training.closure - iteration 1027: loss = -11.983623015169005
2023-04-09 11:58:43,865 - INFO - training.closure - iteration 1028: loss = -11.983708229590016
2023-04-09 11:58:46,375 - INFO - training.closure - iteration 1029: loss = -11.983761870140134
2023-04-09 11:58:48,887 - INFO - training.closure - iteration 1030: loss = -11.984002763109741
2023-04-09 11:58:51,394 - INFO - training.closure - iteration 1031: loss = -11.984234147983617
2023-04-09 11:58:53,921 - INFO - training.closure - iteration 1032: loss = -11.98397226847779
2023-04-09 11:58:56,465 - INFO - training.closure - iteration 1033: loss = -11.984390876542506
2023-04-09 11:58:59,085 - INFO - training.closure - iteration 1034: loss = -11.984839766544251
2023-04-09 11:59:01,635 - INFO - training.closure - iteration 1035: loss = -11.985040239093571
2023-04-09 11:59:04,109 - INFO - training.closure - iteration 1036: loss = -11.98528357703984
2023-04-09 11:59:06,655 - INFO - training.closure - iteration 1037: loss = -11.985515643122188
2023-04-09 11:59:09,152 - INFO - training.closure - iteration 1038: loss = -11.986033610791463
2023-04-09 11:59:11,644 - INFO - training.closure - iteration 1039: loss = -11.986806257844592
2023-04-09 11:59:14,160 - INFO - training.closure - iteration 1040: loss = -11.985432573078885
2023-04-09 11:59:16,675 - INFO - training.closure - iteration 1041: loss = -11.987020866322972
2023-04-09 11:59:19,193 - INFO - training.closure - iteration 1042: loss = -11.987559169652723
2023-04-09 11:59:21,750 - INFO - training.closure - iteration 1043: loss = -11.988357245846235
2023-04-09 11:59:24,320 - INFO - training.closure - iteration 1044: loss = -11.989323576094977
2023-04-09 11:59:26,914 - INFO - training.closure - iteration 1045: loss = -11.990615486425666
2023-04-09 11:59:29,450 - INFO - training.closure - iteration 1046: loss = -11.993480442756075
2023-04-09 11:59:31,925 - INFO - training.closure - iteration 1047: loss = -11.99655904354969
2023-04-09 11:59:34,431 - INFO - training.closure - iteration 1048: loss = -11.998534642405616
2023-04-09 11:59:36,960 - INFO - training.closure - iteration 1049: loss = -11.998894959634189
2023-04-09 11:59:39,525 - INFO - training.closure - iteration 1050: loss = -11.999744665514672
2023-04-09 11:59:42,069 - INFO - training.closure - iteration 1051: loss = -12.000037186940334
2023-04-09 11:59:44,603 - INFO - training.closure - iteration 1052: loss = -12.0002004346545
2023-04-09 11:59:47,205 - INFO - training.closure - iteration 1053: loss = -12.000588984748585
2023-04-09 11:59:49,816 - INFO - training.closure - iteration 1054: loss = -12.000997579103304
2023-04-09 11:59:52,354 - INFO - training.closure - iteration 1055: loss = -11.994331234653336
2023-04-09 11:59:54,894 - INFO - training.closure - iteration 1056: loss = -12.001462988706216
2023-04-09 11:59:57,429 - INFO - training.closure - iteration 1057: loss = -12.001763391472357
2023-04-09 12:00:00,048 - INFO - training.closure - iteration 1058: loss = -12.002389810213684
2023-04-09 12:00:02,632 - INFO - training.closure - iteration 1059: loss = -12.002967083867986
2023-04-09 12:00:05,200 - INFO - training.closure - iteration 1060: loss = -12.004699740266599
2023-04-09 12:00:07,769 - INFO - training.closure - iteration 1061: loss = -12.006896216276266
2023-04-09 12:00:10,339 - INFO - training.closure - iteration 1062: loss = -12.009866979380003
2023-04-09 12:00:12,907 - INFO - training.closure - iteration 1063: loss = -12.012046782053353
2023-04-09 12:00:15,437 - INFO - training.closure - iteration 1064: loss = -12.015223859644216
2023-04-09 12:00:17,979 - INFO - training.closure - iteration 1065: loss = -12.016626111169511
2023-04-09 12:00:20,534 - INFO - training.closure - iteration 1066: loss = -12.002443789437981
2023-04-09 12:00:23,035 - INFO - training.closure - iteration 1067: loss = -12.017395840941873
2023-04-09 12:00:25,539 - INFO - training.closure - iteration 1068: loss = -12.018296102997809
2023-04-09 12:00:28,013 - INFO - training.closure - iteration 1069: loss = -12.018670716971958
2023-04-09 12:00:30,591 - INFO - training.closure - iteration 1070: loss = -12.018939692601736
2023-04-09 12:00:33,186 - INFO - training.closure - iteration 1071: loss = -12.01919743730723
2023-04-09 12:00:35,858 - INFO - training.closure - iteration 1072: loss = -12.018812718763053
2023-04-09 12:00:38,435 - INFO - training.closure - iteration 1073: loss = -12.01966187493099
2023-04-09 12:00:41,043 - INFO - training.closure - iteration 1074: loss = -11.95732323997672
2023-04-09 12:00:43,597 - INFO - training.closure - iteration 1075: loss = -12.020053013767942
2023-04-09 12:00:46,149 - INFO - training.closure - iteration 1076: loss = -12.020982573471972
2023-04-09 12:00:48,818 - INFO - training.closure - iteration 1077: loss = -12.01988124617145
2023-04-09 12:00:51,387 - INFO - training.closure - iteration 1078: loss = -12.021313933098634
2023-04-09 12:00:53,932 - INFO - training.closure - iteration 1079: loss = -12.02172406988576
2023-04-09 12:00:56,504 - INFO - training.closure - iteration 1080: loss = -12.02183768686529
2023-04-09 12:00:59,059 - INFO - training.closure - iteration 1081: loss = -12.022012094454695
2023-04-09 12:01:01,726 - INFO - training.closure - iteration 1082: loss = -12.022243570830621
2023-04-09 12:01:04,322 - INFO - training.closure - iteration 1083: loss = -12.022668251936704
2023-04-09 12:01:06,842 - INFO - training.closure - iteration 1084: loss = -12.023416904892702
2023-04-09 12:01:09,426 - INFO - training.closure - iteration 1085: loss = -12.022805897923732
2023-04-09 12:01:11,918 - INFO - training.closure - iteration 1086: loss = -12.024118731528965
2023-04-09 12:01:14,522 - INFO - training.closure - iteration 1087: loss = -12.025717574421165
2023-04-09 12:01:17,178 - INFO - training.closure - iteration 1088: loss = -12.026704553922418
2023-04-09 12:01:19,686 - INFO - training.closure - iteration 1089: loss = -12.028096392031118
2023-04-09 12:01:22,324 - INFO - training.closure - iteration 1090: loss = -12.02827563299003
2023-04-09 12:01:25,009 - INFO - training.closure - iteration 1091: loss = -12.028458221064856
2023-04-09 12:01:27,494 - INFO - training.closure - iteration 1092: loss = -12.02852264349838
2023-04-09 12:01:29,993 - INFO - training.closure - iteration 1093: loss = -12.028779276868534
2023-04-09 12:01:32,484 - INFO - training.closure - iteration 1094: loss = -12.027514445463215
2023-04-09 12:01:34,947 - INFO - training.closure - iteration 1095: loss = -12.02883790527114
2023-04-09 12:01:37,530 - INFO - training.closure - iteration 1096: loss = -12.028898492589239
2023-04-09 12:01:40,118 - INFO - training.closure - iteration 1097: loss = -12.029126898570972
2023-04-09 12:01:42,610 - INFO - training.closure - iteration 1098: loss = -12.029296197959708
2023-04-09 12:01:45,132 - INFO - training.closure - iteration 1099: loss = -12.029557616894216
2023-04-09 12:01:47,622 - INFO - training.closure - iteration 1100: loss = -12.029937231946985
2023-04-09 12:01:50,196 - INFO - training.closure - iteration 1101: loss = -12.030001747854204
2023-04-09 12:01:52,777 - INFO - training.closure - iteration 1102: loss = -12.030514886932844
2023-04-09 12:01:55,368 - INFO - training.closure - iteration 1103: loss = -12.030809303341023
2023-04-09 12:01:57,959 - INFO - training.closure - iteration 1104: loss = -12.030822086279581
2023-04-09 12:02:00,456 - INFO - training.closure - iteration 1105: loss = -12.031049867951758
2023-04-09 12:02:03,030 - INFO - training.closure - iteration 1106: loss = -12.028981049027529
2023-04-09 12:02:05,546 - INFO - training.closure - iteration 1107: loss = -12.031180164076453
2023-04-09 12:02:08,119 - INFO - training.closure - iteration 1108: loss = -12.03123335342444
2023-04-09 12:02:10,633 - INFO - training.closure - iteration 1109: loss = -12.031279801036465
2023-04-09 12:02:13,183 - INFO - training.closure - iteration 1110: loss = -12.031367956051294
2023-04-09 12:02:15,700 - INFO - training.closure - iteration 1111: loss = -12.03148126670185
2023-04-09 12:02:18,250 - INFO - training.closure - iteration 1112: loss = -12.031631077980684
2023-04-09 12:02:20,755 - INFO - training.closure - iteration 1113: loss = -12.031811631351044
2023-04-09 12:02:23,318 - INFO - training.closure - iteration 1114: loss = -12.03200591997622
2023-04-09 12:02:25,840 - INFO - training.closure - iteration 1115: loss = -12.032255723103436
2023-04-09 12:02:28,372 - INFO - training.closure - iteration 1116: loss = -12.032407105659598
2023-04-09 12:02:30,915 - INFO - training.closure - iteration 1117: loss = -12.032500167629662
2023-04-09 12:02:33,409 - INFO - training.closure - iteration 1118: loss = -12.032577464715107
2023-04-09 12:02:35,893 - INFO - training.closure - iteration 1119: loss = -12.032749469895391
2023-04-09 12:02:38,530 - INFO - training.closure - iteration 1120: loss = -12.032888756625557
2023-04-09 12:02:41,067 - INFO - training.closure - iteration 1121: loss = -12.033269445778956
2023-04-09 12:02:43,628 - INFO - training.closure - iteration 1122: loss = -12.033563219142614
2023-04-09 12:02:46,157 - INFO - training.closure - iteration 1123: loss = -12.033741070439401
2023-04-09 12:02:48,746 - INFO - training.closure - iteration 1124: loss = -12.034145758018425
2023-04-09 12:02:51,276 - INFO - training.closure - iteration 1125: loss = -12.034485660925133
2023-04-09 12:02:53,785 - INFO - training.closure - iteration 1126: loss = -12.034863183207673
2023-04-09 12:02:56,262 - INFO - training.closure - iteration 1127: loss = -12.035042944410892
2023-04-09 12:02:58,769 - INFO - training.closure - iteration 1128: loss = -12.035191359588007
2023-04-09 12:03:01,473 - INFO - training.closure - iteration 1129: loss = -12.03530218843543
2023-04-09 12:03:04,024 - INFO - training.closure - iteration 1130: loss = -12.035377272032886
2023-04-09 12:03:06,628 - INFO - training.closure - iteration 1131: loss = -12.035458835616055
2023-04-09 12:03:09,128 - INFO - training.closure - iteration 1132: loss = -12.035096417612827
2023-04-09 12:03:11,669 - INFO - training.closure - iteration 1133: loss = -12.035525991963246
2023-04-09 12:03:14,224 - INFO - training.closure - iteration 1134: loss = -12.035627497052348
2023-04-09 12:03:16,764 - INFO - training.closure - iteration 1135: loss = -12.035786317514368
2023-04-09 12:03:19,348 - INFO - training.closure - iteration 1136: loss = -12.0359174363162
2023-04-09 12:03:21,944 - INFO - training.closure - iteration 1137: loss = -12.036060175032022
2023-04-09 12:03:24,513 - INFO - training.closure - iteration 1138: loss = -12.036252556958011
2023-04-09 12:03:27,129 - INFO - training.closure - iteration 1139: loss = -12.036417005306204
2023-04-09 12:03:29,666 - INFO - training.closure - iteration 1140: loss = -12.036496121531751
2023-04-09 12:03:32,245 - INFO - training.closure - iteration 1141: loss = -12.036705506278643
2023-04-09 12:03:34,885 - INFO - training.closure - iteration 1142: loss = -12.036810050669207
2023-04-09 12:03:37,508 - INFO - training.closure - iteration 1143: loss = -12.036933578880888
2023-04-09 12:03:40,046 - INFO - training.closure - iteration 1144: loss = -12.037207404035689
2023-04-09 12:03:42,721 - INFO - training.closure - iteration 1145: loss = -12.036690284520919
2023-04-09 12:03:45,221 - INFO - training.closure - iteration 1146: loss = -12.037363473969187
2023-04-09 12:03:47,781 - INFO - training.closure - iteration 1147: loss = -12.037590244390877
2023-04-09 12:31:19,090 - INFO - main.experiment - train - RMSE_b at iteration 0 = 0.09468121353088971
2023-04-09 12:31:19,090 - INFO - main.experiment - train - RMSE_b at last iteration = 0.056200159883089865
2023-04-09 12:31:19,090 - INFO - main.experiment - train - RMSE_a at iteration 0 = 0.4337177321851836
2023-04-09 12:31:19,090 - INFO - main.experiment - train - RMSE_a at last iteration = 0.05190431376042479
2023-04-09 12:31:19,090 - INFO - main.experiment - train - LOGPDF_b at iteration 0 = 86.06819312962102
2023-04-09 12:31:19,090 - INFO - main.experiment - train - LOGPDF_b at last iteration = -2.9764894831045985
2023-04-09 12:31:19,090 - INFO - main.experiment - train - LOGPDF_a at iteration 0 = 3470252.248155119
2023-04-09 12:31:19,090 - INFO - main.experiment - train - LOGPDF_a at last iteration = -3.1154672715610228
2023-04-09 12:31:19,090 - INFO - main.experiment - train - LOSS at iteration 0 = 3470338.3163482486
2023-04-09 12:31:19,090 - INFO - main.experiment - train - LOSS at last iteration = -6.091956754665621
2023-04-09 12:31:56,435 - INFO - main.experiment - test - RMSE_b at iteration 0 = 0.09332845959227923
2023-04-09 12:31:56,435 - INFO - main.experiment - test - RMSE_b at last iteration = 0.13685414808781027
2023-04-09 12:31:56,435 - INFO - main.experiment - test - RMSE_a at iteration 0 = 0.050593275238031754
2023-04-09 12:31:56,436 - INFO - main.experiment - test - RMSE_a at last iteration = 0.05183239895663571
2023-04-09 12:31:56,436 - INFO - main.experiment - test - LOGPDF_b at iteration 0 = 10.523577757658856
2023-04-09 12:31:56,436 - INFO - main.experiment - test - LOGPDF_b at last iteration = 419.7605141693142
2023-04-09 12:31:56,436 - INFO - main.experiment - test - LOGPDF_a at iteration 0 = -3.115141754011752
2023-04-09 12:31:56,437 - INFO - main.experiment - test - LOGPDF_a at last iteration = 82.63792970416108
2023-04-09 12:31:56,437 - INFO - main.experiment - test - LOSS at iteration 0 = 7.408436003647104
2023-04-09 12:31:56,437 - INFO - main.experiment - test - LOSS at last iteration = 502.3984438734753
2023-04-08 21:31:08,164 - INFO - main.experiment - deep = 1 - plot = True - sigma0 = 0.1
2023-04-08 21:31:08,169 - INFO - training.pre_train_full - empirical mean of x0: 3.0003199843299018
2023-04-08 21:31:08,185 - INFO - training.pre_train_full - initial loss: 16.65545453835545
2023-04-08 21:31:08,208 - INFO - training.closure0 - iteration 0: loss = 16.65545453835545
2023-04-08 21:31:08,238 - INFO - training.closure0 - iteration 1: loss = 7.953230489636849
2023-04-08 21:31:08,258 - INFO - training.closure0 - iteration 2: loss = 6.258599819785809
2023-04-08 21:31:08,277 - INFO - training.closure0 - iteration 3: loss = 4.967239973030283
2023-04-08 21:31:08,293 - INFO - training.closure0 - iteration 4: loss = 4.371348870602114
2023-04-08 21:31:08,309 - INFO - training.closure0 - iteration 5: loss = 3.8416996988155456
2023-04-08 21:31:08,335 - INFO - training.closure0 - iteration 6: loss = 2.927597520137522
2023-04-08 21:31:08,357 - INFO - training.closure0 - iteration 7: loss = 7.854110303128189
2023-04-08 21:31:08,381 - INFO - training.closure0 - iteration 8: loss = 2.1522853630489203
2023-04-08 21:31:08,402 - INFO - training.closure0 - iteration 9: loss = 1.6867012881997416
2023-04-08 21:31:08,425 - INFO - training.closure0 - iteration 10: loss = 2.645570969132839
2023-04-08 21:31:08,447 - INFO - training.closure0 - iteration 11: loss = 1.678150627177355
2023-04-08 21:31:08,468 - INFO - training.closure0 - iteration 12: loss = 1.6884654622640476
2023-04-08 21:31:08,487 - INFO - training.closure0 - iteration 13: loss = 1.6294034777455315
2023-04-08 21:31:08,506 - INFO - training.closure0 - iteration 14: loss = 1.5977831396837463
2023-04-08 21:31:08,525 - INFO - training.closure0 - iteration 15: loss = 1.5069220585310752
2023-04-08 21:31:08,545 - INFO - training.closure0 - iteration 16: loss = 1.42313743080549
2023-04-08 21:31:08,564 - INFO - training.closure0 - iteration 17: loss = 3.372906191168302
2023-04-08 21:31:08,582 - INFO - training.closure0 - iteration 18: loss = 1.3608822190962617
2023-04-08 21:31:08,601 - INFO - training.closure0 - iteration 19: loss = 1.339020505827403
2023-04-08 21:31:08,620 - INFO - training.closure0 - iteration 20: loss = 1.240480609219392
2023-04-08 21:31:08,636 - INFO - training.closure0 - iteration 21: loss = 1.2160709778040952
2023-04-08 21:31:08,653 - INFO - training.closure0 - iteration 22: loss = 1.197758470629091
2023-04-08 21:31:08,670 - INFO - training.closure0 - iteration 23: loss = 1.1880375298277166
2023-04-08 21:31:08,687 - INFO - training.closure0 - iteration 24: loss = 1.1742685640107275
2023-04-08 21:31:08,702 - INFO - training.closure0 - iteration 25: loss = 1.143682531063587
2023-04-08 21:31:08,718 - INFO - training.closure0 - iteration 26: loss = 1.1064602152723002
2023-04-08 21:31:08,734 - INFO - training.closure0 - iteration 27: loss = 0.997873354903541
2023-04-08 21:31:08,749 - INFO - training.closure0 - iteration 28: loss = 0.5834426817835365
2023-04-08 21:31:08,765 - INFO - training.closure0 - iteration 29: loss = 0.9449261507395623
2023-04-08 21:31:08,782 - INFO - training.closure0 - iteration 30: loss = 0.31536685410075427
2023-04-08 21:31:08,802 - INFO - training.closure0 - iteration 31: loss = 1.0524138723554837
2023-04-08 21:31:08,821 - INFO - training.closure0 - iteration 32: loss = 0.22620130644152447
2023-04-08 21:31:08,841 - INFO - training.closure0 - iteration 33: loss = 3.8854074582292246
2023-04-08 21:31:08,860 - INFO - training.closure0 - iteration 34: loss = -0.058205156653654055
2023-04-08 21:31:08,878 - INFO - training.closure0 - iteration 35: loss = -0.2131424791240259
2023-04-08 21:31:08,897 - INFO - training.closure0 - iteration 36: loss = -0.4016850410233327
2023-04-08 21:31:08,913 - INFO - training.closure0 - iteration 37: loss = 12.164095580688628
2023-04-08 21:31:08,930 - INFO - training.closure0 - iteration 38: loss = -0.4902224574890533
2023-04-08 21:31:08,946 - INFO - training.closure0 - iteration 39: loss = -0.6502087450469525
2023-04-08 21:31:08,962 - INFO - training.closure0 - iteration 40: loss = 0.9860632753583514
2023-04-08 21:31:08,980 - INFO - training.closure0 - iteration 41: loss = -0.7125313525420105
2023-04-08 21:31:09,000 - INFO - training.closure0 - iteration 42: loss = -0.9045662047229777
2023-04-08 21:31:09,021 - INFO - training.closure0 - iteration 43: loss = -1.2373245127377979
2023-04-08 21:31:09,043 - INFO - training.closure0 - iteration 44: loss = -1.2669451631275712
2023-04-08 21:31:09,061 - INFO - training.closure0 - iteration 45: loss = -1.3130744186181413
2023-04-08 21:31:09,081 - INFO - training.closure0 - iteration 46: loss = -1.4312781897492215
2023-04-08 21:31:09,099 - INFO - training.closure0 - iteration 47: loss = -1.461945052855982
2023-04-08 21:31:09,114 - INFO - training.closure0 - iteration 48: loss = -1.488184797607491
2023-04-08 21:31:09,131 - INFO - training.closure0 - iteration 49: loss = -1.5484076632233472
2023-04-08 21:31:09,147 - INFO - training.closure0 - iteration 50: loss = -1.5918001459570918
2023-04-08 21:31:09,164 - INFO - training.closure0 - iteration 51: loss = -1.7266175151066356
2023-04-08 21:31:09,181 - INFO - training.closure0 - iteration 52: loss = -1.3829606871843168
2023-04-08 21:31:09,196 - INFO - training.closure0 - iteration 53: loss = -1.8014218224741385
2023-04-08 21:31:09,216 - INFO - training.closure0 - iteration 54: loss = -1.8368390539567154
2023-04-08 21:31:09,237 - INFO - training.closure0 - iteration 55: loss = -1.8505493899884213
2023-04-08 21:31:09,254 - INFO - training.closure0 - iteration 56: loss = -1.8516263738684557
2023-04-08 21:31:09,271 - INFO - training.closure0 - iteration 57: loss = -1.8517408547729117
2023-04-08 21:31:09,289 - INFO - training.closure0 - iteration 58: loss = -1.851741353093833
2023-04-08 21:31:09,305 - INFO - training.closure0 - iteration 59: loss = -1.8517414305190518
2023-04-08 21:31:09,321 - INFO - training.closure0 - iteration 60: loss = -1.851741455775189
2023-04-08 21:31:09,337 - INFO - training.closure0 - iteration 61: loss = -1.851741489630233
2023-04-08 21:31:09,353 - INFO - training.closure0 - iteration 62: loss = -1.851741489913621
2023-04-08 21:31:09,369 - INFO - training.closure0 - iteration 63: loss = -1.8517414899228657
2023-04-08 21:31:09,386 - INFO - training.closure0 - iteration 64: loss = -1.851741489923265
2023-04-08 21:31:09,402 - INFO - training.closure0 - iteration 65: loss = -1.8517414899232851
2023-04-08 21:31:09,410 - INFO - training.pre_train_full - a0 mean: [2.99442534 3.00621463]
2023-04-08 21:31:09,411 - INFO - training.pre_train_full - a0 var: [0.01004094 0.00844174]
2023-04-08 21:31:09,413 - INFO - training.pre_train_full - a0 covar: [[0.01004094158250142, -0.0005508687262899394], [-0.0005508687262899394, 0.008441744574349807]]
2023-04-08 21:32:23,048 - INFO - training.closure - iteration 0: loss = 4132.195406516611
2023-04-08 21:32:24,893 - INFO - training.closure - iteration 1: loss = 2202.8282489049125
2023-04-08 21:32:26,716 - INFO - training.closure - iteration 2: loss = 581.9623173506184
2023-04-08 21:32:28,616 - INFO - training.closure - iteration 3: loss = 462.9023426864906
2023-04-08 21:32:30,639 - INFO - training.closure - iteration 4: loss = 355.45843814189885
2023-04-08 21:32:32,516 - INFO - training.closure - iteration 5: loss = 333.5537113487003
2023-04-08 21:32:34,671 - INFO - training.closure - iteration 6: loss = 240.83606437013572
2023-04-08 21:32:36,709 - INFO - training.closure - iteration 7: loss = 182.72617732641098
2023-04-08 21:32:38,684 - INFO - training.closure - iteration 8: loss = 128.7105355482983
2023-04-08 21:32:40,572 - INFO - training.closure - iteration 9: loss = 93.91070525230789
2023-04-08 21:32:42,373 - INFO - training.closure - iteration 10: loss = 36.818709602528465
2023-04-08 21:32:44,084 - INFO - training.closure - iteration 11: loss = 15.552147895609563
2023-04-08 21:32:45,836 - INFO - training.closure - iteration 12: loss = 13.018957900017156
2023-04-08 21:32:47,539 - INFO - training.closure - iteration 13: loss = 10.06700016904968
2023-04-08 21:32:49,240 - INFO - training.closure - iteration 14: loss = 7.027199943989183
2023-04-08 21:32:51,029 - INFO - training.closure - iteration 15: loss = 5.607373903136326
2023-04-08 21:32:52,735 - INFO - training.closure - iteration 16: loss = 4.830493513005864
2023-04-08 21:32:54,443 - INFO - training.closure - iteration 17: loss = 4.197276124321463
2023-04-08 21:32:56,150 - INFO - training.closure - iteration 18: loss = 3.7001472458977034
2023-04-08 21:32:57,861 - INFO - training.closure - iteration 19: loss = 3.437870577700728
2023-04-08 21:32:59,558 - INFO - training.closure - iteration 20: loss = 3.0240308175111985
2023-04-08 21:33:01,265 - INFO - training.closure - iteration 21: loss = 2.81070363618017
2023-04-08 21:33:02,972 - INFO - training.closure - iteration 22: loss = 2.6987030888864365
2023-04-08 21:33:04,673 - INFO - training.closure - iteration 23: loss = 2.5851364549819484
2023-04-08 21:33:06,451 - INFO - training.closure - iteration 24: loss = 2.554676534263897
2023-04-08 21:33:08,381 - INFO - training.closure - iteration 25: loss = 2.5201181696743205
2023-04-08 21:33:10,124 - INFO - training.closure - iteration 26: loss = 2.500477902450807
2023-04-08 21:33:11,820 - INFO - training.closure - iteration 27: loss = 2.415517719760676
2023-04-08 21:33:13,540 - INFO - training.closure - iteration 28: loss = 2.3173386264929956
2023-04-08 21:33:15,408 - INFO - training.closure - iteration 29: loss = 2.077923074789674
2023-04-08 21:33:17,132 - INFO - training.closure - iteration 30: loss = 1.9368683284100907
2023-04-08 21:33:18,873 - INFO - training.closure - iteration 31: loss = 1.814144615535845
2023-04-08 21:33:20,592 - INFO - training.closure - iteration 32: loss = 1.72305084836933
2023-04-08 21:33:22,299 - INFO - training.closure - iteration 33: loss = 1.6866694053863651
2023-04-08 21:33:24,064 - INFO - training.closure - iteration 34: loss = 1.642791260210532
2023-04-08 21:33:25,861 - INFO - training.closure - iteration 35: loss = 1.3721584137206495
2023-04-08 21:33:27,596 - INFO - training.closure - iteration 36: loss = 1.0117222723330181
2023-04-08 21:33:29,318 - INFO - training.closure - iteration 37: loss = 0.7145534716931894
2023-04-08 21:33:31,019 - INFO - training.closure - iteration 38: loss = 0.5485748446740606
2023-04-08 21:33:32,708 - INFO - training.closure - iteration 39: loss = 0.4044477891643481
2023-04-08 21:33:34,433 - INFO - training.closure - iteration 40: loss = 0.8284892476597772
2023-04-08 21:33:36,239 - INFO - training.closure - iteration 41: loss = 0.23044298397845253
2023-04-08 21:33:37,951 - INFO - training.closure - iteration 42: loss = 0.04113870991583535
2023-04-08 21:33:39,690 - INFO - training.closure - iteration 43: loss = 0.06766878612397953
2023-04-08 21:33:41,545 - INFO - training.closure - iteration 44: loss = -0.039360009507380056
2023-04-08 21:33:43,255 - INFO - training.closure - iteration 45: loss = -0.0712174469169562
2023-04-08 21:33:45,162 - INFO - training.closure - iteration 46: loss = -0.3257047066695482
2023-04-08 21:33:46,933 - INFO - training.closure - iteration 47: loss = 2.2479808908003913
2023-04-08 21:33:48,650 - INFO - training.closure - iteration 48: loss = -0.3920484663397623
2023-04-08 21:33:50,368 - INFO - training.closure - iteration 49: loss = -0.5715339383719038
2023-04-08 21:33:52,100 - INFO - training.closure - iteration 50: loss = -0.8874504326772331
2023-04-08 21:33:53,811 - INFO - training.closure - iteration 51: loss = -1.111652038728687
2023-04-08 21:33:55,530 - INFO - training.closure - iteration 52: loss = -1.1089323765700492
2023-04-08 21:33:57,254 - INFO - training.closure - iteration 53: loss = -1.1679911102874745
2023-04-08 21:33:59,232 - INFO - training.closure - iteration 54: loss = -1.2187429864876276
2023-04-08 21:34:00,992 - INFO - training.closure - iteration 55: loss = -1.2556068890571501
2023-04-08 21:34:02,738 - INFO - training.closure - iteration 56: loss = -1.3191729698173331
2023-04-08 21:34:04,453 - INFO - training.closure - iteration 57: loss = -1.5207051631948594
2023-04-08 21:34:06,164 - INFO - training.closure - iteration 58: loss = -1.74498727519955
2023-04-08 21:34:07,869 - INFO - training.closure - iteration 59: loss = -1.3427110121948929
2023-04-08 21:34:09,685 - INFO - training.closure - iteration 60: loss = -1.9831650254730073
2023-04-08 21:34:11,389 - INFO - training.closure - iteration 61: loss = 7.022835286538214
2023-04-08 21:34:13,096 - INFO - training.closure - iteration 62: loss = -2.1766969754249907
2023-04-08 21:34:14,869 - INFO - training.closure - iteration 63: loss = -2.2894783311792617
2023-04-08 21:34:16,589 - INFO - training.closure - iteration 64: loss = -1.7846909042230787
2023-04-08 21:34:18,292 - INFO - training.closure - iteration 65: loss = -2.3918395461562545
2023-04-08 21:34:20,077 - INFO - training.closure - iteration 66: loss = -0.1864648841303458
2023-04-08 21:34:21,801 - INFO - training.closure - iteration 67: loss = -2.518530438648267
2023-04-08 21:34:23,666 - INFO - training.closure - iteration 68: loss = -2.3139754879774417
2023-04-08 21:34:25,384 - INFO - training.closure - iteration 69: loss = -2.6157452034919877
2023-04-08 21:34:27,205 - INFO - training.closure - iteration 70: loss = -2.7144044566326953
2023-04-08 21:34:28,986 - INFO - training.closure - iteration 71: loss = -2.7886747123026465
2023-04-08 21:34:30,743 - INFO - training.closure - iteration 72: loss = -2.8713362782023264
2023-04-08 21:34:32,578 - INFO - training.closure - iteration 73: loss = -2.899863194975607
2023-04-08 21:34:34,290 - INFO - training.closure - iteration 74: loss = -2.926374543683983
2023-04-08 21:34:36,160 - INFO - training.closure - iteration 75: loss = -2.9443463377454964
2023-04-08 21:34:37,905 - INFO - training.closure - iteration 76: loss = -2.9641514957663757
2023-04-08 21:34:39,610 - INFO - training.closure - iteration 77: loss = -3.012026577299239
2023-04-08 21:34:41,328 - INFO - training.closure - iteration 78: loss = -3.076786612954274
2023-04-08 21:34:43,051 - INFO - training.closure - iteration 79: loss = -3.164914336106998
2023-04-08 21:34:44,918 - INFO - training.closure - iteration 80: loss = -3.1958675167990966
2023-04-08 21:34:46,654 - INFO - training.closure - iteration 81: loss = -3.2216839217144253
2023-04-08 21:34:48,375 - INFO - training.closure - iteration 82: loss = -3.21650144827598
2023-04-08 21:34:50,086 - INFO - training.closure - iteration 83: loss = -3.226357733095158
2023-04-08 21:34:51,897 - INFO - training.closure - iteration 84: loss = -3.229350455614422
2023-04-08 21:34:53,638 - INFO - training.closure - iteration 85: loss = -3.235071751351713
2023-04-08 21:34:55,418 - INFO - training.closure - iteration 86: loss = -3.265394679335425
2023-04-08 21:34:57,144 - INFO - training.closure - iteration 87: loss = -3.304481274452108
2023-04-08 21:34:58,866 - INFO - training.closure - iteration 88: loss = -3.3656141325512805
2023-04-08 21:35:00,599 - INFO - training.closure - iteration 89: loss = -3.3906478413428873
2023-04-08 21:35:02,315 - INFO - training.closure - iteration 90: loss = -3.423699553698541
2023-04-08 21:35:04,039 - INFO - training.closure - iteration 91: loss = -3.443933684378399
2023-04-08 21:35:05,930 - INFO - training.closure - iteration 92: loss = -3.4583903690703908
2023-04-08 21:35:07,799 - INFO - training.closure - iteration 93: loss = -3.4640297095707817
2023-04-08 21:35:09,542 - INFO - training.closure - iteration 94: loss = -3.494413820038475
2023-04-08 21:35:11,257 - INFO - training.closure - iteration 95: loss = -3.5108244786376606
2023-04-08 21:35:13,128 - INFO - training.closure - iteration 96: loss = -3.5299787435941403
2023-04-08 21:35:15,086 - INFO - training.closure - iteration 97: loss = -3.5422901623907714
2023-04-08 21:35:16,825 - INFO - training.closure - iteration 98: loss = -3.5456222607944436
2023-04-08 21:35:18,549 - INFO - training.closure - iteration 99: loss = -3.559212674276216
2023-04-08 21:35:20,295 - INFO - training.closure - iteration 100: loss = -3.561783520545821
2023-04-08 21:35:22,029 - INFO - training.closure - iteration 101: loss = -3.5671399708075855
2023-04-08 21:35:23,913 - INFO - training.closure - iteration 102: loss = -3.578786763103907
2023-04-08 21:35:25,634 - INFO - training.closure - iteration 103: loss = -3.5908215545533975
2023-04-08 21:35:27,401 - INFO - training.closure - iteration 104: loss = -3.5903151837934404
2023-04-08 21:35:29,234 - INFO - training.closure - iteration 105: loss = -3.598418857633125
2023-04-08 21:35:31,294 - INFO - training.closure - iteration 106: loss = -3.6086477503246552
2023-04-08 21:35:33,134 - INFO - training.closure - iteration 107: loss = -3.6082993606790703
2023-04-08 21:35:34,889 - INFO - training.closure - iteration 108: loss = -3.614692544711849
2023-04-08 21:35:36,624 - INFO - training.closure - iteration 109: loss = -3.6201154705671987
2023-04-08 21:35:38,338 - INFO - training.closure - iteration 110: loss = -3.626201495790732
2023-04-08 21:35:40,064 - INFO - training.closure - iteration 111: loss = -3.631042081723546
2023-04-08 21:35:41,801 - INFO - training.closure - iteration 112: loss = -3.634125458807196
2023-04-08 21:35:43,535 - INFO - training.closure - iteration 113: loss = -3.640506599963429
2023-04-08 21:35:45,400 - INFO - training.closure - iteration 114: loss = -3.609187960289291
2023-04-08 21:35:47,206 - INFO - training.closure - iteration 115: loss = -3.6507633581807957
2023-04-08 21:35:48,965 - INFO - training.closure - iteration 116: loss = -3.6621696813709956
2023-04-08 21:35:50,762 - INFO - training.closure - iteration 117: loss = -3.675338505904726
2023-04-08 21:35:52,481 - INFO - training.closure - iteration 118: loss = -3.681433522884216
2023-04-08 21:35:54,190 - INFO - training.closure - iteration 119: loss = -3.687288632123634
2023-04-08 21:35:55,921 - INFO - training.closure - iteration 120: loss = -3.693481746362677
2023-04-08 21:35:57,645 - INFO - training.closure - iteration 121: loss = -3.7003018038203987
2023-04-08 21:35:59,368 - INFO - training.closure - iteration 122: loss = -3.70485935745206
2023-04-08 21:36:01,097 - INFO - training.closure - iteration 123: loss = -3.7089072935249794
2023-04-08 21:36:02,804 - INFO - training.closure - iteration 124: loss = -3.711113764936995
2023-04-08 21:36:04,511 - INFO - training.closure - iteration 125: loss = -3.717091884192178
2023-04-08 21:36:06,379 - INFO - training.closure - iteration 126: loss = -3.7195418212459774
2023-04-08 21:36:08,218 - INFO - training.closure - iteration 127: loss = -3.7227249818818473
2023-04-08 21:36:09,941 - INFO - training.closure - iteration 128: loss = -3.733500753385893
2023-04-08 21:36:11,670 - INFO - training.closure - iteration 129: loss = -3.742242372308765
2023-04-08 21:36:13,428 - INFO - training.closure - iteration 130: loss = -3.7443057934162467
2023-04-08 21:36:15,194 - INFO - training.closure - iteration 131: loss = -3.746338761466914
2023-04-08 21:36:17,015 - INFO - training.closure - iteration 132: loss = -3.747388757837969
2023-04-08 21:36:18,729 - INFO - training.closure - iteration 133: loss = -3.7477422685672552
2023-04-08 21:36:20,450 - INFO - training.closure - iteration 134: loss = -3.74811250389231
2023-04-08 21:36:22,270 - INFO - training.closure - iteration 135: loss = -3.7486690235755518
2023-04-08 21:36:24,352 - INFO - training.closure - iteration 136: loss = -3.7496109549744956
2023-04-08 21:36:26,143 - INFO - training.closure - iteration 137: loss = -3.750210663662882
2023-04-08 21:36:27,891 - INFO - training.closure - iteration 138: loss = -3.7514123759262055
2023-04-08 21:36:29,676 - INFO - training.closure - iteration 139: loss = -3.7524234793996514
2023-04-08 21:36:31,421 - INFO - training.closure - iteration 140: loss = -3.7518676651669454
2023-04-08 21:36:33,158 - INFO - training.closure - iteration 141: loss = -3.75323593818955
2023-04-08 21:36:35,072 - INFO - training.closure - iteration 142: loss = -3.754595944257338
2023-04-08 21:36:36,930 - INFO - training.closure - iteration 143: loss = -3.756456266382544
2023-04-08 21:36:38,663 - INFO - training.closure - iteration 144: loss = -3.762868874397956
2023-04-08 21:36:40,377 - INFO - training.closure - iteration 145: loss = -3.773746288563027
2023-04-08 21:36:42,112 - INFO - training.closure - iteration 146: loss = -3.785589087998835
2023-04-08 21:36:43,942 - INFO - training.closure - iteration 147: loss = -3.7707770387184727
2023-04-08 21:36:45,708 - INFO - training.closure - iteration 148: loss = -3.791861025188128
2023-04-08 21:36:47,554 - INFO - training.closure - iteration 149: loss = -3.7968193333035565
2023-04-08 21:36:49,317 - INFO - training.closure - iteration 150: loss = -3.798821403493937
2023-04-08 21:36:51,100 - INFO - training.closure - iteration 151: loss = -3.8006791019027104
2023-04-08 21:36:52,848 - INFO - training.closure - iteration 152: loss = -3.8016379165790015
2023-04-08 21:36:54,657 - INFO - training.closure - iteration 153: loss = -3.8028202341489097
2023-04-08 21:36:56,417 - INFO - training.closure - iteration 154: loss = -3.804213468363299
2023-04-08 21:36:58,330 - INFO - training.closure - iteration 155: loss = -3.8058321608916623
2023-04-08 21:37:00,275 - INFO - training.closure - iteration 156: loss = -3.808203311291279
2023-04-08 21:37:02,039 - INFO - training.closure - iteration 157: loss = -3.8118432543517606
2023-04-08 21:37:03,886 - INFO - training.closure - iteration 158: loss = -3.8160888378391666
2023-04-08 21:37:05,701 - INFO - training.closure - iteration 159: loss = 184117.558898817
2023-04-08 21:37:07,518 - INFO - training.closure - iteration 160: loss = 492.3163532891549
2023-04-08 21:37:09,339 - INFO - training.closure - iteration 161: loss = 12.771474154972877
2023-04-08 21:37:11,141 - INFO - training.closure - iteration 162: loss = -3.2444005185931797
2023-04-08 21:37:12,970 - INFO - training.closure - iteration 163: loss = -3.81712366467064
2023-04-08 21:37:14,744 - INFO - training.closure - iteration 164: loss = -3.82163278755302
2023-04-08 21:37:16,598 - INFO - training.closure - iteration 165: loss = -3.8250199100404414
2023-04-08 21:37:18,424 - INFO - training.closure - iteration 166: loss = -3.829365359370443
2023-04-08 21:37:20,341 - INFO - training.closure - iteration 167: loss = -3.833242349096384
2023-04-08 21:37:22,146 - INFO - training.closure - iteration 168: loss = -3.8346972059399276
2023-04-08 21:37:23,987 - INFO - training.closure - iteration 169: loss = -3.8349657597347733
2023-04-08 21:37:25,812 - INFO - training.closure - iteration 170: loss = -3.835341034206091
2023-04-08 21:37:27,617 - INFO - training.closure - iteration 171: loss = -3.8370442541466847
2023-04-08 21:37:29,384 - INFO - training.closure - iteration 172: loss = -3.83990186572776
2023-04-08 21:37:31,150 - INFO - training.closure - iteration 173: loss = -3.846558986083223
2023-04-08 21:37:32,958 - INFO - training.closure - iteration 174: loss = -3.853085635141456
2023-04-08 21:37:34,755 - INFO - training.closure - iteration 175: loss = -3.876367834823502
2023-04-08 21:37:36,541 - INFO - training.closure - iteration 176: loss = 15.9088606950116
2023-04-08 21:37:38,374 - INFO - training.closure - iteration 177: loss = -3.2181154158696765
2023-04-08 21:37:40,207 - INFO - training.closure - iteration 178: loss = -3.882434059610487
2023-04-08 21:37:42,022 - INFO - training.closure - iteration 179: loss = -3.9114196780118893
2023-04-08 21:37:43,816 - INFO - training.closure - iteration 180: loss = -3.9075089451769704
2023-04-08 21:37:45,610 - INFO - training.closure - iteration 181: loss = -3.9434114249848133
2023-04-08 21:37:47,370 - INFO - training.closure - iteration 182: loss = -3.98991418550021
2023-04-08 21:37:49,261 - INFO - training.closure - iteration 183: loss = -4.030875950872661
2023-04-08 21:37:51,045 - INFO - training.closure - iteration 184: loss = -3.8555716012999603
2023-04-08 21:37:52,823 - INFO - training.closure - iteration 185: loss = -4.042404439375411
2023-04-08 21:37:54,591 - INFO - training.closure - iteration 186: loss = -4.033850964162734
2023-04-08 21:37:56,355 - INFO - training.closure - iteration 187: loss = -4.052192188558486
2023-04-08 21:37:58,177 - INFO - training.closure - iteration 188: loss = -4.0765935129956645
2023-04-08 21:37:59,997 - INFO - training.closure - iteration 189: loss = -4.098173079458597
2023-04-08 21:38:01,814 - INFO - training.closure - iteration 190: loss = -4.118769406058962
2023-04-08 21:38:03,568 - INFO - training.closure - iteration 191: loss = -4.104044988443849
2023-04-08 21:38:05,339 - INFO - training.closure - iteration 192: loss = -4.127710588078486
2023-04-08 21:38:07,364 - INFO - training.closure - iteration 193: loss = -4.137933232219285
2023-04-08 21:38:09,242 - INFO - training.closure - iteration 194: loss = -4.146570738976058
2023-04-08 21:38:11,053 - INFO - training.closure - iteration 195: loss = -4.149878945928609
2023-04-08 21:38:12,836 - INFO - training.closure - iteration 196: loss = -4.154842210859572
2023-04-08 21:38:14,740 - INFO - training.closure - iteration 197: loss = -4.163180578465791
2023-04-08 21:38:16,499 - INFO - training.closure - iteration 198: loss = -4.169905681695187
2023-04-08 21:38:18,348 - INFO - training.closure - iteration 199: loss = -4.17418121029122
2023-04-08 21:38:20,121 - INFO - training.closure - iteration 200: loss = -4.179957714174101
2023-04-08 21:38:21,896 - INFO - training.closure - iteration 201: loss = -4.183087095268271
2023-04-08 21:38:23,654 - INFO - training.closure - iteration 202: loss = -4.186809493345554
2023-04-08 21:38:25,483 - INFO - training.closure - iteration 203: loss = -4.190076049626873
2023-04-08 21:38:27,248 - INFO - training.closure - iteration 204: loss = -4.197712637434956
2023-04-08 21:38:29,013 - INFO - training.closure - iteration 205: loss = -4.202373603587862
2023-04-08 21:38:30,755 - INFO - training.closure - iteration 206: loss = -4.214367624994926
2023-04-08 21:38:32,604 - INFO - training.closure - iteration 207: loss = -4.199653132515039
2023-04-08 21:38:34,452 - INFO - training.closure - iteration 208: loss = -4.222681493405876
2023-04-08 21:38:36,280 - INFO - training.closure - iteration 209: loss = -4.1596674673545175
2023-04-08 21:38:38,026 - INFO - training.closure - iteration 210: loss = -4.233829631090926
2023-04-08 21:38:39,951 - INFO - training.closure - iteration 211: loss = -4.261732085071122
2023-04-08 21:38:41,891 - INFO - training.closure - iteration 212: loss = -4.296995602761965
2023-04-08 21:38:43,683 - INFO - training.closure - iteration 213: loss = -4.315017248522634
2023-04-08 21:38:45,454 - INFO - training.closure - iteration 214: loss = -4.266159970752517
2023-04-08 21:38:47,210 - INFO - training.closure - iteration 215: loss = -4.319433120988027
2023-04-08 21:38:49,408 - INFO - training.closure - iteration 216: loss = -4.3251058801028
2023-04-08 21:38:51,211 - INFO - training.closure - iteration 217: loss = -4.323592795582496
2023-04-08 21:38:53,115 - INFO - training.closure - iteration 218: loss = -4.3337498211271015
2023-04-08 21:38:54,920 - INFO - training.closure - iteration 219: loss = -4.346723392044478
2023-04-08 21:38:56,680 - INFO - training.closure - iteration 220: loss = -4.374980940473211
2023-04-08 21:38:58,527 - INFO - training.closure - iteration 221: loss = -4.419441883907808
2023-04-08 21:39:00,292 - INFO - training.closure - iteration 222: loss = -4.525989204757338
2023-04-08 21:39:02,099 - INFO - training.closure - iteration 223: loss = -4.730320912801316
2023-04-08 21:39:03,895 - INFO - training.closure - iteration 224: loss = -4.762911212103771
2023-04-08 21:39:05,689 - INFO - training.closure - iteration 225: loss = -1.766948942361353
2023-04-08 21:39:07,575 - INFO - training.closure - iteration 226: loss = -4.782456122459742
2023-04-08 21:39:09,404 - INFO - training.closure - iteration 227: loss = -4.814793207410028
2023-04-08 21:39:11,169 - INFO - training.closure - iteration 228: loss = 6.61404094657888
2023-04-08 21:39:12,939 - INFO - training.closure - iteration 229: loss = -4.709039952311626
2023-04-08 21:39:14,775 - INFO - training.closure - iteration 230: loss = -4.926337660047051
2023-04-08 21:39:16,543 - INFO - training.closure - iteration 231: loss = -4.411861725005003
2023-04-08 21:39:18,480 - INFO - training.closure - iteration 232: loss = -5.208940995837498
2023-04-08 21:39:20,284 - INFO - training.closure - iteration 233: loss = 116.31143140783728
2023-04-08 21:39:22,062 - INFO - training.closure - iteration 234: loss = -0.8110264712264766
2023-04-08 21:39:23,836 - INFO - training.closure - iteration 235: loss = -5.2562473184004475
2023-04-08 21:39:25,613 - INFO - training.closure - iteration 236: loss = -5.308635689149188
2023-04-08 21:39:27,368 - INFO - training.closure - iteration 237: loss = -5.656306413844004
2023-04-08 21:39:29,130 - INFO - training.closure - iteration 238: loss = -4.9457324107956175
2023-04-08 21:39:31,119 - INFO - training.closure - iteration 239: loss = -5.702792881994201
2023-04-08 21:39:32,886 - INFO - training.closure - iteration 240: loss = -5.758774335409532
2023-04-08 21:39:34,633 - INFO - training.closure - iteration 241: loss = -5.7733274438669
2023-04-08 21:39:36,589 - INFO - training.closure - iteration 242: loss = -5.943967364998585
2023-04-08 21:39:38,392 - INFO - training.closure - iteration 243: loss = -5.623419211438566
2023-04-08 21:39:40,134 - INFO - training.closure - iteration 244: loss = -6.043790037742714
2023-04-08 21:39:41,909 - INFO - training.closure - iteration 245: loss = -3.956733452575942
2023-04-08 21:39:43,687 - INFO - training.closure - iteration 246: loss = -6.0684431140422905
2023-04-08 21:39:45,506 - INFO - training.closure - iteration 247: loss = -6.1222698634481265
2023-04-08 21:39:47,262 - INFO - training.closure - iteration 248: loss = -6.1782271031678615
2023-04-08 21:39:49,065 - INFO - training.closure - iteration 249: loss = -2.465435104651595
2023-04-08 21:39:50,910 - INFO - training.closure - iteration 250: loss = -6.105584810647281
2023-04-08 21:39:52,663 - INFO - training.closure - iteration 251: loss = -6.192519517229271
2023-04-08 21:39:54,414 - INFO - training.closure - iteration 252: loss = -6.228800273988577
2023-04-08 21:39:56,163 - INFO - training.closure - iteration 253: loss = -6.242180314960871
2023-04-08 21:39:58,071 - INFO - training.closure - iteration 254: loss = -6.255457251221792
2023-04-08 21:39:59,981 - INFO - training.closure - iteration 255: loss = -6.266330476771316
2023-04-08 21:40:01,883 - INFO - training.closure - iteration 256: loss = -6.274649282714755
2023-04-08 21:40:03,733 - INFO - training.closure - iteration 257: loss = -6.2923792142718336
2023-04-08 21:40:05,501 - INFO - training.closure - iteration 258: loss = -6.307020476118546
2023-04-08 21:40:07,273 - INFO - training.closure - iteration 259: loss = -6.323194494406156
2023-04-08 21:40:09,107 - INFO - training.closure - iteration 260: loss = -6.32940349043529
2023-04-08 21:40:10,943 - INFO - training.closure - iteration 261: loss = -6.331971830422871
2023-04-08 21:40:12,826 - INFO - training.closure - iteration 262: loss = -6.3344583821849145
2023-04-08 21:40:14,764 - INFO - training.closure - iteration 263: loss = -6.3358983993888724
2023-04-08 21:40:16,546 - INFO - training.closure - iteration 264: loss = -6.339608957537687
2023-04-08 21:40:18,318 - INFO - training.closure - iteration 265: loss = -6.3461069853755525
2023-04-08 21:40:20,080 - INFO - training.closure - iteration 266: loss = -6.357054027940647
2023-04-08 21:40:21,906 - INFO - training.closure - iteration 267: loss = -6.378815522510506
2023-04-08 21:40:23,746 - INFO - training.closure - iteration 268: loss = -6.23515417422667
2023-04-08 21:40:25,521 - INFO - training.closure - iteration 269: loss = -6.402324704022172
2023-04-08 21:40:27,399 - INFO - training.closure - iteration 270: loss = -6.433004733931188
2023-04-08 21:40:29,227 - INFO - training.closure - iteration 271: loss = -6.462300143214048
2023-04-08 21:40:31,054 - INFO - training.closure - iteration 272: loss = -6.491214579528
2023-04-08 21:40:32,844 - INFO - training.closure - iteration 273: loss = -6.524360506557025
2023-04-08 21:40:34,696 - INFO - training.closure - iteration 274: loss = -5.192468690042432
2023-04-08 21:40:36,454 - INFO - training.closure - iteration 275: loss = -6.525928526479238
2023-04-08 21:40:38,247 - INFO - training.closure - iteration 276: loss = -6.557396016219842
2023-04-08 21:40:40,049 - INFO - training.closure - iteration 277: loss = -6.573200041189923
2023-04-08 21:40:41,884 - INFO - training.closure - iteration 278: loss = -6.580138357345963
2023-04-08 21:40:43,634 - INFO - training.closure - iteration 279: loss = -6.578966263728531
2023-04-08 21:40:45,393 - INFO - training.closure - iteration 280: loss = -6.592027051891928
2023-04-08 21:40:47,209 - INFO - training.closure - iteration 281: loss = -6.607672595954586
2023-04-08 21:40:48,954 - INFO - training.closure - iteration 282: loss = -6.62739490223487
2023-04-08 21:40:50,780 - INFO - training.closure - iteration 283: loss = -6.648477000301238
2023-04-08 21:40:52,585 - INFO - training.closure - iteration 284: loss = -6.660685933802059
2023-04-08 21:40:54,346 - INFO - training.closure - iteration 285: loss = -6.669611381133843
2023-04-08 21:40:56,167 - INFO - training.closure - iteration 286: loss = -6.671158799968253
2023-04-08 21:40:57,949 - INFO - training.closure - iteration 287: loss = -6.676576417704435
2023-04-08 21:40:59,891 - INFO - training.closure - iteration 288: loss = -6.68130357011548
2023-04-08 21:41:01,680 - INFO - training.closure - iteration 289: loss = -6.686033344913072
2023-04-08 21:41:03,598 - INFO - training.closure - iteration 290: loss = -6.693180617276527
2023-04-08 21:41:05,409 - INFO - training.closure - iteration 291: loss = -6.705907732191865
2023-04-08 21:41:07,179 - INFO - training.closure - iteration 292: loss = -6.716329522646196
2023-04-08 21:41:08,952 - INFO - training.closure - iteration 293: loss = -6.726295434521453
2023-04-08 21:41:10,714 - INFO - training.closure - iteration 294: loss = -6.721658039655682
2023-04-08 21:41:12,604 - INFO - training.closure - iteration 295: loss = -6.730779337388776
2023-04-08 21:41:14,354 - INFO - training.closure - iteration 296: loss = -6.739067638903599
2023-04-08 21:41:16,107 - INFO - training.closure - iteration 297: loss = -6.67438398452474
2023-04-08 21:41:17,999 - INFO - training.closure - iteration 298: loss = -6.747003638197748
2023-04-08 21:41:19,927 - INFO - training.closure - iteration 299: loss = -6.690151511857563
2023-04-08 21:41:21,835 - INFO - training.closure - iteration 300: loss = -6.748806312134974
2023-04-08 21:41:23,661 - INFO - training.closure - iteration 301: loss = -6.7576133348699425
2023-04-08 21:41:25,443 - INFO - training.closure - iteration 302: loss = -6.762765114969026
2023-04-08 21:41:27,349 - INFO - training.closure - iteration 303: loss = -6.765032943034351
2023-04-08 21:41:29,279 - INFO - training.closure - iteration 304: loss = -6.766040951741465
2023-04-08 21:41:31,108 - INFO - training.closure - iteration 305: loss = -6.767110402980763
2023-04-08 21:41:32,949 - INFO - training.closure - iteration 306: loss = -6.763381343738498
2023-04-08 21:41:34,751 - INFO - training.closure - iteration 307: loss = -6.768569546366599
2023-04-08 21:41:36,593 - INFO - training.closure - iteration 308: loss = -6.7737952600418785
2023-04-08 21:41:38,380 - INFO - training.closure - iteration 309: loss = -6.777709523898821
2023-04-08 21:41:40,177 - INFO - training.closure - iteration 310: loss = -6.781004167957944
2023-04-08 21:41:42,046 - INFO - training.closure - iteration 311: loss = -6.777371172043919
2023-04-08 21:41:43,931 - INFO - training.closure - iteration 312: loss = -6.7840283856621575
2023-04-08 21:41:45,746 - INFO - training.closure - iteration 313: loss = -6.787089131173422
2023-04-08 21:41:47,525 - INFO - training.closure - iteration 314: loss = -6.790130415842707
2023-04-08 21:41:49,539 - INFO - training.closure - iteration 315: loss = -6.795343622684417
2023-04-08 21:41:51,313 - INFO - training.closure - iteration 316: loss = -6.800023625250155
2023-04-08 21:41:53,241 - INFO - training.closure - iteration 317: loss = -6.809391001722231
2023-04-08 21:41:55,011 - INFO - training.closure - iteration 318: loss = -6.822327760465287
2023-04-08 21:41:56,771 - INFO - training.closure - iteration 319: loss = -6.815389567153124
2023-04-08 21:41:58,597 - INFO - training.closure - iteration 320: loss = -6.8333536989699
2023-04-08 21:42:00,364 - INFO - training.closure - iteration 321: loss = -6.843139345373058
2023-04-08 21:42:02,268 - INFO - training.closure - iteration 322: loss = -6.850492020087061
2023-04-08 21:42:04,026 - INFO - training.closure - iteration 323: loss = -6.8527858652364735
2023-04-08 21:42:05,772 - INFO - training.closure - iteration 324: loss = -6.862736303337801
2023-04-08 21:42:07,530 - INFO - training.closure - iteration 325: loss = -6.86691560271522
2023-04-08 21:42:09,277 - INFO - training.closure - iteration 326: loss = -6.871177236130561
2023-04-08 21:42:11,046 - INFO - training.closure - iteration 327: loss = -6.879031039685925
2023-04-08 21:42:12,815 - INFO - training.closure - iteration 328: loss = -6.889331778242525
2023-04-08 21:42:14,571 - INFO - training.closure - iteration 329: loss = -6.899969658813379
2023-04-08 21:42:16,451 - INFO - training.closure - iteration 330: loss = -6.910687195100337
2023-04-08 21:42:18,307 - INFO - training.closure - iteration 331: loss = -6.928176972371682
2023-04-08 21:42:20,100 - INFO - training.closure - iteration 332: loss = -6.950221519672252
2023-04-08 21:42:21,963 - INFO - training.closure - iteration 333: loss = -6.962434264204748
2023-04-08 21:42:23,802 - INFO - training.closure - iteration 334: loss = -6.967060682971995
2023-04-08 21:42:25,617 - INFO - training.closure - iteration 335: loss = -6.990696840565285
2023-04-08 21:42:27,381 - INFO - training.closure - iteration 336: loss = -7.0159388720081575
2023-04-08 21:42:29,135 - INFO - training.closure - iteration 337: loss = -7.047418233993902
2023-04-08 21:42:30,901 - INFO - training.closure - iteration 338: loss = -7.08201829237583
2023-04-08 21:42:32,661 - INFO - training.closure - iteration 339: loss = -7.089912780256201
2023-04-08 21:42:34,426 - INFO - training.closure - iteration 340: loss = -7.121982010735701
2023-04-08 21:42:36,253 - INFO - training.closure - iteration 341: loss = -7.125909428139137
2023-04-08 21:42:38,046 - INFO - training.closure - iteration 342: loss = -7.146695886687668
2023-04-08 21:42:39,804 - INFO - training.closure - iteration 343: loss = -7.16350421058841
2023-04-08 21:42:41,658 - INFO - training.closure - iteration 344: loss = -7.185701446996971
2023-04-08 21:42:43,414 - INFO - training.closure - iteration 345: loss = -7.198826780156507
2023-04-08 21:42:45,171 - INFO - training.closure - iteration 346: loss = -7.208190549778559
2023-04-08 21:42:46,957 - INFO - training.closure - iteration 347: loss = -7.218714142845489
2023-04-08 21:42:48,723 - INFO - training.closure - iteration 348: loss = -7.228206860137005
2023-04-08 21:42:50,583 - INFO - training.closure - iteration 349: loss = -7.236559308243085
2023-04-08 21:42:52,432 - INFO - training.closure - iteration 350: loss = -7.23990659318221
2023-04-08 21:42:54,198 - INFO - training.closure - iteration 351: loss = -7.244917671771632
2023-04-08 21:42:56,036 - INFO - training.closure - iteration 352: loss = -7.25403761998371
2023-04-08 21:42:57,799 - INFO - training.closure - iteration 353: loss = -7.264529815199915
2023-04-08 21:42:59,551 - INFO - training.closure - iteration 354: loss = -7.266266940511942
2023-04-08 21:43:01,311 - INFO - training.closure - iteration 355: loss = -7.27233997742877
2023-04-08 21:43:03,153 - INFO - training.closure - iteration 356: loss = -7.282399917624531
2023-04-08 21:43:05,026 - INFO - training.closure - iteration 357: loss = -7.301743364572092
2023-04-08 21:43:06,800 - INFO - training.closure - iteration 358: loss = -7.308755208796914
2023-04-08 21:43:08,584 - INFO - training.closure - iteration 359: loss = -7.319039321013104
2023-04-08 21:43:10,354 - INFO - training.closure - iteration 360: loss = -7.330713278620994
2023-04-08 21:43:12,135 - INFO - training.closure - iteration 361: loss = -7.341811830424517
2023-04-08 21:43:13,917 - INFO - training.closure - iteration 362: loss = -7.348678724789406
2023-04-08 21:43:15,841 - INFO - training.closure - iteration 363: loss = -7.35513149237902
2023-04-08 21:43:17,647 - INFO - training.closure - iteration 364: loss = -7.359796808988717
2023-04-08 21:43:19,402 - INFO - training.closure - iteration 365: loss = -7.360143537646519
2023-04-08 21:43:21,159 - INFO - training.closure - iteration 366: loss = -7.361978969090429
2023-04-08 21:43:22,900 - INFO - training.closure - iteration 367: loss = -7.362237807245137
2023-04-08 21:43:24,663 - INFO - training.closure - iteration 368: loss = -7.363958922463599
2023-04-08 21:43:26,435 - INFO - training.closure - iteration 369: loss = -7.364687405837381
2023-04-08 21:43:28,187 - INFO - training.closure - iteration 370: loss = -7.365756536020206
2023-04-08 21:43:29,942 - INFO - training.closure - iteration 371: loss = -7.367006144464481
2023-04-08 21:43:31,779 - INFO - training.closure - iteration 372: loss = -7.36947855698194
2023-04-08 21:43:33,586 - INFO - training.closure - iteration 373: loss = -7.373124630703664
2023-04-08 21:43:35,393 - INFO - training.closure - iteration 374: loss = -7.374642816058765
2023-04-08 21:43:37,172 - INFO - training.closure - iteration 375: loss = -7.377075069475876
2023-04-08 21:43:38,943 - INFO - training.closure - iteration 376: loss = -7.381716394033357
2023-04-08 21:43:40,736 - INFO - training.closure - iteration 377: loss = -7.38914012775616
2023-04-08 21:43:42,517 - INFO - training.closure - iteration 378: loss = -7.3962538637666215
2023-04-08 21:43:44,358 - INFO - training.closure - iteration 379: loss = -7.4007471344364255
2023-04-08 21:43:46,178 - INFO - training.closure - iteration 380: loss = -7.408812259690947
2023-04-08 21:43:47,996 - INFO - training.closure - iteration 381: loss = -7.415804112628587
2023-04-08 21:43:49,947 - INFO - training.closure - iteration 382: loss = -7.426216771634543
2023-04-08 21:43:51,874 - INFO - training.closure - iteration 383: loss = -7.436664214189224
2023-04-08 21:43:53,669 - INFO - training.closure - iteration 384: loss = -7.452542835145383
2023-04-08 21:43:55,438 - INFO - training.closure - iteration 385: loss = -7.467980769075327
2023-04-08 21:43:57,258 - INFO - training.closure - iteration 386: loss = -7.46995366567852
2023-04-08 21:43:59,058 - INFO - training.closure - iteration 387: loss = -7.474311613367401
2023-04-08 21:44:00,851 - INFO - training.closure - iteration 388: loss = -7.466368013103051
2023-04-08 21:44:02,623 - INFO - training.closure - iteration 389: loss = -7.478313104072966
2023-04-08 21:44:04,434 - INFO - training.closure - iteration 390: loss = -7.479770997990172
2023-04-08 21:44:06,188 - INFO - training.closure - iteration 391: loss = -7.4822419539334994
2023-04-08 21:44:07,958 - INFO - training.closure - iteration 392: loss = -7.485955038096703
2023-04-08 21:44:09,797 - INFO - training.closure - iteration 393: loss = -7.490087369998111
2023-04-08 21:44:11,567 - INFO - training.closure - iteration 394: loss = -7.4942298828305915
2023-04-08 21:44:13,337 - INFO - training.closure - iteration 395: loss = -7.496021764493452
2023-04-08 21:44:15,102 - INFO - training.closure - iteration 396: loss = -7.497429657662792
2023-04-08 21:44:16,874 - INFO - training.closure - iteration 397: loss = -7.501561864527248
2023-04-08 21:44:18,690 - INFO - training.closure - iteration 398: loss = -7.508047126292276
2023-04-08 21:44:20,524 - INFO - training.closure - iteration 399: loss = -7.513653764555437
2023-04-08 21:44:22,613 - INFO - training.closure - iteration 400: loss = -7.513483896099105
2023-04-08 21:44:24,460 - INFO - training.closure - iteration 401: loss = -7.51950522366419
2023-04-08 21:44:26,566 - INFO - training.closure - iteration 402: loss = -7.524097530736726
2023-04-08 21:44:28,453 - INFO - training.closure - iteration 403: loss = -7.528210005240181
2023-04-08 21:44:30,232 - INFO - training.closure - iteration 404: loss = -7.534003390236158
2023-04-08 21:44:32,091 - INFO - training.closure - iteration 405: loss = -7.542278636224993
2023-04-08 21:44:33,981 - INFO - training.closure - iteration 406: loss = -7.548556468846801
2023-04-08 21:44:35,758 - INFO - training.closure - iteration 407: loss = -7.556395654286363
2023-04-08 21:44:37,538 - INFO - training.closure - iteration 408: loss = -7.56207102574832
2023-04-08 21:44:39,317 - INFO - training.closure - iteration 409: loss = -7.565751292981086
2023-04-08 21:44:41,088 - INFO - training.closure - iteration 410: loss = -7.567707666236538
2023-04-08 21:44:43,053 - INFO - training.closure - iteration 411: loss = -7.5694009216537275
2023-04-08 21:44:44,931 - INFO - training.closure - iteration 412: loss = -7.570615280223367
2023-04-08 21:44:46,923 - INFO - training.closure - iteration 413: loss = -7.571646340034471
2023-04-08 21:44:48,839 - INFO - training.closure - iteration 414: loss = -7.572472594309193
2023-04-08 21:44:50,594 - INFO - training.closure - iteration 415: loss = -7.57526928989258
2023-04-08 21:44:52,421 - INFO - training.closure - iteration 416: loss = -7.548628147613448
2023-04-08 21:44:54,191 - INFO - training.closure - iteration 417: loss = -7.576483838510666
2023-04-08 21:44:56,127 - INFO - training.closure - iteration 418: loss = -7.5794791007395315
2023-04-08 21:44:57,934 - INFO - training.closure - iteration 419: loss = -7.58430352277673
2023-04-08 21:44:59,682 - INFO - training.closure - iteration 420: loss = -7.597812487497419
2023-04-08 21:45:01,437 - INFO - training.closure - iteration 421: loss = -7.606882980002672
2023-04-08 21:45:03,192 - INFO - training.closure - iteration 422: loss = -7.617900643172061
2023-04-08 21:45:05,086 - INFO - training.closure - iteration 423: loss = -7.613776014404003
2023-04-08 21:45:06,864 - INFO - training.closure - iteration 424: loss = -7.627022580602878
2023-04-08 21:45:08,661 - INFO - training.closure - iteration 425: loss = -7.627548942507956
2023-04-08 21:45:10,418 - INFO - training.closure - iteration 426: loss = -7.631407918969278
2023-04-08 21:45:12,211 - INFO - training.closure - iteration 427: loss = -7.629043785129352
2023-04-08 21:45:13,972 - INFO - training.closure - iteration 428: loss = -7.633477497551496
2023-04-08 21:45:15,731 - INFO - training.closure - iteration 429: loss = -7.635752140543488
2023-04-08 21:45:17,494 - INFO - training.closure - iteration 430: loss = -7.637911467674107
2023-04-08 21:45:19,360 - INFO - training.closure - iteration 431: loss = -7.639964460958922
2023-04-08 21:45:21,126 - INFO - training.closure - iteration 432: loss = -7.642988198500858
2023-04-08 21:45:22,921 - INFO - training.closure - iteration 433: loss = -7.643982479378035
2023-04-08 21:45:24,890 - INFO - training.closure - iteration 434: loss = -7.648077932244365
2023-04-08 21:45:26,639 - INFO - training.closure - iteration 435: loss = -7.650110474244058
2023-04-08 21:45:28,407 - INFO - training.closure - iteration 436: loss = -7.652687885524398
2023-04-08 21:45:30,153 - INFO - training.closure - iteration 437: loss = -7.65441858078171
2023-04-08 21:45:31,895 - INFO - training.closure - iteration 438: loss = -7.656649468864385
2023-04-08 21:45:33,639 - INFO - training.closure - iteration 439: loss = -7.657453409012641
2023-04-08 21:45:35,411 - INFO - training.closure - iteration 440: loss = -7.6583431206112635
2023-04-08 21:45:37,189 - INFO - training.closure - iteration 441: loss = -7.6590638456685225
2023-04-08 21:45:38,956 - INFO - training.closure - iteration 442: loss = -7.6598446164977165
2023-04-08 21:45:40,724 - INFO - training.closure - iteration 443: loss = -7.6606911709782715
2023-04-08 21:45:42,504 - INFO - training.closure - iteration 444: loss = -7.663114395065746
2023-04-08 21:45:44,473 - INFO - training.closure - iteration 445: loss = -7.665237228435782
2023-04-08 21:45:46,324 - INFO - training.closure - iteration 446: loss = -7.666990747286858
2023-04-08 21:45:48,231 - INFO - training.closure - iteration 447: loss = -7.666414041785478
2023-04-08 21:45:50,004 - INFO - training.closure - iteration 448: loss = -7.66753064530212
2023-04-08 21:45:51,760 - INFO - training.closure - iteration 449: loss = -7.668711949945294
2023-04-08 21:45:53,519 - INFO - training.closure - iteration 450: loss = -7.669812218937145
2023-04-08 21:45:55,268 - INFO - training.closure - iteration 451: loss = -7.671096598843527
2023-04-08 21:45:57,042 - INFO - training.closure - iteration 452: loss = -7.594108817058618
2023-04-08 21:45:58,801 - INFO - training.closure - iteration 453: loss = -7.671424297973828
2023-04-08 21:46:00,674 - INFO - training.closure - iteration 454: loss = -7.672548569875358
2023-04-08 21:46:02,423 - INFO - training.closure - iteration 455: loss = -7.674183705614691
2023-04-08 21:46:04,166 - INFO - training.closure - iteration 456: loss = -7.674763349386141
2023-04-08 21:46:06,002 - INFO - training.closure - iteration 457: loss = -7.675016150044993
2023-04-08 21:46:07,784 - INFO - training.closure - iteration 458: loss = -7.675169407547033
2023-04-08 21:46:09,544 - INFO - training.closure - iteration 459: loss = -7.675217353297599
2023-04-08 21:46:11,327 - INFO - training.closure - iteration 460: loss = -7.675315252518371
2023-04-08 21:46:13,161 - INFO - training.closure - iteration 461: loss = -7.6754289844771115
2023-04-08 21:46:15,034 - INFO - training.closure - iteration 462: loss = -7.675612290192036
2023-04-08 21:46:16,871 - INFO - training.closure - iteration 463: loss = -7.6758481918211885
2023-04-08 21:46:18,716 - INFO - training.closure - iteration 464: loss = -7.67615273590982
2023-04-08 21:46:20,487 - INFO - training.closure - iteration 465: loss = -7.676504518553386
2023-04-08 21:46:22,266 - INFO - training.closure - iteration 466: loss = -7.676783809664541
2023-04-08 21:46:24,034 - INFO - training.closure - iteration 467: loss = -7.676915903518098
2023-04-08 21:46:25,782 - INFO - training.closure - iteration 468: loss = -7.677380194250966
2023-04-08 21:46:27,545 - INFO - training.closure - iteration 469: loss = -7.676724677187821
2023-04-08 21:46:29,303 - INFO - training.closure - iteration 470: loss = -7.6777820463073825
2023-04-08 21:46:31,103 - INFO - training.closure - iteration 471: loss = -7.678456100014825
2023-04-08 21:46:32,898 - INFO - training.closure - iteration 472: loss = -7.680090474061077
2023-04-08 21:46:34,653 - INFO - training.closure - iteration 473: loss = -7.683192147172237
2023-04-08 21:46:36,402 - INFO - training.closure - iteration 474: loss = -7.6870145112475665
2023-04-08 21:46:38,234 - INFO - training.closure - iteration 475: loss = -7.689891193749582
2023-04-08 21:46:40,011 - INFO - training.closure - iteration 476: loss = -7.694498063353637
2023-04-08 21:46:41,784 - INFO - training.closure - iteration 477: loss = -7.696845794322823
2023-04-08 21:46:43,848 - INFO - training.closure - iteration 478: loss = -7.699841822276609
2023-04-08 21:46:45,717 - INFO - training.closure - iteration 479: loss = -7.700982826032218
2023-04-08 21:46:47,474 - INFO - training.closure - iteration 480: loss = -7.701982134518279
2023-04-08 21:46:49,222 - INFO - training.closure - iteration 481: loss = -7.702690086855712
2023-04-08 21:46:50,998 - INFO - training.closure - iteration 482: loss = -7.703540487436677
2023-04-08 21:46:52,780 - INFO - training.closure - iteration 483: loss = -7.704125511604298
2023-04-08 21:46:54,624 - INFO - training.closure - iteration 484: loss = -7.704758837682622
2023-04-08 21:46:56,394 - INFO - training.closure - iteration 485: loss = -7.705028198428597
2023-04-08 21:46:58,162 - INFO - training.closure - iteration 486: loss = -7.705201392858436
2023-04-08 21:46:59,929 - INFO - training.closure - iteration 487: loss = -7.7052999184203
2023-04-08 21:47:01,690 - INFO - training.closure - iteration 488: loss = -7.705392427779884
2023-04-08 21:47:03,443 - INFO - training.closure - iteration 489: loss = -7.705526512965294
2023-04-08 21:47:05,212 - INFO - training.closure - iteration 490: loss = -7.70565002508652
2023-04-08 21:47:06,983 - INFO - training.closure - iteration 491: loss = -7.705759176393121
2023-04-08 21:47:08,757 - INFO - training.closure - iteration 492: loss = -7.705866213741929
2023-04-08 21:47:10,531 - INFO - training.closure - iteration 493: loss = -7.705952940325208
2023-04-08 21:47:12,300 - INFO - training.closure - iteration 494: loss = -7.706076037687321
2023-04-08 21:47:14,131 - INFO - training.closure - iteration 495: loss = -7.706229769111719
2023-04-08 21:47:15,873 - INFO - training.closure - iteration 496: loss = -7.7064837964895165
2023-04-08 21:47:17,619 - INFO - training.closure - iteration 497: loss = -7.706882866314622
2023-04-08 21:47:19,389 - INFO - training.closure - iteration 498: loss = -7.707592306543597
2023-04-08 21:47:21,230 - INFO - training.closure - iteration 499: loss = -7.708484650345052
2023-04-08 21:47:23,064 - INFO - training.closure - iteration 500: loss = -7.6916125010803125
2023-04-08 21:47:24,831 - INFO - training.closure - iteration 501: loss = -7.7086285439841244
2023-04-08 21:47:26,596 - INFO - training.closure - iteration 502: loss = -7.698588920086693
2023-04-08 21:47:28,357 - INFO - training.closure - iteration 503: loss = -7.708830403449122
2023-04-08 21:47:30,255 - INFO - training.closure - iteration 504: loss = -7.709633349694212
2023-04-08 21:47:32,082 - INFO - training.closure - iteration 505: loss = -7.710495865132387
2023-04-08 21:47:33,833 - INFO - training.closure - iteration 506: loss = -7.711282370286413
2023-04-08 21:47:35,655 - INFO - training.closure - iteration 507: loss = -7.712293230431621
2023-04-08 21:47:37,539 - INFO - training.closure - iteration 508: loss = -7.714179623378323
2023-04-08 21:47:39,407 - INFO - training.closure - iteration 509: loss = -7.716010368606449
2023-04-08 21:47:41,174 - INFO - training.closure - iteration 510: loss = -7.718800925665924
2023-04-08 21:47:43,105 - INFO - training.closure - iteration 511: loss = -7.722760579097296
2023-04-08 21:47:44,874 - INFO - training.closure - iteration 512: loss = -7.725848158539371
2023-04-08 21:47:46,691 - INFO - training.closure - iteration 513: loss = -7.730365065901041
2023-04-08 21:47:48,546 - INFO - training.closure - iteration 514: loss = -7.738228485201698
2023-04-08 21:47:50,335 - INFO - training.closure - iteration 515: loss = -7.7462278428987705
2023-04-08 21:47:52,151 - INFO - training.closure - iteration 516: loss = -7.7430824857161165
2023-04-08 21:47:53,896 - INFO - training.closure - iteration 517: loss = -7.752769783557996
2023-04-08 21:47:55,688 - INFO - training.closure - iteration 518: loss = -7.754372100318827
2023-04-08 21:47:57,455 - INFO - training.closure - iteration 519: loss = -7.7573685172956965
2023-04-08 21:47:59,223 - INFO - training.closure - iteration 520: loss = -7.758005030991607
2023-04-08 21:48:01,043 - INFO - training.closure - iteration 521: loss = -7.761337800595317
2023-04-08 21:48:02,804 - INFO - training.closure - iteration 522: loss = -7.761869340209855
2023-04-08 21:48:04,574 - INFO - training.closure - iteration 523: loss = -7.763255176929512
2023-04-08 21:48:06,343 - INFO - training.closure - iteration 524: loss = -7.764356601679151
2023-04-08 21:48:08,205 - INFO - training.closure - iteration 525: loss = -7.7650556543125315
2023-04-08 21:48:09,968 - INFO - training.closure - iteration 526: loss = -7.765724891871319
2023-04-08 21:48:11,915 - INFO - training.closure - iteration 527: loss = -7.767094030685976
2023-04-08 21:48:13,793 - INFO - training.closure - iteration 528: loss = -7.768440067029276
2023-04-08 21:48:15,547 - INFO - training.closure - iteration 529: loss = -7.769759622460887
2023-04-08 21:48:17,296 - INFO - training.closure - iteration 530: loss = -7.770753341800482
2023-04-08 21:48:19,064 - INFO - training.closure - iteration 531: loss = -7.771623582152831
2023-04-08 21:48:20,824 - INFO - training.closure - iteration 532: loss = -7.7737961791570855
2023-04-08 21:48:22,602 - INFO - training.closure - iteration 533: loss = -7.774342917728856
2023-04-08 21:48:24,538 - INFO - training.closure - iteration 534: loss = -7.774965130711298
2023-04-08 21:48:26,308 - INFO - training.closure - iteration 535: loss = -7.775407316846137
2023-04-08 21:48:28,288 - INFO - training.closure - iteration 536: loss = -7.775707537037562
2023-04-08 21:48:30,043 - INFO - training.closure - iteration 537: loss = -7.776057877764774
2023-04-08 21:48:31,795 - INFO - training.closure - iteration 538: loss = -7.7762278283800494
2023-04-08 21:48:33,551 - INFO - training.closure - iteration 539: loss = -7.776418337066225
2023-04-08 21:48:35,304 - INFO - training.closure - iteration 540: loss = -7.7766142544616255
2023-04-08 21:48:37,111 - INFO - training.closure - iteration 541: loss = -7.776876590802944
2023-04-08 21:48:39,086 - INFO - training.closure - iteration 542: loss = -7.777449898238759
2023-04-08 21:48:40,869 - INFO - training.closure - iteration 543: loss = -7.778055136772856
2023-04-08 21:48:42,634 - INFO - training.closure - iteration 544: loss = -7.778765256666112
2023-04-08 21:48:44,394 - INFO - training.closure - iteration 545: loss = -7.712852066034552
2023-04-08 21:48:46,296 - INFO - training.closure - iteration 546: loss = -7.779672499105426
2023-04-08 21:48:48,167 - INFO - training.closure - iteration 547: loss = -7.781530714273207
2023-04-08 21:48:49,946 - INFO - training.closure - iteration 548: loss = -7.781751331874695
2023-04-08 21:48:51,717 - INFO - training.closure - iteration 549: loss = -7.782127133651562
2023-04-08 21:48:53,498 - INFO - training.closure - iteration 550: loss = -7.782283416292826
2023-04-08 21:48:55,262 - INFO - training.closure - iteration 551: loss = -7.782385135144622
2023-04-08 21:48:57,018 - INFO - training.closure - iteration 552: loss = -7.782456103692637
2023-04-08 21:48:58,881 - INFO - training.closure - iteration 553: loss = -7.782586453852218
2023-04-08 21:49:00,796 - INFO - training.closure - iteration 554: loss = -7.7828720803800175
2023-04-08 21:49:02,586 - INFO - training.closure - iteration 555: loss = -7.777353359919898
2023-04-08 21:49:04,358 - INFO - training.closure - iteration 556: loss = -7.782896260657543
2023-04-08 21:49:06,218 - INFO - training.closure - iteration 557: loss = -7.783073809977193
2023-04-08 21:49:08,005 - INFO - training.closure - iteration 558: loss = -7.77362898396105
2023-04-08 21:49:09,777 - INFO - training.closure - iteration 559: loss = -7.783115960539426
2023-04-08 21:49:11,535 - INFO - training.closure - iteration 560: loss = -7.783293094344753
2023-04-08 21:49:13,377 - INFO - training.closure - iteration 561: loss = -7.78363083607608
2023-04-08 21:49:15,145 - INFO - training.closure - iteration 562: loss = -7.783978299462877
2023-04-08 21:49:16,919 - INFO - training.closure - iteration 563: loss = -7.784251465873364
2023-04-08 21:49:18,704 - INFO - training.closure - iteration 564: loss = -7.7832682990812
2023-04-08 21:49:20,480 - INFO - training.closure - iteration 565: loss = -7.784376894621126
2023-04-08 21:49:22,366 - INFO - training.closure - iteration 566: loss = -7.784515010098719
2023-04-08 21:49:24,182 - INFO - training.closure - iteration 567: loss = -7.7845667209700435
2023-04-08 21:49:26,287 - INFO - training.closure - iteration 568: loss = -7.784627118326746
2023-04-08 21:49:28,050 - INFO - training.closure - iteration 569: loss = -7.784701313635567
2023-04-08 21:49:29,821 - INFO - training.closure - iteration 570: loss = -7.784824640166951
2023-04-08 21:49:31,611 - INFO - training.closure - iteration 571: loss = -7.785075951933742
2023-04-08 21:49:33,396 - INFO - training.closure - iteration 572: loss = -7.785334979434411
2023-04-08 21:49:35,190 - INFO - training.closure - iteration 573: loss = -7.78553712121966
2023-04-08 21:49:37,118 - INFO - training.closure - iteration 574: loss = -7.785787072646898
2023-04-08 21:49:38,935 - INFO - training.closure - iteration 575: loss = -7.785676011344406
2023-04-08 21:49:40,752 - INFO - training.closure - iteration 576: loss = -7.785849471444492
2023-04-08 21:49:42,592 - INFO - training.closure - iteration 577: loss = -7.785931983966572
2023-04-08 21:49:44,357 - INFO - training.closure - iteration 578: loss = -7.786073627860698
2023-04-08 21:49:46,113 - INFO - training.closure - iteration 579: loss = -7.786179396615541
2023-04-08 21:49:47,866 - INFO - training.closure - iteration 580: loss = -7.7862272286480625
2023-04-08 21:49:49,846 - INFO - training.closure - iteration 581: loss = -7.786292621845623
2023-04-08 21:49:51,692 - INFO - training.closure - iteration 582: loss = -7.786312264407348
2023-04-08 21:49:53,467 - INFO - training.closure - iteration 583: loss = -7.786340488268186
2023-04-08 21:49:55,243 - INFO - training.closure - iteration 584: loss = -7.786373880370348
2023-04-08 21:49:57,004 - INFO - training.closure - iteration 585: loss = -7.7864609945577055
2023-04-08 21:49:58,776 - INFO - training.closure - iteration 586: loss = -7.786579881159907
2023-04-08 21:50:00,613 - INFO - training.closure - iteration 587: loss = -7.786517618407498
2023-04-08 21:50:02,362 - INFO - training.closure - iteration 588: loss = -7.786682460548732
2023-04-08 21:50:04,125 - INFO - training.closure - iteration 589: loss = -7.786881224980711
2023-04-08 21:50:05,891 - INFO - training.closure - iteration 590: loss = -7.787101154511699
2023-04-08 21:50:07,673 - INFO - training.closure - iteration 591: loss = -7.787322419416473
2023-04-08 21:50:09,428 - INFO - training.closure - iteration 592: loss = -7.787653907834277
2023-04-08 21:50:11,189 - INFO - training.closure - iteration 593: loss = -7.787674752979718
2023-04-08 21:50:13,010 - INFO - training.closure - iteration 594: loss = -7.787907444299729
2023-04-08 21:50:14,817 - INFO - training.closure - iteration 595: loss = -7.788419485987706
2023-04-08 21:50:16,591 - INFO - training.closure - iteration 596: loss = -7.788772194256429
2023-04-08 21:50:18,353 - INFO - training.closure - iteration 597: loss = -7.78884470720331
2023-04-08 21:50:20,196 - INFO - training.closure - iteration 598: loss = -7.789407570443208
2023-04-08 21:50:21,961 - INFO - training.closure - iteration 599: loss = -7.789570199874664
2023-04-08 21:50:23,733 - INFO - training.closure - iteration 600: loss = -7.7897793645853906
2023-04-08 21:50:25,547 - INFO - training.closure - iteration 601: loss = -7.790211854242054
2023-04-08 21:50:27,342 - INFO - training.closure - iteration 602: loss = -7.790663078279367
2023-04-08 21:50:29,207 - INFO - training.closure - iteration 603: loss = -7.790870485310434
2023-04-08 21:50:30,986 - INFO - training.closure - iteration 604: loss = -7.790135331884473
2023-04-08 21:50:32,747 - INFO - training.closure - iteration 605: loss = -7.790903706247348
2023-04-08 21:50:34,501 - INFO - training.closure - iteration 606: loss = -7.790978200509226
2023-04-08 21:50:36,266 - INFO - training.closure - iteration 607: loss = -7.791289347132031
2023-04-08 21:50:38,039 - INFO - training.closure - iteration 608: loss = -7.790653521453301
2023-04-08 21:50:39,862 - INFO - training.closure - iteration 609: loss = -7.791442203766408
2023-04-08 21:50:41,648 - INFO - training.closure - iteration 610: loss = -7.791753692518889
2023-04-08 21:50:43,455 - INFO - training.closure - iteration 611: loss = -7.791989689577441
2023-04-08 21:50:45,256 - INFO - training.closure - iteration 612: loss = -7.792217607732018
2023-04-08 21:50:47,057 - INFO - training.closure - iteration 613: loss = -7.79249598156548
2023-04-08 21:50:48,878 - INFO - training.closure - iteration 614: loss = -7.793208713217256
2023-04-08 21:50:50,650 - INFO - training.closure - iteration 615: loss = -7.793749708351916
2023-04-08 21:50:52,407 - INFO - training.closure - iteration 616: loss = -7.793887416461192
2023-04-08 21:50:54,201 - INFO - training.closure - iteration 617: loss = -7.794262298700751
2023-04-08 21:50:56,073 - INFO - training.closure - iteration 618: loss = -7.7947174638447585
2023-04-08 21:50:57,958 - INFO - training.closure - iteration 619: loss = -7.795010527650319
2023-04-08 21:50:59,802 - INFO - training.closure - iteration 620: loss = -7.795136350132269
2023-04-08 21:51:01,738 - INFO - training.closure - iteration 621: loss = -7.7955503349178095
2023-04-08 21:51:03,626 - INFO - training.closure - iteration 622: loss = -7.795981718692733
2023-04-08 21:51:05,430 - INFO - training.closure - iteration 623: loss = -7.796551419187697
2023-04-08 21:51:07,191 - INFO - training.closure - iteration 624: loss = -7.79689257223311
2023-04-08 21:51:08,978 - INFO - training.closure - iteration 625: loss = -7.797153779528741
2023-04-08 21:51:10,749 - INFO - training.closure - iteration 626: loss = -7.7975318255886314
2023-04-08 21:51:12,526 - INFO - training.closure - iteration 627: loss = -7.7974330314694775
2023-04-08 21:51:14,373 - INFO - training.closure - iteration 628: loss = -7.797761064226473
2023-04-08 21:51:16,160 - INFO - training.closure - iteration 629: loss = -7.798047410133801
2023-04-08 21:51:17,954 - INFO - training.closure - iteration 630: loss = -7.798272524824793
2023-04-08 21:51:19,863 - INFO - training.closure - iteration 631: loss = -7.798637225007234
2023-04-08 21:51:21,655 - INFO - training.closure - iteration 632: loss = -7.798871715518327
2023-04-08 21:51:23,469 - INFO - training.closure - iteration 633: loss = -7.799091324003203
2023-04-08 21:51:25,269 - INFO - training.closure - iteration 634: loss = -7.799465139247315
2023-04-08 21:51:27,190 - INFO - training.closure - iteration 635: loss = -7.799625009657561
2023-04-08 21:51:28,962 - INFO - training.closure - iteration 636: loss = -7.800097847975136
2023-04-08 21:51:30,852 - INFO - training.closure - iteration 637: loss = -7.800405055559887
2023-04-08 21:51:32,614 - INFO - training.closure - iteration 638: loss = -7.800788841772281
2023-04-08 21:51:34,447 - INFO - training.closure - iteration 639: loss = -7.800907715418955
2023-04-08 21:51:36,227 - INFO - training.closure - iteration 640: loss = -7.801432647316609
2023-04-08 21:51:38,067 - INFO - training.closure - iteration 641: loss = -7.801715091926388
2023-04-08 21:51:39,814 - INFO - training.closure - iteration 642: loss = -7.802502326216452
2023-04-08 21:51:41,588 - INFO - training.closure - iteration 643: loss = -7.803155049825762
2023-04-08 21:51:43,363 - INFO - training.closure - iteration 644: loss = -7.803548619307504
2023-04-08 21:51:45,112 - INFO - training.closure - iteration 645: loss = -7.803719614883739
2023-04-08 21:51:46,985 - INFO - training.closure - iteration 646: loss = -7.803833132843635
2023-04-08 21:51:48,803 - INFO - training.closure - iteration 647: loss = -7.804019925021393
2023-04-08 21:51:50,871 - INFO - training.closure - iteration 648: loss = -7.804334201079615
2023-04-08 21:51:52,642 - INFO - training.closure - iteration 649: loss = -7.804677602463905
2023-04-08 21:51:54,414 - INFO - training.closure - iteration 650: loss = -7.804997951631007
2023-04-08 21:51:56,281 - INFO - training.closure - iteration 651: loss = -7.805037194905733
2023-04-08 21:51:58,053 - INFO - training.closure - iteration 652: loss = -7.805159324358801
2023-04-08 21:51:59,824 - INFO - training.closure - iteration 653: loss = -7.805195583284998
2023-04-08 21:52:01,585 - INFO - training.closure - iteration 654: loss = -7.805334935953226
2023-04-08 21:52:03,341 - INFO - training.closure - iteration 655: loss = -7.8050998082401435
2023-04-08 21:52:05,166 - INFO - training.closure - iteration 656: loss = -7.805474767960624
2023-04-08 21:52:07,054 - INFO - training.closure - iteration 657: loss = -7.805674709698181
2023-04-08 21:52:08,854 - INFO - training.closure - iteration 658: loss = -7.806117961232416
2023-04-08 21:52:10,842 - INFO - training.closure - iteration 659: loss = -7.806541751926355
2023-04-08 21:52:12,638 - INFO - training.closure - iteration 660: loss = -7.806883407831148
2023-04-08 21:52:14,441 - INFO - training.closure - iteration 661: loss = -7.807464240354898
2023-04-08 21:52:16,214 - INFO - training.closure - iteration 662: loss = -7.807736267495451
2023-04-08 21:52:17,965 - INFO - training.closure - iteration 663: loss = -7.808055443733023
2023-04-08 21:52:19,729 - INFO - training.closure - iteration 664: loss = -7.8082079783128915
2023-04-08 21:52:21,549 - INFO - training.closure - iteration 665: loss = -7.80828289883242
2023-04-08 21:52:23,378 - INFO - training.closure - iteration 666: loss = -7.8083454780237265
2023-04-08 21:52:25,148 - INFO - training.closure - iteration 667: loss = -7.808509585119896
2023-04-08 21:52:26,907 - INFO - training.closure - iteration 668: loss = -7.808598354862171
2023-04-08 21:52:28,748 - INFO - training.closure - iteration 669: loss = -7.808818530808226
2023-04-08 21:52:30,503 - INFO - training.closure - iteration 670: loss = -7.808934485026361
2023-04-08 21:52:32,285 - INFO - training.closure - iteration 671: loss = -7.809000445502208
2023-04-08 21:52:34,057 - INFO - training.closure - iteration 672: loss = -7.809030959842744
2023-04-08 21:52:35,972 - INFO - training.closure - iteration 673: loss = -7.809074778086026
2023-04-08 21:52:37,796 - INFO - training.closure - iteration 674: loss = -7.8091281250739835
2023-04-08 21:52:39,662 - INFO - training.closure - iteration 675: loss = -7.809239080842595
2023-04-08 21:52:41,532 - INFO - training.closure - iteration 676: loss = -7.809335060660969
2023-04-08 21:52:43,310 - INFO - training.closure - iteration 677: loss = -7.809448554349133
2023-04-08 21:52:45,076 - INFO - training.closure - iteration 678: loss = -7.809505288868225
2023-04-08 21:52:46,940 - INFO - training.closure - iteration 679: loss = -7.809561758556986
2023-04-08 21:52:48,757 - INFO - training.closure - iteration 680: loss = -7.809658996365686
2023-04-08 21:52:50,523 - INFO - training.closure - iteration 681: loss = -7.809749625367267
2023-04-08 21:52:52,293 - INFO - training.closure - iteration 682: loss = -7.809977600117751
2023-04-08 21:52:54,058 - INFO - training.closure - iteration 683: loss = -7.81023825536567
2023-04-08 21:52:55,839 - INFO - training.closure - iteration 684: loss = -7.81055785946397
2023-04-08 21:52:57,617 - INFO - training.closure - iteration 685: loss = -7.810829463879634
2023-04-08 21:52:59,391 - INFO - training.closure - iteration 686: loss = -7.811144178736572
2023-04-08 21:53:01,256 - INFO - training.closure - iteration 687: loss = -7.811274100378155
2023-04-08 21:53:03,022 - INFO - training.closure - iteration 688: loss = -7.811438515936574
2023-04-08 21:53:04,794 - INFO - training.closure - iteration 689: loss = -7.810438199986386
2023-04-08 21:53:06,567 - INFO - training.closure - iteration 690: loss = -7.811540232496602
2023-04-08 21:53:08,459 - INFO - training.closure - iteration 691: loss = -7.811633743303398
2023-04-08 21:53:10,225 - INFO - training.closure - iteration 692: loss = -7.811693251911885
2023-04-08 21:53:12,010 - INFO - training.closure - iteration 693: loss = -7.811749280658642
2023-04-08 21:53:13,820 - INFO - training.closure - iteration 694: loss = -7.811794364890892
2023-04-08 21:53:15,597 - INFO - training.closure - iteration 695: loss = -7.811850047473052
2023-04-08 21:53:17,346 - INFO - training.closure - iteration 696: loss = -7.811893851165871
2023-04-08 21:53:19,105 - INFO - training.closure - iteration 697: loss = -7.8119400950497955
2023-04-08 21:53:20,869 - INFO - training.closure - iteration 698: loss = -7.811991632683079
2023-04-08 21:53:22,629 - INFO - training.closure - iteration 699: loss = -7.8120770906755554
2023-04-08 21:53:24,477 - INFO - training.closure - iteration 700: loss = -7.812179800656521
2023-04-08 21:53:26,296 - INFO - training.closure - iteration 701: loss = -7.812378935096257
2023-04-08 21:53:28,135 - INFO - training.closure - iteration 702: loss = -7.812478317245372
2023-04-08 21:53:29,896 - INFO - training.closure - iteration 703: loss = -7.812569700893529
2023-04-08 21:53:31,652 - INFO - training.closure - iteration 704: loss = -7.812704913301893
2023-04-08 21:53:33,421 - INFO - training.closure - iteration 705: loss = -7.812714785661344
2023-04-08 21:53:35,173 - INFO - training.closure - iteration 706: loss = -7.812760314339059
2023-04-08 21:53:36,938 - INFO - training.closure - iteration 707: loss = -7.812768608573105
2023-04-08 21:53:38,952 - INFO - training.closure - iteration 708: loss = -7.8127766363009865
2023-04-08 21:53:40,717 - INFO - training.closure - iteration 709: loss = -7.812797100280359
2023-04-08 21:53:42,562 - INFO - training.closure - iteration 710: loss = -7.812783770012596
2023-04-08 21:53:44,343 - INFO - training.closure - iteration 711: loss = -7.812813125863384
2023-04-08 21:53:46,182 - INFO - training.closure - iteration 712: loss = -7.812837371871823
2023-04-08 21:53:48,067 - INFO - training.closure - iteration 713: loss = -7.8129111591211675
2023-04-08 21:53:49,896 - INFO - training.closure - iteration 714: loss = -7.812981946372883
2023-04-08 21:53:51,738 - INFO - training.closure - iteration 715: loss = -7.813066239206837
2023-04-08 21:53:53,504 - INFO - training.closure - iteration 716: loss = -7.8131670415606465
2023-04-08 21:53:55,255 - INFO - training.closure - iteration 717: loss = -7.8132701297804665
2023-04-08 21:53:57,037 - INFO - training.closure - iteration 718: loss = -7.813365258890803
2023-04-08 21:53:58,796 - INFO - training.closure - iteration 719: loss = -7.813448252700006
2023-04-08 21:54:00,561 - INFO - training.closure - iteration 720: loss = -7.813486487806901
2023-04-08 21:54:02,375 - INFO - training.closure - iteration 721: loss = -7.813511556299233
2023-04-08 21:54:04,143 - INFO - training.closure - iteration 722: loss = -7.813490095482338
2023-04-08 21:54:05,900 - INFO - training.closure - iteration 723: loss = -7.81352673152214
2023-04-08 21:54:07,660 - INFO - training.closure - iteration 724: loss = -7.8135549240611795
2023-04-08 21:54:09,443 - INFO - training.closure - iteration 725: loss = -7.813573316511598
2023-04-08 21:54:11,215 - INFO - training.closure - iteration 726: loss = -7.813593479316435
2023-04-08 21:54:13,095 - INFO - training.closure - iteration 727: loss = -7.813618025443638
2023-04-08 21:54:14,881 - INFO - training.closure - iteration 728: loss = -7.813651826407975
2023-04-08 21:54:16,826 - INFO - training.closure - iteration 729: loss = -7.813677170970539
2023-04-08 21:54:18,643 - INFO - training.closure - iteration 730: loss = -7.81368584903583
2023-04-08 21:54:20,416 - INFO - training.closure - iteration 731: loss = -7.81369030942097
2023-04-08 21:54:22,442 - INFO - training.closure - iteration 732: loss = -7.813693245330059
2023-04-08 21:54:24,281 - INFO - training.closure - iteration 733: loss = -7.81369913539837
2023-04-08 21:54:26,143 - INFO - training.closure - iteration 734: loss = -7.8136778413198815
2023-04-08 21:54:27,960 - INFO - training.closure - iteration 735: loss = -7.813700385712583
2023-04-08 21:54:29,758 - INFO - training.closure - iteration 736: loss = -7.813702545628029
2023-04-08 21:54:31,541 - INFO - training.closure - iteration 737: loss = -7.813709956368784
2023-04-08 21:54:33,395 - INFO - training.closure - iteration 738: loss = -7.813713777478794
2023-04-08 21:54:35,142 - INFO - training.closure - iteration 739: loss = -7.813717299447888
2023-04-08 21:54:36,988 - INFO - training.closure - iteration 740: loss = -7.8137192903720685
2023-04-08 21:54:38,837 - INFO - training.closure - iteration 741: loss = -7.813722813682361
2023-04-08 21:54:40,607 - INFO - training.closure - iteration 742: loss = -7.8137264308532
2023-04-08 21:54:42,367 - INFO - training.closure - iteration 743: loss = -7.813729260044589
2023-04-08 21:54:44,154 - INFO - training.closure - iteration 744: loss = -7.813732538235343
2023-04-08 21:54:45,902 - INFO - training.closure - iteration 745: loss = -7.813748353764055
2023-04-08 21:54:47,653 - INFO - training.closure - iteration 746: loss = -7.813733417622423
2023-04-08 21:54:49,409 - INFO - training.closure - iteration 747: loss = -7.813754775533386
2023-04-08 21:54:51,174 - INFO - training.closure - iteration 748: loss = -7.813763905294552
2023-04-08 21:54:52,959 - INFO - training.closure - iteration 749: loss = -7.813786277420321
2023-04-08 21:54:54,726 - INFO - training.closure - iteration 750: loss = -7.813795496116185
2023-04-08 21:54:56,564 - INFO - training.closure - iteration 751: loss = -7.813799399358135
2023-04-08 21:54:58,379 - INFO - training.closure - iteration 752: loss = -7.813814191481859
2023-04-08 21:55:00,293 - INFO - training.closure - iteration 753: loss = -7.813816946391485
2023-04-08 21:55:02,086 - INFO - training.closure - iteration 754: loss = -7.813818626577156
2023-04-08 21:55:03,873 - INFO - training.closure - iteration 755: loss = -7.813823338842539
2023-04-08 21:55:05,647 - INFO - training.closure - iteration 756: loss = -7.813830687619114
2023-04-08 21:55:07,479 - INFO - training.closure - iteration 757: loss = -7.813881724370894
2023-04-08 21:55:09,253 - INFO - training.closure - iteration 758: loss = -7.813941152996396
2023-04-08 21:55:11,063 - INFO - training.closure - iteration 759: loss = -7.814019676779746
2023-04-08 21:55:12,825 - INFO - training.closure - iteration 760: loss = -7.813969694125877
2023-04-08 21:55:14,621 - INFO - training.closure - iteration 761: loss = -7.814042103020214
2023-04-08 21:55:16,462 - INFO - training.closure - iteration 762: loss = -7.814090030178945
2023-04-08 21:55:18,265 - INFO - training.closure - iteration 763: loss = -7.814124206262608
2023-04-08 21:55:20,037 - INFO - training.closure - iteration 764: loss = -7.8141414298584975
2023-04-08 21:55:21,804 - INFO - training.closure - iteration 765: loss = -7.814156993382227
2023-04-08 21:55:23,582 - INFO - training.closure - iteration 766: loss = -7.81419413701885
2023-04-08 21:55:25,356 - INFO - training.closure - iteration 767: loss = -7.8142322555047485
2023-04-08 21:55:27,118 - INFO - training.closure - iteration 768: loss = -7.814257023839243
2023-04-08 21:55:28,908 - INFO - training.closure - iteration 769: loss = -7.814270978763092
2023-04-08 21:55:30,687 - INFO - training.closure - iteration 770: loss = -7.8142210268618335
2023-04-08 21:55:32,454 - INFO - training.closure - iteration 771: loss = -7.81427391899871
2023-04-08 21:55:34,226 - INFO - training.closure - iteration 772: loss = -7.814284228275415
2023-04-08 21:55:36,069 - INFO - training.closure - iteration 773: loss = -7.814298048403439
2023-04-08 21:55:37,848 - INFO - training.closure - iteration 774: loss = -7.814311333818379
2023-04-08 21:55:39,622 - INFO - training.closure - iteration 775: loss = -7.814321200744197
2023-04-08 21:55:41,396 - INFO - training.closure - iteration 776: loss = -7.814329928649647
2023-04-08 21:55:43,280 - INFO - training.closure - iteration 777: loss = -7.81434394921205
2023-04-08 21:55:45,180 - INFO - training.closure - iteration 778: loss = -7.814345226935405
2023-04-08 21:55:47,011 - INFO - training.closure - iteration 779: loss = -7.814364531890279
2023-04-08 21:55:48,775 - INFO - training.closure - iteration 780: loss = -7.814394572770926
2023-04-08 21:55:50,538 - INFO - training.closure - iteration 781: loss = -7.814459041978712
2023-04-08 21:55:52,407 - INFO - training.closure - iteration 782: loss = -7.814524386970283
2023-04-08 21:55:54,273 - INFO - training.closure - iteration 783: loss = -7.814634866352677
2023-04-08 21:55:56,040 - INFO - training.closure - iteration 784: loss = -7.8147877342965675
2023-04-08 21:55:57,812 - INFO - training.closure - iteration 785: loss = -7.814856501111608
2023-04-08 21:55:59,581 - INFO - training.closure - iteration 786: loss = -7.8150760397776455
2023-04-08 21:56:01,339 - INFO - training.closure - iteration 787: loss = -7.815173873607426
2023-04-08 21:56:03,093 - INFO - training.closure - iteration 788: loss = -7.815212815952264
2023-04-08 21:56:04,845 - INFO - training.closure - iteration 789: loss = -7.81529275979801
2023-04-08 21:56:06,597 - INFO - training.closure - iteration 790: loss = -7.815295050242803
2023-04-08 21:56:08,377 - INFO - training.closure - iteration 791: loss = -7.8153241777247935
2023-04-08 21:56:10,252 - INFO - training.closure - iteration 792: loss = -7.815362617437206
2023-04-08 21:56:12,049 - INFO - training.closure - iteration 793: loss = -7.81542171292126
2023-04-08 21:56:13,907 - INFO - training.closure - iteration 794: loss = -7.815466901731162
2023-04-08 21:56:15,685 - INFO - training.closure - iteration 795: loss = -7.8135064739188635
2023-04-08 21:56:17,454 - INFO - training.closure - iteration 796: loss = -7.815480203422496
2023-04-08 21:56:19,218 - INFO - training.closure - iteration 797: loss = -7.815512375156264
2023-04-08 21:56:21,012 - INFO - training.closure - iteration 798: loss = -7.815562977393845
2023-04-08 21:56:22,783 - INFO - training.closure - iteration 799: loss = -7.8156391294409495
2023-04-08 21:56:24,721 - INFO - training.closure - iteration 800: loss = -7.815749238937807
2023-04-08 21:56:26,475 - INFO - training.closure - iteration 801: loss = -7.815885743842719
2023-04-08 21:56:28,243 - INFO - training.closure - iteration 802: loss = -7.81587644508354
2023-04-08 21:56:30,067 - INFO - training.closure - iteration 803: loss = -7.81591638939485
2023-04-08 21:56:32,289 - INFO - training.closure - iteration 804: loss = -7.815960048276349
2023-04-08 21:56:34,091 - INFO - training.closure - iteration 805: loss = -7.8159693779536275
2023-04-08 21:56:35,893 - INFO - training.closure - iteration 806: loss = -7.815984437364279
2023-04-08 21:56:37,665 - INFO - training.closure - iteration 807: loss = -7.81600413297644
2023-04-08 21:56:39,528 - INFO - training.closure - iteration 808: loss = -7.815957523330963
2023-04-08 21:56:41,292 - INFO - training.closure - iteration 809: loss = -7.816018570911317
2023-04-08 21:56:43,131 - INFO - training.closure - iteration 810: loss = -7.816048486007838
2023-04-08 21:56:44,929 - INFO - training.closure - iteration 811: loss = -7.816066666941497
2023-04-08 21:56:46,746 - INFO - training.closure - iteration 812: loss = -7.816086793210054
2023-04-08 21:56:48,516 - INFO - training.closure - iteration 813: loss = -7.81615726297192
2023-04-08 21:56:50,358 - INFO - training.closure - iteration 814: loss = -7.816232381770259
2023-04-08 21:56:52,229 - INFO - training.closure - iteration 815: loss = -7.816228159467904
2023-04-08 21:56:53,989 - INFO - training.closure - iteration 816: loss = -7.816254830690042
2023-04-08 21:56:55,746 - INFO - training.closure - iteration 817: loss = -7.816282255540656
2023-04-08 21:56:57,569 - INFO - training.closure - iteration 818: loss = -7.816310753622557
2023-04-08 21:56:59,362 - INFO - training.closure - iteration 819: loss = -7.816336110923176
2023-04-08 21:57:01,228 - INFO - training.closure - iteration 820: loss = -7.816391930412456
2023-04-08 21:57:03,032 - INFO - training.closure - iteration 821: loss = -7.816199597659774
2023-04-08 21:57:04,799 - INFO - training.closure - iteration 822: loss = -7.816399485418675
2023-04-08 21:57:06,647 - INFO - training.closure - iteration 823: loss = -7.81642821890461
2023-04-08 21:57:08,419 - INFO - training.closure - iteration 824: loss = -7.816451659282379
2023-04-08 21:57:10,276 - INFO - training.closure - iteration 825: loss = -7.816477013863843
2023-04-08 21:57:12,033 - INFO - training.closure - iteration 826: loss = -7.816509510003341
2023-04-08 21:57:13,906 - INFO - training.closure - iteration 827: loss = -7.816551794742406
2023-04-08 21:57:15,662 - INFO - training.closure - iteration 828: loss = -7.81653068976091
2023-04-08 21:57:17,505 - INFO - training.closure - iteration 829: loss = -7.816586041175754
2023-04-08 21:57:19,270 - INFO - training.closure - iteration 830: loss = -7.816653052310195
2023-04-08 21:57:21,181 - INFO - training.closure - iteration 831: loss = -7.816675527749698
2023-04-08 21:57:23,016 - INFO - training.closure - iteration 832: loss = -7.816702483134961
2023-04-08 21:57:24,904 - INFO - training.closure - iteration 833: loss = -7.8167265686394805
2023-04-08 21:57:26,665 - INFO - training.closure - iteration 834: loss = -7.816567949193281
2023-04-08 21:57:28,494 - INFO - training.closure - iteration 835: loss = -7.816736974264847
2023-04-08 21:57:30,276 - INFO - training.closure - iteration 836: loss = -7.816755613104592
2023-04-08 21:57:32,047 - INFO - training.closure - iteration 837: loss = -7.816781457352262
2023-04-08 21:57:33,892 - INFO - training.closure - iteration 838: loss = -7.816787849705861
2023-04-08 21:57:35,674 - INFO - training.closure - iteration 839: loss = -7.816799012925432
2023-04-08 21:57:37,443 - INFO - training.closure - iteration 840: loss = -7.8168170991115815
2023-04-08 21:57:39,197 - INFO - training.closure - iteration 841: loss = -7.81682719377781
2023-04-08 21:57:40,964 - INFO - training.closure - iteration 842: loss = -7.8168274312806805
2023-04-08 21:57:42,925 - INFO - training.closure - iteration 843: loss = -7.81683349312776
2023-04-08 21:57:44,873 - INFO - training.closure - iteration 844: loss = -7.816839859933813
2023-04-08 21:57:46,651 - INFO - training.closure - iteration 845: loss = -7.81684682338898
2023-04-08 21:57:48,439 - INFO - training.closure - iteration 846: loss = -7.816853200796154
2023-04-08 21:57:50,219 - INFO - training.closure - iteration 847: loss = -7.816864290489098
2023-04-08 21:57:52,007 - INFO - training.closure - iteration 848: loss = -7.816872060902839
2023-04-08 21:57:53,879 - INFO - training.closure - iteration 849: loss = -7.816882119676642
2023-04-08 21:57:55,634 - INFO - training.closure - iteration 850: loss = -7.816887923412839
2023-04-08 21:57:57,578 - INFO - training.closure - iteration 851: loss = -7.816897921179601
2023-04-08 21:57:59,324 - INFO - training.closure - iteration 852: loss = -7.8169043013125785
2023-04-08 21:58:01,108 - INFO - training.closure - iteration 853: loss = -7.8169132216990915
2023-04-08 21:58:02,865 - INFO - training.closure - iteration 854: loss = -7.816918475322561
2023-04-08 21:58:04,714 - INFO - training.closure - iteration 855: loss = -7.816927352970716
2023-04-08 21:58:06,482 - INFO - training.closure - iteration 856: loss = -7.81694438010148
2023-04-08 21:58:08,260 - INFO - training.closure - iteration 857: loss = -7.816674301758794
2023-04-08 21:58:10,036 - INFO - training.closure - iteration 858: loss = -7.816957476063695
2023-04-08 21:58:11,816 - INFO - training.closure - iteration 859: loss = -7.816990464895463
2023-04-08 21:58:13,592 - INFO - training.closure - iteration 860: loss = -7.817033615703499
2023-04-08 21:58:15,363 - INFO - training.closure - iteration 861: loss = -7.817081558839102
2023-04-08 21:58:17,137 - INFO - training.closure - iteration 862: loss = -7.8170696935948305
2023-04-08 21:58:18,892 - INFO - training.closure - iteration 863: loss = -7.8171025674835475
2023-04-08 21:58:20,715 - INFO - training.closure - iteration 864: loss = -7.817126094780856
2023-04-08 21:58:22,550 - INFO - training.closure - iteration 865: loss = -7.817141179407842
2023-04-08 21:58:24,352 - INFO - training.closure - iteration 866: loss = -7.8171670227441625
2023-04-08 21:58:26,195 - INFO - training.closure - iteration 867: loss = -7.81719941170342
2023-04-08 21:58:28,027 - INFO - training.closure - iteration 868: loss = -7.817223621425424
2023-04-08 21:58:29,896 - INFO - training.closure - iteration 869: loss = -7.81731890396064
2023-04-08 21:58:31,682 - INFO - training.closure - iteration 870: loss = -7.817369095818357
2023-04-08 21:58:33,511 - INFO - training.closure - iteration 871: loss = -7.817399112149577
2023-04-08 21:58:35,311 - INFO - training.closure - iteration 872: loss = -7.81741471553563
2023-04-08 21:58:37,102 - INFO - training.closure - iteration 873: loss = -7.81730229091737
2023-04-08 21:58:39,178 - INFO - training.closure - iteration 874: loss = -7.817427087186605
2023-04-08 21:58:41,174 - INFO - training.closure - iteration 875: loss = -7.817458612346386
2023-04-08 21:58:43,100 - INFO - training.closure - iteration 876: loss = -7.817484865164136
2023-04-08 21:58:44,897 - INFO - training.closure - iteration 877: loss = -7.817499255683524
2023-04-08 21:58:46,787 - INFO - training.closure - iteration 878: loss = -7.817478647295047
2023-04-08 21:58:48,843 - INFO - training.closure - iteration 879: loss = -7.817502709532964
2023-04-08 21:58:50,617 - INFO - training.closure - iteration 880: loss = -7.817508468937451
2023-04-08 21:58:52,389 - INFO - training.closure - iteration 881: loss = -7.817521818174642
2023-04-08 21:58:54,213 - INFO - training.closure - iteration 882: loss = -7.8175355578470445
2023-04-08 21:58:55,960 - INFO - training.closure - iteration 883: loss = -7.817559766988066
2023-04-08 21:58:57,808 - INFO - training.closure - iteration 884: loss = -7.817586004730044
2023-04-08 21:58:59,652 - INFO - training.closure - iteration 885: loss = -7.817518802890418
2023-04-08 21:59:01,428 - INFO - training.closure - iteration 886: loss = -7.817595837026289
2023-04-08 21:59:03,228 - INFO - training.closure - iteration 887: loss = -7.817609740984302
2023-04-08 21:59:05,011 - INFO - training.closure - iteration 888: loss = -7.817621145451181
2023-04-08 21:59:06,935 - INFO - training.closure - iteration 889: loss = -7.817628539146444
2023-04-08 21:59:08,771 - INFO - training.closure - iteration 890: loss = -7.817633588324919
2023-04-08 21:59:10,552 - INFO - training.closure - iteration 891: loss = -7.817648202259337
2023-04-08 21:59:12,303 - INFO - training.closure - iteration 892: loss = -7.817684933314204
2023-04-08 21:59:14,100 - INFO - training.closure - iteration 893: loss = -7.817759821978187
2023-04-08 21:59:15,925 - INFO - training.closure - iteration 894: loss = -7.817343458071601
2023-04-08 21:59:17,678 - INFO - training.closure - iteration 895: loss = -7.8177927903888484
2023-04-08 21:59:19,453 - INFO - training.closure - iteration 896: loss = -7.817925482948256
2023-04-08 21:59:21,231 - INFO - training.closure - iteration 897: loss = -7.8180832683383406
2023-04-08 21:59:23,144 - INFO - training.closure - iteration 898: loss = -7.817995678506829
2023-04-08 21:59:24,938 - INFO - training.closure - iteration 899: loss = -7.818102783469142
2023-04-08 21:59:26,710 - INFO - training.closure - iteration 900: loss = -7.818154054177204
2023-04-08 21:59:28,454 - INFO - training.closure - iteration 901: loss = -7.818171132413442
2023-04-08 21:59:30,208 - INFO - training.closure - iteration 902: loss = -7.818185291585132
2023-04-08 21:59:31,971 - INFO - training.closure - iteration 903: loss = -7.818196322378652
2023-04-08 21:59:33,740 - INFO - training.closure - iteration 904: loss = -7.818218572665313
2023-04-08 21:59:35,563 - INFO - training.closure - iteration 905: loss = -7.818226621246091
2023-04-08 21:59:37,328 - INFO - training.closure - iteration 906: loss = -7.818244621759103
2023-04-08 21:59:39,094 - INFO - training.closure - iteration 907: loss = -7.818258677537495
2023-04-08 21:59:40,911 - INFO - training.closure - iteration 908: loss = -7.818283068993377
2023-04-08 21:59:42,720 - INFO - training.closure - iteration 909: loss = -7.818313130735682
2023-04-08 21:59:44,533 - INFO - training.closure - iteration 910: loss = -7.818336600372924
2023-04-08 21:59:46,316 - INFO - training.closure - iteration 911: loss = -7.818358210570338
2023-04-08 21:59:48,112 - INFO - training.closure - iteration 912: loss = -7.81838538956533
2023-04-08 21:59:49,890 - INFO - training.closure - iteration 913: loss = -7.818398277125371
2023-04-08 21:59:51,653 - INFO - training.closure - iteration 914: loss = -7.818418121302457
2023-04-08 21:59:53,640 - INFO - training.closure - iteration 915: loss = -7.818446251521689
2023-04-08 21:59:55,403 - INFO - training.closure - iteration 916: loss = -7.818515003880181
2023-04-08 21:59:57,157 - INFO - training.closure - iteration 917: loss = -7.818579480226415
2023-04-08 21:59:59,046 - INFO - training.closure - iteration 918: loss = -7.818358380756525
2023-04-08 22:00:00,900 - INFO - training.closure - iteration 919: loss = -7.818599240063683
2023-04-08 22:00:02,880 - INFO - training.closure - iteration 920: loss = -7.818641781974387
2023-04-08 22:00:04,673 - INFO - training.closure - iteration 921: loss = -7.81866089227758
2023-04-08 22:00:06,443 - INFO - training.closure - iteration 922: loss = -7.818676209033014
2023-04-08 22:00:08,248 - INFO - training.closure - iteration 923: loss = -7.818668794897553
2023-04-08 22:00:10,007 - INFO - training.closure - iteration 924: loss = -7.818685142379123
2023-04-08 22:00:11,752 - INFO - training.closure - iteration 925: loss = -7.818701909781817
2023-04-08 22:00:13,599 - INFO - training.closure - iteration 926: loss = -7.818724246757208
2023-04-08 22:00:15,356 - INFO - training.closure - iteration 927: loss = -7.818742976113365
2023-04-08 22:00:17,150 - INFO - training.closure - iteration 928: loss = -7.818758629278382
2023-04-08 22:00:18,946 - INFO - training.closure - iteration 929: loss = -7.818768152322509
2023-04-08 22:00:20,741 - INFO - training.closure - iteration 930: loss = -7.818790060725427
2023-04-08 22:00:22,536 - INFO - training.closure - iteration 931: loss = -7.818816679550455
2023-04-08 22:00:24,334 - INFO - training.closure - iteration 932: loss = -7.818850717984741
2023-04-08 22:00:26,102 - INFO - training.closure - iteration 933: loss = -7.818879997544008
2023-04-08 22:00:27,851 - INFO - training.closure - iteration 934: loss = -7.818880505571171
2023-04-08 22:00:29,696 - INFO - training.closure - iteration 935: loss = -7.8188980255437635
2023-04-08 22:00:31,481 - INFO - training.closure - iteration 936: loss = -7.818908856887985
2023-04-08 22:00:33,429 - INFO - training.closure - iteration 937: loss = -7.818924991331704
2023-04-08 22:00:35,227 - INFO - training.closure - iteration 938: loss = -7.818941140947269
2023-04-08 22:00:37,068 - INFO - training.closure - iteration 939: loss = -7.818967398678432
2023-04-08 22:00:38,817 - INFO - training.closure - iteration 940: loss = -7.818953843203441
2023-04-08 22:00:40,620 - INFO - training.closure - iteration 941: loss = -7.818973051330966
2023-04-08 22:00:42,393 - INFO - training.closure - iteration 942: loss = -7.818983012720966
2023-04-08 22:00:44,250 - INFO - training.closure - iteration 943: loss = -7.818986542700794
2023-04-08 22:00:46,038 - INFO - training.closure - iteration 944: loss = -7.818990892898154
2023-04-08 22:00:47,957 - INFO - training.closure - iteration 945: loss = -7.819001104242753
2023-04-08 22:00:49,782 - INFO - training.closure - iteration 946: loss = -7.819021123103822
2023-04-08 22:00:51,621 - INFO - training.closure - iteration 947: loss = -7.8190555051842345
2023-04-08 22:00:53,397 - INFO - training.closure - iteration 948: loss = -7.8187794348999375
2023-04-08 22:00:55,206 - INFO - training.closure - iteration 949: loss = -7.819066737202061
2023-04-08 22:00:56,984 - INFO - training.closure - iteration 950: loss = -7.819100465053435
2023-04-08 22:00:58,775 - INFO - training.closure - iteration 951: loss = -7.819123762306712
2023-04-08 22:01:00,536 - INFO - training.closure - iteration 952: loss = -7.819131409524253
2023-04-08 22:01:02,334 - INFO - training.closure - iteration 953: loss = -7.819146635845698
2023-04-08 22:01:04,093 - INFO - training.closure - iteration 954: loss = -7.819176332962951
2023-04-08 22:01:05,856 - INFO - training.closure - iteration 955: loss = -7.8190967518496155
2023-04-08 22:01:07,858 - INFO - training.closure - iteration 956: loss = -7.819185707747627
2023-04-08 22:01:09,760 - INFO - training.closure - iteration 957: loss = -7.819213666861213
2023-04-08 22:01:11,542 - INFO - training.closure - iteration 958: loss = -7.819225656033171
2023-04-08 22:01:13,311 - INFO - training.closure - iteration 959: loss = -7.81923128642007
2023-04-08 22:01:15,086 - INFO - training.closure - iteration 960: loss = -7.81923742366007
2023-04-08 22:01:16,907 - INFO - training.closure - iteration 961: loss = -7.8192436868764
2023-04-08 22:01:18,770 - INFO - training.closure - iteration 962: loss = -7.819249574860584
2023-04-08 22:01:20,526 - INFO - training.closure - iteration 963: loss = -7.819255943597371
2023-04-08 22:01:22,379 - INFO - training.closure - iteration 964: loss = -7.819261104290332
2023-04-08 22:01:24,242 - INFO - training.closure - iteration 965: loss = -7.81927797775821
2023-04-08 22:01:26,017 - INFO - training.closure - iteration 966: loss = -7.819296428927672
2023-04-08 22:01:27,871 - INFO - training.closure - iteration 967: loss = -7.819303060502155
2023-04-08 22:01:29,709 - INFO - training.closure - iteration 968: loss = -7.819367545243312
2023-04-08 22:01:31,739 - INFO - training.closure - iteration 969: loss = -7.8193874922316615
2023-04-08 22:01:33,650 - INFO - training.closure - iteration 970: loss = -7.81940120325539
2023-04-08 22:01:35,418 - INFO - training.closure - iteration 971: loss = -7.819408434923476
2023-04-08 22:01:37,202 - INFO - training.closure - iteration 972: loss = -7.819415612106864
2023-04-08 22:01:38,973 - INFO - training.closure - iteration 973: loss = -7.819420837497775
2023-04-08 22:01:40,751 - INFO - training.closure - iteration 974: loss = -7.819423334260158
2023-04-08 22:01:42,520 - INFO - training.closure - iteration 975: loss = -7.819426663654692
2023-04-08 22:01:44,427 - INFO - training.closure - iteration 976: loss = -7.819429854469007
2023-04-08 22:01:46,182 - INFO - training.closure - iteration 977: loss = -7.819434973129348
2023-04-08 22:01:47,950 - INFO - training.closure - iteration 978: loss = -7.819440189090231
2023-04-08 22:01:49,721 - INFO - training.closure - iteration 979: loss = -7.81941930683523
2023-04-08 22:01:51,512 - INFO - training.closure - iteration 980: loss = -7.81944162852257
2023-04-08 22:01:53,288 - INFO - training.closure - iteration 981: loss = -7.819445869895729
2023-04-08 22:01:55,064 - INFO - training.closure - iteration 982: loss = -7.81944838716891
2023-04-08 22:01:56,989 - INFO - training.closure - iteration 983: loss = -7.819450166364054
2023-04-08 22:01:58,799 - INFO - training.closure - iteration 984: loss = -7.819452391963143
2023-04-08 22:02:00,751 - INFO - training.closure - iteration 985: loss = -7.819457365451056
2023-04-08 22:02:02,513 - INFO - training.closure - iteration 986: loss = -7.819467368557377
2023-04-08 22:02:04,502 - INFO - training.closure - iteration 987: loss = -7.819445421365637
2023-04-08 22:02:06,258 - INFO - training.closure - iteration 988: loss = -7.819472312673735
2023-04-08 22:02:08,056 - INFO - training.closure - iteration 989: loss = -7.8194897951136175
2023-04-08 22:02:09,865 - INFO - training.closure - iteration 990: loss = -7.819499632230782
2023-04-08 22:02:11,766 - INFO - training.closure - iteration 991: loss = -7.819505584825322
2023-04-08 22:02:13,538 - INFO - training.closure - iteration 992: loss = -7.819508135872346
2023-04-08 22:02:15,304 - INFO - training.closure - iteration 993: loss = -7.819520750432167
2023-04-08 22:02:17,107 - INFO - training.closure - iteration 994: loss = -7.819095615036296
2023-04-08 22:02:18,883 - INFO - training.closure - iteration 995: loss = -7.819529319140166
2023-04-08 22:02:20,647 - INFO - training.closure - iteration 996: loss = -7.819545888038075
2023-04-08 22:02:22,497 - INFO - training.closure - iteration 997: loss = -7.819555253659445
2023-04-08 22:02:24,269 - INFO - training.closure - iteration 998: loss = -7.819558385194588
2023-04-08 22:02:26,036 - INFO - training.closure - iteration 999: loss = -7.819560521598295
2023-04-08 22:02:27,808 - INFO - training.closure - iteration 1000: loss = -7.819567700536046
2023-04-08 22:02:29,587 - INFO - training.closure - iteration 1001: loss = -7.819579867582712
2023-04-08 22:02:31,371 - INFO - training.closure - iteration 1002: loss = -7.819571189294055
2023-04-08 22:02:33,149 - INFO - training.closure - iteration 1003: loss = -7.8195872044395145
2023-04-08 22:02:34,920 - INFO - training.closure - iteration 1004: loss = -7.819606646904919
2023-04-08 22:02:36,773 - INFO - training.closure - iteration 1005: loss = -7.819618720142495
2023-04-08 22:02:38,612 - INFO - training.closure - iteration 1006: loss = -7.819623929263978
2023-04-08 22:02:40,371 - INFO - training.closure - iteration 1007: loss = -7.819629062505919
2023-04-08 22:02:42,208 - INFO - training.closure - iteration 1008: loss = -7.819633435296234
2023-04-08 22:02:43,956 - INFO - training.closure - iteration 1009: loss = -7.819641392206931
2023-04-08 22:02:45,704 - INFO - training.closure - iteration 1010: loss = -7.819647519449621
2023-04-08 22:02:47,457 - INFO - training.closure - iteration 1011: loss = -7.819656840290731
2023-04-08 22:02:49,206 - INFO - training.closure - iteration 1012: loss = -7.819615506570859
2023-04-08 22:02:50,972 - INFO - training.closure - iteration 1013: loss = -7.819666113627008
2023-04-08 22:02:52,726 - INFO - training.closure - iteration 1014: loss = -7.819692025214329
2023-04-08 22:02:54,514 - INFO - training.closure - iteration 1015: loss = -7.819744625270928
2023-04-08 22:02:56,289 - INFO - training.closure - iteration 1016: loss = -7.819791119221009
2023-04-08 22:02:58,043 - INFO - training.closure - iteration 1017: loss = -7.819743095630228
2023-04-08 22:02:59,815 - INFO - training.closure - iteration 1018: loss = -7.819810946319739
2023-04-08 22:03:01,670 - INFO - training.closure - iteration 1019: loss = -7.819818005402541
2023-04-08 22:03:03,427 - INFO - training.closure - iteration 1020: loss = -7.819840081540823
2023-04-08 22:03:05,202 - INFO - training.closure - iteration 1021: loss = -7.819845648732413
2023-04-08 22:03:06,978 - INFO - training.closure - iteration 1022: loss = -7.819852416555591
2023-04-08 22:03:08,749 - INFO - training.closure - iteration 1023: loss = -7.819860197880876
2023-04-08 22:03:10,585 - INFO - training.closure - iteration 1024: loss = -7.8198743738890535
2023-04-08 22:03:12,349 - INFO - training.closure - iteration 1025: loss = -7.819894725915296
2023-04-08 22:03:14,229 - INFO - training.closure - iteration 1026: loss = -7.81988843703315
2023-04-08 22:03:16,006 - INFO - training.closure - iteration 1027: loss = -7.819905129980559
2023-04-08 22:03:17,957 - INFO - training.closure - iteration 1028: loss = -7.8199221474849265
2023-04-08 22:03:19,732 - INFO - training.closure - iteration 1029: loss = -7.819932671296712
2023-04-08 22:03:21,507 - INFO - training.closure - iteration 1030: loss = -7.819950616727441
2023-04-08 22:03:23,272 - INFO - training.closure - iteration 1031: loss = -7.819962498943482
2023-04-08 22:03:25,059 - INFO - training.closure - iteration 1032: loss = -7.819981855543265
2023-04-08 22:03:26,807 - INFO - training.closure - iteration 1033: loss = -7.819998949729785
2023-04-08 22:03:28,560 - INFO - training.closure - iteration 1034: loss = -7.8200322137812135
2023-04-08 22:03:30,361 - INFO - training.closure - iteration 1035: loss = -7.820053579097114
2023-04-08 22:03:32,230 - INFO - training.closure - iteration 1036: loss = -7.820080273449037
2023-04-08 22:03:34,003 - INFO - training.closure - iteration 1037: loss = -7.820117293145727
2023-04-08 22:03:35,852 - INFO - training.closure - iteration 1038: loss = -7.820178984146155
2023-04-08 22:03:37,696 - INFO - training.closure - iteration 1039: loss = -7.820222746588783
2023-04-08 22:03:39,495 - INFO - training.closure - iteration 1040: loss = -7.820254006259538
2023-04-08 22:03:41,274 - INFO - training.closure - iteration 1041: loss = -7.820270078159115
2023-04-08 22:03:43,063 - INFO - training.closure - iteration 1042: loss = -7.820294826189957
2023-04-08 22:03:44,826 - INFO - training.closure - iteration 1043: loss = -7.820338725191672
2023-04-08 22:03:46,592 - INFO - training.closure - iteration 1044: loss = -7.818583776400025
2023-04-08 22:03:48,420 - INFO - training.closure - iteration 1045: loss = -7.820351904850559
2023-04-08 22:03:50,330 - INFO - training.closure - iteration 1046: loss = -7.820381390577255
2023-04-08 22:03:52,101 - INFO - training.closure - iteration 1047: loss = -7.820406390965386
2023-04-08 22:03:53,877 - INFO - training.closure - iteration 1048: loss = -7.820430021097679
2023-04-08 22:03:55,702 - INFO - training.closure - iteration 1049: loss = -7.820461695834943
2023-04-08 22:03:57,457 - INFO - training.closure - iteration 1050: loss = -7.8204737475633355
2023-04-08 22:03:59,265 - INFO - training.closure - iteration 1051: loss = -7.82050668581186
2023-04-08 22:04:01,034 - INFO - training.closure - iteration 1052: loss = -7.8205155925265295
2023-04-08 22:04:02,804 - INFO - training.closure - iteration 1053: loss = -7.820534365305511
2023-04-08 22:04:04,603 - INFO - training.closure - iteration 1054: loss = -7.8205485180004315
2023-04-08 22:04:06,396 - INFO - training.closure - iteration 1055: loss = -7.820549975906283
2023-04-08 22:04:08,288 - INFO - training.closure - iteration 1056: loss = -7.820557781180465
2023-04-08 22:04:10,051 - INFO - training.closure - iteration 1057: loss = -7.8205722432611795
2023-04-08 22:04:11,891 - INFO - training.closure - iteration 1058: loss = -7.8205945546329705
2023-04-08 22:04:13,672 - INFO - training.closure - iteration 1059: loss = -7.820628730041221
2023-04-08 22:04:15,445 - INFO - training.closure - iteration 1060: loss = -7.820678548201445
2023-04-08 22:04:17,215 - INFO - training.closure - iteration 1061: loss = -7.8207356870402265
2023-04-08 22:04:19,141 - INFO - training.closure - iteration 1062: loss = -7.8206981896351095
2023-04-08 22:04:20,994 - INFO - training.closure - iteration 1063: loss = -7.820770394522791
2023-04-08 22:04:22,798 - INFO - training.closure - iteration 1064: loss = -7.820811832037322
2023-04-08 22:04:24,549 - INFO - training.closure - iteration 1065: loss = -7.8208637906226635
2023-04-08 22:04:26,581 - INFO - training.closure - iteration 1066: loss = -7.820902393772749
2023-04-08 22:04:28,349 - INFO - training.closure - iteration 1067: loss = -7.820957102359761
2023-04-08 22:04:30,168 - INFO - training.closure - iteration 1068: loss = -7.821012746603226
2023-04-08 22:04:32,060 - INFO - training.closure - iteration 1069: loss = -7.821044122671276
2023-04-08 22:04:33,834 - INFO - training.closure - iteration 1070: loss = -7.821062853380182
2023-04-08 22:04:35,609 - INFO - training.closure - iteration 1071: loss = -7.8210734085244376
2023-04-08 22:04:37,385 - INFO - training.closure - iteration 1072: loss = -7.821105175761733
2023-04-08 22:04:39,228 - INFO - training.closure - iteration 1073: loss = -7.821161243866875
2023-04-08 22:04:41,087 - INFO - training.closure - iteration 1074: loss = -7.821169990307967
2023-04-08 22:04:42,869 - INFO - training.closure - iteration 1075: loss = -7.821213049642615
2023-04-08 22:04:44,648 - INFO - training.closure - iteration 1076: loss = -7.821275580550315
2023-04-08 22:04:46,417 - INFO - training.closure - iteration 1077: loss = -7.821299512754394
2023-04-08 22:04:48,281 - INFO - training.closure - iteration 1078: loss = -7.821324825403316
2023-04-08 22:04:50,130 - INFO - training.closure - iteration 1079: loss = -7.82134589727402
2023-04-08 22:04:51,906 - INFO - training.closure - iteration 1080: loss = -7.821421367516563
2023-04-08 22:04:53,650 - INFO - training.closure - iteration 1081: loss = -7.8214962284197975
2023-04-08 22:04:55,397 - INFO - training.closure - iteration 1082: loss = -7.821366832540599
2023-04-08 22:04:57,222 - INFO - training.closure - iteration 1083: loss = -7.821528698695897
2023-04-08 22:04:59,065 - INFO - training.closure - iteration 1084: loss = -7.821572760153012
2023-04-08 22:05:00,925 - INFO - training.closure - iteration 1085: loss = -7.821586339151793
2023-04-08 22:05:02,673 - INFO - training.closure - iteration 1086: loss = -7.821600212443996
2023-04-08 22:05:04,484 - INFO - training.closure - iteration 1087: loss = -7.821579680762868
2023-04-08 22:05:06,303 - INFO - training.closure - iteration 1088: loss = -7.821602380549261
2023-04-08 22:05:08,152 - INFO - training.closure - iteration 1089: loss = -7.821606822988866
2023-04-08 22:05:10,179 - INFO - training.closure - iteration 1090: loss = -7.8216162174297015
2023-04-08 22:05:11,949 - INFO - training.closure - iteration 1091: loss = -7.82162295121282
2023-04-08 22:05:13,794 - INFO - training.closure - iteration 1092: loss = -7.821641117994707
2023-04-08 22:05:15,579 - INFO - training.closure - iteration 1093: loss = -7.821675359918371
2023-04-08 22:05:17,353 - INFO - training.closure - iteration 1094: loss = -7.82170490932328
2023-04-08 22:05:19,102 - INFO - training.closure - iteration 1095: loss = -7.821729763122642
2023-04-08 22:05:20,849 - INFO - training.closure - iteration 1096: loss = -7.821756225324134
2023-04-08 22:05:22,666 - INFO - training.closure - iteration 1097: loss = -7.821764542493178
2023-04-08 22:05:24,477 - INFO - training.closure - iteration 1098: loss = -7.821772257341024
2023-04-08 22:05:26,252 - INFO - training.closure - iteration 1099: loss = -7.821784634729052
2023-04-08 22:05:28,028 - INFO - training.closure - iteration 1100: loss = -7.821784318643971
2023-04-08 22:05:30,093 - INFO - training.closure - iteration 1101: loss = -7.821795661848789
2023-04-08 22:05:32,005 - INFO - training.closure - iteration 1102: loss = -7.821818449153273
2023-04-08 22:05:33,772 - INFO - training.closure - iteration 1103: loss = -7.8218453894711715
2023-04-08 22:05:35,546 - INFO - training.closure - iteration 1104: loss = -7.8218617159900345
2023-04-08 22:05:37,317 - INFO - training.closure - iteration 1105: loss = -7.821875068447777
2023-04-08 22:05:39,086 - INFO - training.closure - iteration 1106: loss = -7.821889405982506
2023-04-08 22:05:40,847 - INFO - training.closure - iteration 1107: loss = -7.821904912385142
2023-04-08 22:05:42,695 - INFO - training.closure - iteration 1108: loss = -7.821923709935445
2023-04-08 22:05:44,532 - INFO - training.closure - iteration 1109: loss = -7.821952946427104
2023-04-08 22:05:46,374 - INFO - training.closure - iteration 1110: loss = -7.821971058319026
2023-04-08 22:05:48,134 - INFO - training.closure - iteration 1111: loss = -7.821979149789421
2023-04-08 22:05:49,895 - INFO - training.closure - iteration 1112: loss = -7.822012124952088
2023-04-08 22:05:51,861 - INFO - training.closure - iteration 1113: loss = -7.8220235364070785
2023-04-08 22:05:53,943 - INFO - training.closure - iteration 1114: loss = -7.822037390778696
2023-04-08 22:05:55,726 - INFO - training.closure - iteration 1115: loss = -7.8220646008929595
2023-04-08 22:05:57,482 - INFO - training.closure - iteration 1116: loss = -7.822101211358413
2023-04-08 22:05:59,313 - INFO - training.closure - iteration 1117: loss = -7.821969806832744
2023-04-08 22:06:01,251 - INFO - training.closure - iteration 1118: loss = -7.822116960065669
2023-04-08 22:06:03,038 - INFO - training.closure - iteration 1119: loss = -7.822137807913908
2023-04-08 22:06:04,890 - INFO - training.closure - iteration 1120: loss = -7.822167611527636
2023-04-08 22:06:06,654 - INFO - training.closure - iteration 1121: loss = -7.8221949951193555
2023-04-08 22:06:08,564 - INFO - training.closure - iteration 1122: loss = -7.822221422098658
2023-04-08 22:06:10,335 - INFO - training.closure - iteration 1123: loss = -7.8221559101504585
2023-04-08 22:06:12,247 - INFO - training.closure - iteration 1124: loss = -7.822239213525262
2023-04-08 22:06:14,027 - INFO - training.closure - iteration 1125: loss = -7.822274851571568
2023-04-08 22:06:15,783 - INFO - training.closure - iteration 1126: loss = -7.822287305460688
2023-04-08 22:06:17,574 - INFO - training.closure - iteration 1127: loss = -7.8222972673893665
2023-04-08 22:06:19,340 - INFO - training.closure - iteration 1128: loss = -7.8223168948606805
2023-04-08 22:06:21,108 - INFO - training.closure - iteration 1129: loss = -7.822364643550971
2023-04-08 22:06:23,005 - INFO - training.closure - iteration 1130: loss = -7.822401133472042
2023-04-08 22:06:24,931 - INFO - training.closure - iteration 1131: loss = -7.8224849729404955
2023-04-08 22:06:26,780 - INFO - training.closure - iteration 1132: loss = -7.822597486618589
2023-04-08 22:06:28,652 - INFO - training.closure - iteration 1133: loss = -7.822691250600415
2023-04-08 22:06:30,748 - INFO - training.closure - iteration 1134: loss = -7.822728025258899
2023-04-08 22:11:09,680 - INFO - main.experiment - train - RMSE_b at iteration 0 = 0.12104007267134081
2023-04-08 22:11:09,680 - INFO - main.experiment - train - RMSE_b at last iteration = 0.05343299830339667
2023-04-08 22:11:09,680 - INFO - main.experiment - train - RMSE_a at iteration 0 = 0.24255095422179962
2023-04-08 22:11:09,680 - INFO - main.experiment - train - RMSE_a at last iteration = 0.051690085188325186
2023-04-08 22:11:09,680 - INFO - main.experiment - train - LOGPDF_b at iteration 0 = -0.9221990622756602
2023-04-08 22:11:09,680 - INFO - main.experiment - train - LOGPDF_b at last iteration = -3.0699133743866898
2023-04-08 22:11:09,680 - INFO - main.experiment - train - LOGPDF_a at iteration 0 = 4.3583673665704
2023-04-08 22:11:09,680 - INFO - main.experiment - train - LOGPDF_a at last iteration = -3.1649563762412196
2023-04-08 22:11:09,680 - INFO - main.experiment - train - LOSS at iteration 0 = 3.43616830429474
2023-04-08 22:11:09,680 - INFO - main.experiment - train - LOSS at last iteration = -6.23486975062791
2023-04-08 22:11:59,477 - INFO - main.experiment - test - RMSE_b at iteration 0 = 0.1321029686230339
2023-04-08 22:11:59,477 - INFO - main.experiment - test - RMSE_b at last iteration = 0.07040934235772432
2023-04-08 22:11:59,477 - INFO - main.experiment - test - RMSE_a at iteration 0 = 0.0892232113700829
2023-04-08 22:11:59,477 - INFO - main.experiment - test - RMSE_a at last iteration = 0.03977353638777556
2023-04-08 22:11:59,477 - INFO - main.experiment - test - LOGPDF_b at iteration 0 = -0.597944635580864
2023-04-08 22:11:59,477 - INFO - main.experiment - test - LOGPDF_b at last iteration = 4.323239085201142
2023-04-08 22:11:59,478 - INFO - main.experiment - test - LOGPDF_a at iteration 0 = -1.7049754610036731
2023-04-08 22:11:59,478 - INFO - main.experiment - test - LOGPDF_a at last iteration = 2.293173582481762
2023-04-08 22:11:59,478 - INFO - main.experiment - test - LOSS at iteration 0 = -2.302920096584537
2023-04-08 22:11:59,478 - INFO - main.experiment - test - LOSS at last iteration = 6.616412667682904
2023-04-08 22:11:59,478 - INFO - main.experiment - deep = 5 - plot = True - sigma0 = 0.01
2023-04-08 22:11:59,483 - INFO - training.pre_train_full - empirical mean of x0: 3.0000319984329904
2023-04-08 22:11:59,501 - INFO - training.pre_train_full - initial loss: 12.265865141028847
2023-04-08 22:11:59,522 - INFO - training.closure0 - iteration 0: loss = 12.265865141028847
2023-04-08 22:11:59,548 - INFO - training.closure0 - iteration 1: loss = 6.567511909597451
2023-04-08 22:11:59,570 - INFO - training.closure0 - iteration 2: loss = 5.698984117088218
2023-04-08 22:11:59,594 - INFO - training.closure0 - iteration 3: loss = 4.946693362539229
2023-04-08 22:11:59,614 - INFO - training.closure0 - iteration 4: loss = 4.599205389147149
2023-04-08 22:11:59,630 - INFO - training.closure0 - iteration 5: loss = 4.21948059603673
2023-04-08 22:11:59,648 - INFO - training.closure0 - iteration 6: loss = 3.5284765634506523
2023-04-08 22:11:59,666 - INFO - training.closure0 - iteration 7: loss = 2.648191390684571
2023-04-08 22:11:59,684 - INFO - training.closure0 - iteration 8: loss = 28598.14518979142
2023-04-08 22:11:59,708 - INFO - training.closure0 - iteration 9: loss = 362.83388793889225
2023-04-08 22:11:59,732 - INFO - training.closure0 - iteration 10: loss = 10.712376684735704
2023-04-08 22:11:59,758 - INFO - training.closure0 - iteration 11: loss = 2.572925179347852
2023-04-08 22:11:59,783 - INFO - training.closure0 - iteration 12: loss = 2.4524292332593842
2023-04-08 22:11:59,809 - INFO - training.closure0 - iteration 13: loss = 2.4060212264866956
2023-04-08 22:11:59,829 - INFO - training.closure0 - iteration 14: loss = 2.1868138373782413
2023-04-08 22:11:59,850 - INFO - training.closure0 - iteration 15: loss = 1.1551050605139124
2023-04-08 22:11:59,873 - INFO - training.closure0 - iteration 16: loss = 0.983374100443656
2023-04-08 22:11:59,892 - INFO - training.closure0 - iteration 17: loss = 116189255035.64713
2023-04-08 22:11:59,911 - INFO - training.closure0 - iteration 18: loss = 26565150.685521662
2023-04-08 22:11:59,936 - INFO - training.closure0 - iteration 19: loss = 63632.66902585594
2023-04-08 22:11:59,957 - INFO - training.closure0 - iteration 20: loss = 607.4456894970341
2023-04-08 22:11:59,974 - INFO - training.closure0 - iteration 21: loss = 10.733531115438126
2023-04-08 22:11:59,989 - INFO - training.closure0 - iteration 22: loss = 0.46618538685377253
2023-04-08 22:12:00,007 - INFO - training.closure0 - iteration 23: loss = 0.4283635236073654
2023-04-08 22:12:00,027 - INFO - training.closure0 - iteration 24: loss = 20547.034430198677
2023-04-08 22:12:00,042 - INFO - training.closure0 - iteration 25: loss = 333.4227753141145
2023-04-08 22:12:00,058 - INFO - training.closure0 - iteration 26: loss = 10.098347536802011
2023-04-08 22:12:00,074 - INFO - training.closure0 - iteration 27: loss = 0.5247674022385146
2023-04-08 22:12:00,089 - INFO - training.closure0 - iteration 28: loss = 0.33460907508104304
2023-04-08 22:12:00,107 - INFO - training.closure0 - iteration 29: loss = 20.173045790337355
2023-04-08 22:12:00,124 - INFO - training.closure0 - iteration 30: loss = -0.09585141843995365
2023-04-08 22:12:00,142 - INFO - training.closure0 - iteration 31: loss = -0.22589480797549813
2023-04-08 22:12:00,159 - INFO - training.closure0 - iteration 32: loss = 176.4293295861861
2023-04-08 22:12:00,183 - INFO - training.closure0 - iteration 33: loss = 5.307562961792753
2023-04-08 22:12:00,203 - INFO - training.closure0 - iteration 34: loss = -0.15032828219973854
2023-04-08 22:12:00,225 - INFO - training.closure0 - iteration 35: loss = -0.262846120033654
2023-04-08 22:12:00,253 - INFO - training.closure0 - iteration 36: loss = -0.4045328454979833
2023-04-08 22:12:00,277 - INFO - training.closure0 - iteration 37: loss = -0.409270384267139
2023-04-08 22:12:00,299 - INFO - training.closure0 - iteration 38: loss = -0.41660656212813435
2023-04-08 22:12:00,325 - INFO - training.closure0 - iteration 39: loss = -0.4305588346481829
2023-04-08 22:12:00,349 - INFO - training.closure0 - iteration 40: loss = -0.43041931435739567
2023-04-08 22:12:00,373 - INFO - training.closure0 - iteration 41: loss = -0.44898649238795973
2023-04-08 22:12:00,397 - INFO - training.closure0 - iteration 42: loss = -0.4639944834940457
2023-04-08 22:12:00,424 - INFO - training.closure0 - iteration 43: loss = -0.5507391997456361
2023-04-08 22:12:00,445 - INFO - training.closure0 - iteration 44: loss = 0.647519727057434
2023-04-08 22:12:00,462 - INFO - training.closure0 - iteration 45: loss = -0.5572732192630232
2023-04-08 22:12:00,483 - INFO - training.closure0 - iteration 46: loss = -0.6196602798323128
2023-04-08 22:12:00,503 - INFO - training.closure0 - iteration 47: loss = -0.6663999616686794
2023-04-08 22:12:00,522 - INFO - training.closure0 - iteration 48: loss = -0.6825339035404071
2023-04-08 22:12:00,540 - INFO - training.closure0 - iteration 49: loss = 9397.390027077863
2023-04-08 22:12:00,557 - INFO - training.closure0 - iteration 50: loss = 194.27596385102072
2023-04-08 22:12:00,577 - INFO - training.closure0 - iteration 51: loss = 4.158023213953818
2023-04-08 22:12:00,602 - INFO - training.closure0 - iteration 52: loss = -0.6830723818553175
2023-04-08 22:12:00,627 - INFO - training.closure0 - iteration 53: loss = -0.7449438911731359
2023-04-08 22:12:00,649 - INFO - training.closure0 - iteration 54: loss = -0.8402383502876316
2023-04-08 22:12:00,670 - INFO - training.closure0 - iteration 55: loss = -0.355636943813884
2023-04-08 22:12:00,689 - INFO - training.closure0 - iteration 56: loss = -0.8763569003176942
2023-04-08 22:12:00,705 - INFO - training.closure0 - iteration 57: loss = -0.8914468974183685
2023-04-08 22:12:00,722 - INFO - training.closure0 - iteration 58: loss = -0.8929742794334954
2023-04-08 22:12:00,737 - INFO - training.closure0 - iteration 59: loss = -0.917864812223153
2023-04-08 22:12:00,755 - INFO - training.closure0 - iteration 60: loss = -0.9435956044817493
2023-04-08 22:12:00,779 - INFO - training.closure0 - iteration 61: loss = -1.064139643913509
2023-04-08 22:12:00,804 - INFO - training.closure0 - iteration 62: loss = 0.33067343744727734
2023-04-08 22:12:00,826 - INFO - training.closure0 - iteration 63: loss = -1.0619362182608474
2023-04-08 22:12:00,849 - INFO - training.closure0 - iteration 64: loss = -1.0694914874676689
2023-04-08 22:12:00,868 - INFO - training.closure0 - iteration 65: loss = -1.094272858713217
2023-04-08 22:12:00,887 - INFO - training.closure0 - iteration 66: loss = -1.0973686345626281
2023-04-08 22:12:00,906 - INFO - training.closure0 - iteration 67: loss = -1.0991490992636441
2023-04-08 22:12:00,925 - INFO - training.closure0 - iteration 68: loss = -1.099536255515557
2023-04-08 22:12:00,944 - INFO - training.closure0 - iteration 69: loss = -1.1020064705428183
2023-04-08 22:12:00,962 - INFO - training.closure0 - iteration 70: loss = -1.112046786845725
2023-04-08 22:12:00,979 - INFO - training.closure0 - iteration 71: loss = -0.8755562189773678
2023-04-08 22:12:00,996 - INFO - training.closure0 - iteration 72: loss = -1.1500920857610029
2023-04-08 22:12:01,018 - INFO - training.closure0 - iteration 73: loss = -0.618799799996543
2023-04-08 22:12:01,036 - INFO - training.closure0 - iteration 74: loss = -1.2635748670633982
2023-04-08 22:12:01,057 - INFO - training.closure0 - iteration 75: loss = -0.9587914221659755
2023-04-08 22:12:01,076 - INFO - training.closure0 - iteration 76: loss = -1.397613987047968
2023-04-08 22:12:01,096 - INFO - training.closure0 - iteration 77: loss = 690475952769.2122
2023-04-08 22:12:01,115 - INFO - training.closure0 - iteration 78: loss = 14306522909.368336
2023-04-08 22:12:01,134 - INFO - training.closure0 - iteration 79: loss = 343057253.1798756
2023-04-08 22:12:01,153 - INFO - training.closure0 - iteration 80: loss = 2285010.5195714235
2023-04-08 22:12:01,172 - INFO - training.closure0 - iteration 81: loss = 8905.508681114552
2023-04-08 22:12:01,194 - INFO - training.closure0 - iteration 82: loss = 70.81263664414061
2023-04-08 22:12:01,212 - INFO - training.closure0 - iteration 83: loss = -3.0813510693357262
2023-04-08 22:12:01,231 - INFO - training.closure0 - iteration 84: loss = -3.0543312124179227
2023-04-08 22:12:01,249 - INFO - training.closure0 - iteration 85: loss = -3.135779707836303
2023-04-08 22:12:01,269 - INFO - training.closure0 - iteration 86: loss = 2999.383514382525
2023-04-08 22:12:01,286 - INFO - training.closure0 - iteration 87: loss = 90.9483755583348
2023-04-08 22:12:01,304 - INFO - training.closure0 - iteration 88: loss = -0.07409113945439305
2023-04-08 22:12:01,322 - INFO - training.closure0 - iteration 89: loss = -3.1371314890941773
2023-04-08 22:12:01,338 - INFO - training.closure0 - iteration 90: loss = 14.445373276647652
2023-04-08 22:12:01,354 - INFO - training.closure0 - iteration 91: loss = -2.9343645965987424
2023-04-08 22:12:01,370 - INFO - training.closure0 - iteration 92: loss = -3.1676605312542567
2023-04-08 22:12:01,388 - INFO - training.closure0 - iteration 93: loss = -3.249695227131454
2023-04-08 22:12:01,409 - INFO - training.closure0 - iteration 94: loss = -1.1626241543893276
2023-04-08 22:12:01,428 - INFO - training.closure0 - iteration 95: loss = -3.4323975716412964
2023-04-08 22:12:01,444 - INFO - training.closure0 - iteration 96: loss = -2.997784156190126
2023-04-08 22:12:01,462 - INFO - training.closure0 - iteration 97: loss = -3.4831477998932097
2023-04-08 22:12:01,482 - INFO - training.closure0 - iteration 98: loss = -3.4898344672870367
2023-04-08 22:12:01,500 - INFO - training.closure0 - iteration 99: loss = -3.49356758080478
2023-04-08 22:12:01,518 - INFO - training.closure0 - iteration 100: loss = -3.50193152051532
2023-04-08 22:12:01,538 - INFO - training.closure0 - iteration 101: loss = -3.6034936095786865
2023-04-08 22:12:01,554 - INFO - training.closure0 - iteration 102: loss = -1.849816915343495
2023-04-08 22:12:01,570 - INFO - training.closure0 - iteration 103: loss = -3.7252063302467864
2023-04-08 22:12:01,587 - INFO - training.closure0 - iteration 104: loss = -4.302302422068502
2023-04-08 22:12:01,605 - INFO - training.closure0 - iteration 105: loss = 759899825.8814201
2023-04-08 22:12:01,623 - INFO - training.closure0 - iteration 106: loss = -4.302302478744393
2023-04-08 22:12:01,643 - INFO - training.closure0 - iteration 107: loss = 13373.557006340528
2023-04-08 22:12:01,661 - INFO - training.closure0 - iteration 108: loss = 259.69000607852854
2023-04-08 22:12:01,684 - INFO - training.closure0 - iteration 109: loss = 2.8741368005799104
2023-04-08 22:12:01,706 - INFO - training.closure0 - iteration 110: loss = -4.249281112131755
2023-04-08 22:12:01,726 - INFO - training.closure0 - iteration 111: loss = -4.374070661668975
2023-04-08 22:12:01,746 - INFO - training.closure0 - iteration 112: loss = -0.608475414509226
2023-04-08 22:12:01,766 - INFO - training.closure0 - iteration 113: loss = -4.641622506195654
2023-04-08 22:12:01,785 - INFO - training.closure0 - iteration 114: loss = -4.660152003389885
2023-04-08 22:12:01,803 - INFO - training.closure0 - iteration 115: loss = -4.687648113011306
2023-04-08 22:12:01,825 - INFO - training.closure0 - iteration 116: loss = -4.693836564581176
2023-04-08 22:12:01,850 - INFO - training.closure0 - iteration 117: loss = -4.704592707364609
2023-04-08 22:12:01,878 - INFO - training.closure0 - iteration 118: loss = -4.718055795818324
2023-04-08 22:12:01,902 - INFO - training.closure0 - iteration 119: loss = -4.725561575986347
2023-04-08 22:12:01,923 - INFO - training.closure0 - iteration 120: loss = -4.765955771595154
2023-04-08 22:12:01,941 - INFO - training.closure0 - iteration 121: loss = -4.999273033808331
2023-04-08 22:12:01,957 - INFO - training.closure0 - iteration 122: loss = -5.428440941209766
2023-04-08 22:12:01,972 - INFO - training.closure0 - iteration 123: loss = 7.809346938420022
2023-04-08 22:12:01,988 - INFO - training.closure0 - iteration 124: loss = -5.3157334214068666
2023-04-08 22:12:02,002 - INFO - training.closure0 - iteration 125: loss = -5.582053990464516
2023-04-08 22:12:02,017 - INFO - training.closure0 - iteration 126: loss = 5947.577805720407
2023-04-08 22:12:02,035 - INFO - training.closure0 - iteration 127: loss = 101.0180225315394
2023-04-08 22:12:02,060 - INFO - training.closure0 - iteration 128: loss = -2.9771764723438485
2023-04-08 22:12:02,084 - INFO - training.closure0 - iteration 129: loss = -5.681393961316683
2023-04-08 22:12:02,109 - INFO - training.closure0 - iteration 130: loss = -5.9454111564606
2023-04-08 22:12:02,133 - INFO - training.closure0 - iteration 131: loss = -5.974882185460157
2023-04-08 22:12:02,154 - INFO - training.closure0 - iteration 132: loss = -6.081724791385717
2023-04-08 22:12:02,174 - INFO - training.closure0 - iteration 133: loss = -6.271982151374408
2023-04-08 22:12:02,192 - INFO - training.closure0 - iteration 134: loss = -6.393417016501189
2023-04-08 22:12:02,208 - INFO - training.closure0 - iteration 135: loss = -6.410304735841535
2023-04-08 22:12:02,224 - INFO - training.closure0 - iteration 136: loss = -6.440160887480802
2023-04-08 22:12:02,240 - INFO - training.closure0 - iteration 137: loss = -6.4504848816158935
2023-04-08 22:12:02,259 - INFO - training.closure0 - iteration 138: loss = -6.456471676442159
2023-04-08 22:12:02,281 - INFO - training.closure0 - iteration 139: loss = -6.45685572702318
2023-04-08 22:12:02,303 - INFO - training.closure0 - iteration 140: loss = -6.456883506551618
2023-04-08 22:12:02,325 - INFO - training.closure0 - iteration 141: loss = -6.456909862683195
2023-04-08 22:12:02,345 - INFO - training.closure0 - iteration 142: loss = -6.456911622309291
2023-04-08 22:12:02,361 - INFO - training.closure0 - iteration 143: loss = -6.456911674990788
2023-04-08 22:12:02,378 - INFO - training.closure0 - iteration 144: loss = -6.456911675862885
2023-04-08 22:12:02,394 - INFO - training.closure0 - iteration 145: loss = -6.456911675911172
2023-04-08 22:12:02,410 - INFO - training.closure0 - iteration 146: loss = -6.4569116759113765
2023-04-08 22:12:02,418 - INFO - training.pre_train_full - a0 mean: [2.99944253 3.00062146]
2023-04-08 22:12:02,419 - INFO - training.pre_train_full - a0 var: [1.00409413e-04 8.44174489e-05]
2023-04-08 22:12:02,422 - INFO - training.pre_train_full - a0 covar: [[0.00010040941316125027, -5.508686098286393e-06], [-5.508686098286393e-06, 8.441744894213088e-05]]
2023-04-08 22:12:32,969 - INFO - training.closure - iteration 0: loss = 103943555.23476657
2023-04-08 22:12:36,257 - INFO - training.closure - iteration 1: loss = 2062329.7177526788
2023-04-08 22:12:39,476 - INFO - training.closure - iteration 2: loss = 1675550.122153927
2023-04-08 22:12:42,629 - INFO - training.closure - iteration 3: loss = 255950.34042955385
2023-04-08 22:12:45,722 - INFO - training.closure - iteration 4: loss = 85808.50230241078
2023-04-08 22:12:48,718 - INFO - training.closure - iteration 5: loss = 52145.192291903135
2023-04-08 22:12:51,705 - INFO - training.closure - iteration 6: loss = 36415.781621071
2023-04-08 22:12:54,766 - INFO - training.closure - iteration 7: loss = 18197.003686786033
2023-04-08 22:12:57,589 - INFO - training.closure - iteration 8: loss = 14974.687713047131
2023-04-08 22:13:00,984 - INFO - training.closure - iteration 9: loss = 14793.645125659896
2023-04-08 22:13:04,065 - INFO - training.closure - iteration 10: loss = 14358.480276825641
2023-04-08 22:13:07,067 - INFO - training.closure - iteration 11: loss = 12391.50848561102
2023-04-08 22:13:09,847 - INFO - training.closure - iteration 12: loss = 8275.70661728273
2023-04-08 22:13:12,842 - INFO - training.closure - iteration 13: loss = 4122.280918129148
2023-04-08 22:13:16,265 - INFO - training.closure - iteration 14: loss = 2450.4420846896023
2023-04-08 22:13:19,280 - INFO - training.closure - iteration 15: loss = 1459.0961307966784
2023-04-08 22:13:22,364 - INFO - training.closure - iteration 16: loss = 958.0707861715929
2023-04-08 22:13:25,732 - INFO - training.closure - iteration 17: loss = 694.4590234294276
2023-04-08 22:13:28,747 - INFO - training.closure - iteration 18: loss = 530.9556104348534
2023-04-08 22:13:31,423 - INFO - training.closure - iteration 19: loss = 386.7959936110874
2023-04-08 22:13:34,719 - INFO - training.closure - iteration 20: loss = 269.9614065388897
2023-04-08 22:13:37,831 - INFO - training.closure - iteration 21: loss = 243.3268931608207
2023-04-08 22:13:40,625 - INFO - training.closure - iteration 22: loss = 213.7730610449907
2023-04-08 22:13:43,475 - INFO - training.closure - iteration 23: loss = 161.3662047253564
2023-04-08 22:13:46,487 - INFO - training.closure - iteration 24: loss = 98.63565648972806
2023-04-08 22:13:49,371 - INFO - training.closure - iteration 25: loss = 64.99486188606802
2023-04-08 22:13:52,074 - INFO - training.closure - iteration 26: loss = 49.95898599696427
2023-04-08 22:13:54,778 - INFO - training.closure - iteration 27: loss = 35.6530440703381
2023-04-08 22:13:57,452 - INFO - training.closure - iteration 28: loss = 14.535632075112602
2023-04-08 22:14:00,291 - INFO - training.closure - iteration 29: loss = 7.2441049929475865
2023-04-08 22:14:02,993 - INFO - training.closure - iteration 30: loss = 3.441599851793761
2023-04-08 22:14:05,757 - INFO - training.closure - iteration 31: loss = 2.1260795765792277
2023-04-08 22:14:08,401 - INFO - training.closure - iteration 32: loss = 1.7859003925431054
2023-04-08 22:14:11,222 - INFO - training.closure - iteration 33: loss = 1.6349794136729106
2023-04-08 22:14:13,867 - INFO - training.closure - iteration 34: loss = 1.4395017713447045
2023-04-08 22:14:16,503 - INFO - training.closure - iteration 35: loss = 1.2026204162209524
2023-04-08 22:14:19,139 - INFO - training.closure - iteration 36: loss = 0.9938193731740768
2023-04-08 22:14:21,910 - INFO - training.closure - iteration 37: loss = 0.778955848636647
2023-04-08 22:14:24,628 - INFO - training.closure - iteration 38: loss = 0.48365755359404083
2023-04-08 22:14:27,361 - INFO - training.closure - iteration 39: loss = 0.2108877064706709
2023-04-08 22:14:30,015 - INFO - training.closure - iteration 40: loss = -0.11268503580098344
2023-04-08 22:14:32,820 - INFO - training.closure - iteration 41: loss = -0.19713269787492482
2023-04-08 22:14:35,695 - INFO - training.closure - iteration 42: loss = -0.523597537436232
2023-04-08 22:14:38,343 - INFO - training.closure - iteration 43: loss = -0.8925071141960803
2023-04-08 22:14:40,996 - INFO - training.closure - iteration 44: loss = -1.2696184312369496
2023-04-08 22:14:43,684 - INFO - training.closure - iteration 45: loss = -1.4999953273553537
2023-04-08 22:14:46,496 - INFO - training.closure - iteration 46: loss = -1.74250957562945
2023-04-08 22:14:49,115 - INFO - training.closure - iteration 47: loss = -1.994701639099043
2023-04-08 22:14:51,721 - INFO - training.closure - iteration 48: loss = -2.3777088970762756
2023-04-08 22:14:54,345 - INFO - training.closure - iteration 49: loss = -2.2134565518572975
2023-04-08 22:14:56,946 - INFO - training.closure - iteration 50: loss = -2.571589499316329
2023-04-08 22:14:59,530 - INFO - training.closure - iteration 51: loss = 0.6180993928797687
2023-04-08 22:15:02,288 - INFO - training.closure - iteration 52: loss = -2.8276213470111613
2023-04-08 22:15:04,898 - INFO - training.closure - iteration 53: loss = -2.9532526240653123
2023-04-08 22:15:07,497 - INFO - training.closure - iteration 54: loss = -3.10090117604133
2023-04-08 22:15:10,087 - INFO - training.closure - iteration 55: loss = -3.378059912262949
2023-04-08 22:15:12,703 - INFO - training.closure - iteration 56: loss = -3.5052986611723482
2023-04-08 22:15:15,287 - INFO - training.closure - iteration 57: loss = -3.5306182083616275
2023-04-08 22:15:17,984 - INFO - training.closure - iteration 58: loss = -3.6501869323299587
2023-04-08 22:15:20,583 - INFO - training.closure - iteration 59: loss = -3.767823859449045
2023-04-08 22:15:23,159 - INFO - training.closure - iteration 60: loss = -3.8842970202898854
2023-04-08 22:15:25,748 - INFO - training.closure - iteration 61: loss = -3.838358624738731
2023-04-08 22:15:28,331 - INFO - training.closure - iteration 62: loss = -3.899045804539839
2023-04-08 22:15:30,941 - INFO - training.closure - iteration 63: loss = -3.9092422709575807
2023-04-08 22:15:33,537 - INFO - training.closure - iteration 64: loss = -3.9260604749333776
2023-04-08 22:15:36,220 - INFO - training.closure - iteration 65: loss = -3.9439198067347934
2023-04-08 22:15:38,817 - INFO - training.closure - iteration 66: loss = -3.9898028428212164
2023-04-08 22:15:41,438 - INFO - training.closure - iteration 67: loss = -4.0508989702128915
2023-04-08 22:15:44,077 - INFO - training.closure - iteration 68: loss = -4.073600772135337
2023-04-08 22:15:46,778 - INFO - training.closure - iteration 69: loss = -4.084430014197714
2023-04-08 22:15:49,437 - INFO - training.closure - iteration 70: loss = -4.103327814289965
2023-04-08 22:15:52,105 - INFO - training.closure - iteration 71: loss = -4.125221410687944
2023-04-08 22:15:54,729 - INFO - training.closure - iteration 72: loss = -4.151397915539904
2023-04-08 22:15:57,338 - INFO - training.closure - iteration 73: loss = -4.15715577877593
2023-04-08 22:15:59,961 - INFO - training.closure - iteration 74: loss = -4.159725354788867
2023-04-08 22:16:02,572 - INFO - training.closure - iteration 75: loss = -4.163876455874943
2023-04-08 22:16:05,183 - INFO - training.closure - iteration 76: loss = -4.182639078721482
2023-04-08 22:16:07,875 - INFO - training.closure - iteration 77: loss = -4.206380228760969
2023-04-08 22:16:10,488 - INFO - training.closure - iteration 78: loss = -4.245044449510219
2023-04-08 22:16:13,198 - INFO - training.closure - iteration 79: loss = -4.300389305761957
2023-04-08 22:16:16,076 - INFO - training.closure - iteration 80: loss = -4.329774555295195
2023-04-08 22:16:18,676 - INFO - training.closure - iteration 81: loss = -4.361423919170423
2023-04-08 22:16:21,262 - INFO - training.closure - iteration 82: loss = -4.385644230367766
2023-04-08 22:16:23,864 - INFO - training.closure - iteration 83: loss = -4.391775178253027
2023-04-08 22:16:26,616 - INFO - training.closure - iteration 84: loss = -4.398391393043006
2023-04-08 22:16:29,381 - INFO - training.closure - iteration 85: loss = -4.411805549764774
2023-04-08 22:16:32,072 - INFO - training.closure - iteration 86: loss = -4.442801041186476
2023-04-08 22:16:34,844 - INFO - training.closure - iteration 87: loss = -4.491458636257978
2023-04-08 22:16:37,512 - INFO - training.closure - iteration 88: loss = -4.553858677675532
2023-04-08 22:16:40,162 - INFO - training.closure - iteration 89: loss = -4.574968017747439
2023-04-08 22:16:42,839 - INFO - training.closure - iteration 90: loss = -4.6879800141483425
2023-04-08 22:16:45,581 - INFO - training.closure - iteration 91: loss = -4.825358283308158
2023-04-08 22:16:48,185 - INFO - training.closure - iteration 92: loss = -2.3440068168005044
2023-04-08 22:16:50,918 - INFO - training.closure - iteration 93: loss = -4.957249038388603
2023-04-08 22:16:53,506 - INFO - training.closure - iteration 94: loss = -4.416136575621686
2023-04-08 22:16:56,092 - INFO - training.closure - iteration 95: loss = -5.07369115658677
2023-04-08 22:16:58,796 - INFO - training.closure - iteration 96: loss = -1.7598645355179734
2023-04-08 22:17:01,423 - INFO - training.closure - iteration 97: loss = -5.120498879573093
2023-04-08 22:17:04,037 - INFO - training.closure - iteration 98: loss = -5.220907169336624
2023-04-08 22:17:06,656 - INFO - training.closure - iteration 99: loss = -5.302183350920177
2023-04-08 22:17:09,265 - INFO - training.closure - iteration 100: loss = -5.333616216892706
2023-04-08 22:17:11,883 - INFO - training.closure - iteration 101: loss = -5.365113655668952
2023-04-08 22:17:14,600 - INFO - training.closure - iteration 102: loss = -5.3995354347286675
2023-04-08 22:17:17,286 - INFO - training.closure - iteration 103: loss = -5.4420409364400735
2023-04-08 22:17:19,889 - INFO - training.closure - iteration 104: loss = -5.484149551923106
2023-04-08 22:17:22,537 - INFO - training.closure - iteration 105: loss = -5.552475856562183
2023-04-08 22:17:25,202 - INFO - training.closure - iteration 106: loss = -5.624666720076442
2023-04-08 22:17:27,811 - INFO - training.closure - iteration 107: loss = -5.655092355613691
2023-04-08 22:17:30,774 - INFO - training.closure - iteration 108: loss = -5.6852615576312955
2023-04-08 22:17:33,647 - INFO - training.closure - iteration 109: loss = -5.714205044918119
2023-04-08 22:17:36,266 - INFO - training.closure - iteration 110: loss = -5.787059211012747
2023-04-08 22:17:38,884 - INFO - training.closure - iteration 111: loss = -5.8597313973282175
2023-04-08 22:17:41,516 - INFO - training.closure - iteration 112: loss = -5.891792329354082
2023-04-08 22:17:44,170 - INFO - training.closure - iteration 113: loss = -6.039337386022958
2023-04-08 22:17:46,815 - INFO - training.closure - iteration 114: loss = -6.255708417304294
2023-04-08 22:17:49,512 - INFO - training.closure - iteration 115: loss = -6.308544849914317
2023-04-08 22:17:52,093 - INFO - training.closure - iteration 116: loss = -6.337009997964729
2023-04-08 22:17:54,652 - INFO - training.closure - iteration 117: loss = -6.509391213249332
2023-04-08 22:17:57,223 - INFO - training.closure - iteration 118: loss = -5.315127587198876
2023-04-08 22:17:59,783 - INFO - training.closure - iteration 119: loss = -6.6670398872530034
2023-04-08 22:18:02,499 - INFO - training.closure - iteration 120: loss = -6.720633701633841
2023-04-08 22:18:05,282 - INFO - training.closure - iteration 121: loss = -6.817516980105835
2023-04-08 22:18:07,985 - INFO - training.closure - iteration 122: loss = -6.8790941321775
2023-04-08 22:18:10,625 - INFO - training.closure - iteration 123: loss = -7.033568228686503
2023-04-08 22:18:13,304 - INFO - training.closure - iteration 124: loss = -7.0811216786513205
2023-04-08 22:18:15,995 - INFO - training.closure - iteration 125: loss = -7.108137298328948
2023-04-08 22:18:18,677 - INFO - training.closure - iteration 126: loss = -7.121281376904914
2023-04-08 22:18:21,372 - INFO - training.closure - iteration 127: loss = -7.130107923974048
2023-04-08 22:18:24,258 - INFO - training.closure - iteration 128: loss = -7.145952580189857
2023-04-08 22:18:26,917 - INFO - training.closure - iteration 129: loss = -7.158027052381202
2023-04-08 22:18:29,579 - INFO - training.closure - iteration 130: loss = -7.180876962854013
2023-04-08 22:18:32,246 - INFO - training.closure - iteration 131: loss = -7.228704646508129
2023-04-08 22:18:34,983 - INFO - training.closure - iteration 132: loss = -7.299004084339144
2023-04-08 22:18:37,616 - INFO - training.closure - iteration 133: loss = -7.3512144560310775
2023-04-08 22:18:40,424 - INFO - training.closure - iteration 134: loss = -7.408673764567539
2023-04-08 22:18:43,242 - INFO - training.closure - iteration 135: loss = -7.490218849876149
2023-04-08 22:18:46,055 - INFO - training.closure - iteration 136: loss = -7.504655732403483
2023-04-08 22:18:48,726 - INFO - training.closure - iteration 137: loss = -7.520141074186246
2023-04-08 22:18:51,467 - INFO - training.closure - iteration 138: loss = -7.527480240532831
2023-04-08 22:18:54,121 - INFO - training.closure - iteration 139: loss = -7.5491420574861605
2023-04-08 22:18:56,786 - INFO - training.closure - iteration 140: loss = -7.597676487775786
2023-04-08 22:18:59,587 - INFO - training.closure - iteration 141: loss = -7.589266672840468
2023-04-08 22:19:02,355 - INFO - training.closure - iteration 142: loss = -7.614813305241444
2023-04-08 22:19:05,065 - INFO - training.closure - iteration 143: loss = -7.6422316118756815
2023-04-08 22:19:07,720 - INFO - training.closure - iteration 144: loss = -7.6845761253799925
2023-04-08 22:19:10,612 - INFO - training.closure - iteration 145: loss = -7.720456646344234
2023-04-08 22:19:13,323 - INFO - training.closure - iteration 146: loss = -7.783737990825986
2023-04-08 22:19:16,192 - INFO - training.closure - iteration 147: loss = -7.8095790874398805
2023-04-08 22:19:18,908 - INFO - training.closure - iteration 148: loss = -7.81512449091331
2023-04-08 22:19:21,565 - INFO - training.closure - iteration 149: loss = -7.818584870653657
2023-04-08 22:19:24,202 - INFO - training.closure - iteration 150: loss = -7.8196431452732496
2023-04-08 22:19:26,869 - INFO - training.closure - iteration 151: loss = -7.820908204293154
2023-04-08 22:19:29,516 - INFO - training.closure - iteration 152: loss = -7.821776642665674
2023-04-08 22:19:32,421 - INFO - training.closure - iteration 153: loss = -7.8245910267869005
2023-04-08 22:19:35,085 - INFO - training.closure - iteration 154: loss = -7.831737657892615
2023-04-08 22:19:37,740 - INFO - training.closure - iteration 155: loss = -7.834475445515724
2023-04-08 22:19:40,389 - INFO - training.closure - iteration 156: loss = -7.837153399512983
2023-04-08 22:19:43,049 - INFO - training.closure - iteration 157: loss = -7.839634818005037
2023-04-08 22:19:45,687 - INFO - training.closure - iteration 158: loss = -7.844392823404056
2023-04-08 22:19:48,440 - INFO - training.closure - iteration 159: loss = -7.8475614188995735
2023-04-08 22:19:51,165 - INFO - training.closure - iteration 160: loss = -7.851362785927184
2023-04-08 22:19:53,929 - INFO - training.closure - iteration 161: loss = -7.859809423966194
2023-04-08 22:19:56,592 - INFO - training.closure - iteration 162: loss = -7.872674607824702
2023-04-08 22:19:59,218 - INFO - training.closure - iteration 163: loss = -7.896340463158366
2023-04-08 22:20:01,880 - INFO - training.closure - iteration 164: loss = -7.932852945008033
2023-04-08 22:20:04,560 - INFO - training.closure - iteration 165: loss = -7.991444728973811
2023-04-08 22:20:07,269 - INFO - training.closure - iteration 166: loss = -7.777514434588043
2023-04-08 22:20:09,917 - INFO - training.closure - iteration 167: loss = -8.03609618454308
2023-04-08 22:20:12,644 - INFO - training.closure - iteration 168: loss = -8.064991007356758
2023-04-08 22:20:15,286 - INFO - training.closure - iteration 169: loss = -7.264777027682248
2023-04-08 22:20:18,036 - INFO - training.closure - iteration 170: loss = -8.115946141179343
2023-04-08 22:20:20,702 - INFO - training.closure - iteration 171: loss = -8.138147024068026
2023-04-08 22:20:23,446 - INFO - training.closure - iteration 172: loss = -8.19930772630066
2023-04-08 22:20:26,082 - INFO - training.closure - iteration 173: loss = -8.225576300784137
2023-04-08 22:20:28,750 - INFO - training.closure - iteration 174: loss = -8.246154530193945
2023-04-08 22:20:31,525 - INFO - training.closure - iteration 175: loss = -8.261787322612747
2023-04-08 22:20:34,187 - INFO - training.closure - iteration 176: loss = -8.277976180284817
2023-04-08 22:20:36,958 - INFO - training.closure - iteration 177: loss = -8.304251154591363
2023-04-08 22:20:39,671 - INFO - training.closure - iteration 178: loss = -8.348970535524902
2023-04-08 22:20:42,591 - INFO - training.closure - iteration 179: loss = -8.3587313121713
2023-04-08 22:20:45,329 - INFO - training.closure - iteration 180: loss = -8.375311777986441
2023-04-08 22:20:48,096 - INFO - training.closure - iteration 181: loss = -8.390968672648704
2023-04-08 22:20:50,965 - INFO - training.closure - iteration 182: loss = -8.412428619073214
2023-04-08 22:20:53,609 - INFO - training.closure - iteration 183: loss = -8.444449487343924
2023-04-08 22:20:56,262 - INFO - training.closure - iteration 184: loss = -8.481352738653504
2023-04-08 22:20:59,061 - INFO - training.closure - iteration 185: loss = -8.530246833542643
2023-04-08 22:21:01,806 - INFO - training.closure - iteration 186: loss = -8.55727533582584
2023-04-08 22:21:04,494 - INFO - training.closure - iteration 187: loss = -8.56944962916627
2023-04-08 22:21:07,133 - INFO - training.closure - iteration 188: loss = -8.614334860060614
2023-04-08 22:21:10,005 - INFO - training.closure - iteration 189: loss = -8.667340982870856
2023-04-08 22:21:12,810 - INFO - training.closure - iteration 190: loss = -8.473600087627592
2023-04-08 22:21:15,505 - INFO - training.closure - iteration 191: loss = -8.698044132052662
2023-04-08 22:21:18,131 - INFO - training.closure - iteration 192: loss = -8.811862180582308
2023-04-08 22:21:20,769 - INFO - training.closure - iteration 193: loss = 12.83882355876305
2023-04-08 22:21:23,440 - INFO - training.closure - iteration 194: loss = -8.014436739353815
2023-04-08 22:21:26,156 - INFO - training.closure - iteration 195: loss = -8.836405258850935
2023-04-08 22:21:28,887 - INFO - training.closure - iteration 196: loss = -8.896706927256602
2023-04-08 22:21:31,504 - INFO - training.closure - iteration 197: loss = -8.888379433270206
2023-04-08 22:21:34,240 - INFO - training.closure - iteration 198: loss = -8.94669058903173
2023-04-08 22:21:36,884 - INFO - training.closure - iteration 199: loss = -8.981849679177298
2023-04-08 22:21:39,677 - INFO - training.closure - iteration 200: loss = -8.958505041428062
2023-04-08 22:21:42,326 - INFO - training.closure - iteration 201: loss = -9.023073075362271
2023-04-08 22:21:44,989 - INFO - training.closure - iteration 202: loss = -9.068689837822006
2023-04-08 22:21:47,641 - INFO - training.closure - iteration 203: loss = -9.123273111680794
2023-04-08 22:21:50,358 - INFO - training.closure - iteration 204: loss = -9.195889285459485
2023-04-08 22:21:53,009 - INFO - training.closure - iteration 205: loss = -7.741489222076759
2023-04-08 22:21:55,628 - INFO - training.closure - iteration 206: loss = -9.203502103810234
2023-04-08 22:21:58,288 - INFO - training.closure - iteration 207: loss = -9.220549250665039
2023-04-08 22:22:00,938 - INFO - training.closure - iteration 208: loss = -9.30231985220385
2023-04-08 22:22:03,693 - INFO - training.closure - iteration 209: loss = -9.357212837377629
2023-04-08 22:22:06,519 - INFO - training.closure - iteration 210: loss = -9.404269452464792
2023-04-08 22:22:09,376 - INFO - training.closure - iteration 211: loss = -8.89264434709659
2023-04-08 22:22:12,088 - INFO - training.closure - iteration 212: loss = -9.416769476668687
2023-04-08 22:22:14,751 - INFO - training.closure - iteration 213: loss = -9.427500599233504
2023-04-08 22:22:17,375 - INFO - training.closure - iteration 214: loss = -9.450582759060335
2023-04-08 22:22:20,181 - INFO - training.closure - iteration 215: loss = -9.47084093358133
2023-04-08 22:22:22,832 - INFO - training.closure - iteration 216: loss = -9.487827022889242
2023-04-08 22:22:25,538 - INFO - training.closure - iteration 217: loss = -9.494616633998069
2023-04-08 22:22:28,184 - INFO - training.closure - iteration 218: loss = -9.504942264061663
2023-04-08 22:22:30,825 - INFO - training.closure - iteration 219: loss = -9.525057494877938
2023-04-08 22:22:33,507 - INFO - training.closure - iteration 220: loss = -9.289165154574354
2023-04-08 22:22:36,131 - INFO - training.closure - iteration 221: loss = -9.530414803857543
2023-04-08 22:22:38,784 - INFO - training.closure - iteration 222: loss = -9.545998022921493
2023-04-08 22:22:41,569 - INFO - training.closure - iteration 223: loss = -9.556618124673676
2023-04-08 22:22:44,223 - INFO - training.closure - iteration 224: loss = -9.572879616917339
2023-04-08 22:22:46,866 - INFO - training.closure - iteration 225: loss = -9.579725360691164
2023-04-08 22:22:49,558 - INFO - training.closure - iteration 226: loss = -9.591785936724596
2023-04-08 22:22:52,205 - INFO - training.closure - iteration 227: loss = -9.59616813280566
2023-04-08 22:22:54,855 - INFO - training.closure - iteration 228: loss = -9.60071389322585
2023-04-08 22:22:57,576 - INFO - training.closure - iteration 229: loss = -9.621850143661266
2023-04-08 22:23:00,250 - INFO - training.closure - iteration 230: loss = -9.635644881373388
2023-04-08 22:23:02,940 - INFO - training.closure - iteration 231: loss = -9.65442502561089
2023-04-08 22:23:05,602 - INFO - training.closure - iteration 232: loss = -9.690713638981356
2023-04-08 22:23:08,342 - INFO - training.closure - iteration 233: loss = -9.705836668763144
2023-04-08 22:23:11,100 - INFO - training.closure - iteration 234: loss = -9.71756639195484
2023-04-08 22:23:13,756 - INFO - training.closure - iteration 235: loss = -9.728344096674258
2023-04-08 22:23:16,477 - INFO - training.closure - iteration 236: loss = -9.739795138163961
2023-04-08 22:23:19,102 - INFO - training.closure - iteration 237: loss = -9.762080176423984
2023-04-08 22:23:21,772 - INFO - training.closure - iteration 238: loss = -8.98178771718652
2023-04-08 22:23:24,409 - INFO - training.closure - iteration 239: loss = -9.772299327266438
2023-04-08 22:23:27,053 - INFO - training.closure - iteration 240: loss = -9.82066842379608
2023-04-08 22:23:29,703 - INFO - training.closure - iteration 241: loss = -9.878716783073006
2023-04-08 22:23:32,435 - INFO - training.closure - iteration 242: loss = -9.925897807432019
2023-04-08 22:23:35,095 - INFO - training.closure - iteration 243: loss = -9.643969213960556
2023-04-08 22:23:37,746 - INFO - training.closure - iteration 244: loss = -9.935189273321647
2023-04-08 22:23:40,389 - INFO - training.closure - iteration 245: loss = -9.969151364794632
2023-04-08 22:23:43,046 - INFO - training.closure - iteration 246: loss = -10.055605554136461
2023-04-08 22:23:45,748 - INFO - training.closure - iteration 247: loss = -10.124093381596607
2023-04-08 22:23:48,560 - INFO - training.closure - iteration 248: loss = -10.121756049125668
2023-04-08 22:23:51,211 - INFO - training.closure - iteration 249: loss = -10.147084464218636
2023-04-08 22:23:53,836 - INFO - training.closure - iteration 250: loss = -10.188543490226955
2023-04-08 22:23:56,464 - INFO - training.closure - iteration 251: loss = -8.947456072951715
2023-04-08 22:23:59,091 - INFO - training.closure - iteration 252: loss = -10.19453586860984
2023-04-08 22:24:01,760 - INFO - training.closure - iteration 253: loss = -10.21908567601194
2023-04-08 22:24:04,415 - INFO - training.closure - iteration 254: loss = -10.236344685914219
2023-04-08 22:24:07,130 - INFO - training.closure - iteration 255: loss = -10.25534918159611
2023-04-08 22:24:09,745 - INFO - training.closure - iteration 256: loss = -10.280742149078758
2023-04-08 22:24:12,441 - INFO - training.closure - iteration 257: loss = -10.298072509011226
2023-04-08 22:24:15,092 - INFO - training.closure - iteration 258: loss = -10.314154245391123
2023-04-08 22:24:17,734 - INFO - training.closure - iteration 259: loss = -10.323200480779192
2023-04-08 22:24:20,399 - INFO - training.closure - iteration 260: loss = -10.329064851394623
2023-04-08 22:24:23,091 - INFO - training.closure - iteration 261: loss = -10.336151769324463
2023-04-08 22:24:25,752 - INFO - training.closure - iteration 262: loss = -10.346889279297724
2023-04-08 22:24:28,410 - INFO - training.closure - iteration 263: loss = -10.35588486282119
2023-04-08 22:24:31,084 - INFO - training.closure - iteration 264: loss = -10.364561406331008
2023-04-08 22:24:33,739 - INFO - training.closure - iteration 265: loss = -10.372355941060874
2023-04-08 22:24:36,479 - INFO - training.closure - iteration 266: loss = -10.391508380603943
2023-04-08 22:24:39,231 - INFO - training.closure - iteration 267: loss = -10.391569972823671
2023-04-08 22:24:41,871 - INFO - training.closure - iteration 268: loss = -10.40177051419818
2023-04-08 22:24:44,518 - INFO - training.closure - iteration 269: loss = -10.413517599573737
2023-04-08 22:24:47,146 - INFO - training.closure - iteration 270: loss = -10.420585336406095
2023-04-08 22:24:49,766 - INFO - training.closure - iteration 271: loss = -10.424768914071452
2023-04-08 22:24:52,383 - INFO - training.closure - iteration 272: loss = -10.43009848923786
2023-04-08 22:24:55,013 - INFO - training.closure - iteration 273: loss = -10.434685338299213
2023-04-08 22:24:57,721 - INFO - training.closure - iteration 274: loss = -10.440012930290887
2023-04-08 22:25:00,361 - INFO - training.closure - iteration 275: loss = -10.430943637267315
2023-04-08 22:25:03,005 - INFO - training.closure - iteration 276: loss = -10.442149371172103
2023-04-08 22:25:05,746 - INFO - training.closure - iteration 277: loss = -10.44417266298527
2023-04-08 22:25:08,365 - INFO - training.closure - iteration 278: loss = -10.445958025360328
2023-04-08 22:25:11,144 - INFO - training.closure - iteration 279: loss = -10.368271401948011
2023-04-08 22:25:13,877 - INFO - training.closure - iteration 280: loss = -10.44707359158544
2023-04-08 22:25:16,509 - INFO - training.closure - iteration 281: loss = -10.451635188547872
2023-04-08 22:25:19,132 - INFO - training.closure - iteration 282: loss = -10.459401526373416
2023-04-08 22:25:21,772 - INFO - training.closure - iteration 283: loss = -10.461073205285661
2023-04-08 22:25:24,885 - INFO - training.closure - iteration 284: loss = -10.46856586486836
2023-04-08 22:25:27,552 - INFO - training.closure - iteration 285: loss = -10.471213589114619
2023-04-08 22:25:30,309 - INFO - training.closure - iteration 286: loss = -10.471962754637374
2023-04-08 22:25:33,027 - INFO - training.closure - iteration 287: loss = -10.472552400751903
2023-04-08 22:25:35,736 - INFO - training.closure - iteration 288: loss = -10.473000507882492
2023-04-08 22:25:38,407 - INFO - training.closure - iteration 289: loss = -10.474793347702489
2023-04-08 22:25:41,077 - INFO - training.closure - iteration 290: loss = -10.47645843601612
2023-04-08 22:25:43,762 - INFO - training.closure - iteration 291: loss = -10.48404916016361
2023-04-08 22:25:46,421 - INFO - training.closure - iteration 292: loss = -10.498051984224647
2023-04-08 22:25:49,200 - INFO - training.closure - iteration 293: loss = -10.518913925846746
2023-04-08 22:25:52,090 - INFO - training.closure - iteration 294: loss = -10.601365167992077
2023-04-08 22:25:54,738 - INFO - training.closure - iteration 295: loss = -10.57395952235268
2023-04-08 22:25:57,384 - INFO - training.closure - iteration 296: loss = -10.614530365429104
2023-04-08 22:26:00,157 - INFO - training.closure - iteration 297: loss = -10.62936129940497
2023-04-08 22:26:02,854 - INFO - training.closure - iteration 298: loss = -10.635947710208365
2023-04-08 22:26:05,575 - INFO - training.closure - iteration 299: loss = -10.63838691856239
2023-04-08 22:26:08,247 - INFO - training.closure - iteration 300: loss = -10.639978679340258
2023-04-08 22:26:10,943 - INFO - training.closure - iteration 301: loss = -10.642011685262892
2023-04-08 22:26:13,598 - INFO - training.closure - iteration 302: loss = -10.644011164178465
2023-04-08 22:26:16,254 - INFO - training.closure - iteration 303: loss = -10.646353283761155
2023-04-08 22:26:19,047 - INFO - training.closure - iteration 304: loss = -10.651448252260089
2023-04-08 22:26:21,866 - INFO - training.closure - iteration 305: loss = -10.658199361380095
2023-04-08 22:26:24,546 - INFO - training.closure - iteration 306: loss = -10.66442691389022
2023-04-08 22:26:27,193 - INFO - training.closure - iteration 307: loss = -10.67096382950026
2023-04-08 22:26:29,927 - INFO - training.closure - iteration 308: loss = -10.679179437954254
2023-04-08 22:26:32,584 - INFO - training.closure - iteration 309: loss = -10.703271555339793
2023-04-08 22:26:35,228 - INFO - training.closure - iteration 310: loss = -10.733534149103066
2023-04-08 22:26:37,864 - INFO - training.closure - iteration 311: loss = -10.755662948068377
2023-04-08 22:26:40,560 - INFO - training.closure - iteration 312: loss = -10.760904586736824
2023-04-08 22:26:43,220 - INFO - training.closure - iteration 313: loss = -10.76894372324142
2023-04-08 22:26:45,883 - INFO - training.closure - iteration 314: loss = -10.767719281144581
2023-04-08 22:26:48,531 - INFO - training.closure - iteration 315: loss = -10.77119583324168
2023-04-08 22:26:51,169 - INFO - training.closure - iteration 316: loss = -10.777538795318176
2023-04-08 22:26:54,098 - INFO - training.closure - iteration 317: loss = -10.787167399929402
2023-04-08 22:26:56,928 - INFO - training.closure - iteration 318: loss = -10.789760681075004
2023-04-08 22:26:59,584 - INFO - training.closure - iteration 319: loss = -10.796385437847327
2023-04-08 22:27:02,217 - INFO - training.closure - iteration 320: loss = -10.801301539956786
2023-04-08 22:27:04,860 - INFO - training.closure - iteration 321: loss = -10.810138329961042
2023-04-08 22:27:07,525 - INFO - training.closure - iteration 322: loss = -10.823009947541308
2023-04-08 22:27:10,165 - INFO - training.closure - iteration 323: loss = -10.837472695745085
2023-04-08 22:27:12,886 - INFO - training.closure - iteration 324: loss = -10.843519469620055
2023-04-08 22:27:15,581 - INFO - training.closure - iteration 325: loss = -10.847328794012835
2023-04-08 22:27:18,294 - INFO - training.closure - iteration 326: loss = -10.851135237228679
2023-04-08 22:27:21,123 - INFO - training.closure - iteration 327: loss = -10.857002340412297
2023-04-08 22:27:23,786 - INFO - training.closure - iteration 328: loss = -10.85892362301843
2023-04-08 22:27:26,580 - INFO - training.closure - iteration 329: loss = -10.860568848988514
2023-04-08 22:27:29,269 - INFO - training.closure - iteration 330: loss = -10.863928974931085
2023-04-08 22:27:32,030 - INFO - training.closure - iteration 331: loss = -10.868861903126806
2023-04-08 22:27:34,680 - INFO - training.closure - iteration 332: loss = -10.863199358552139
2023-04-08 22:27:37,329 - INFO - training.closure - iteration 333: loss = -10.871831622913058
2023-04-08 22:27:39,976 - INFO - training.closure - iteration 334: loss = -10.876781910606732
2023-04-08 22:27:42,698 - INFO - training.closure - iteration 335: loss = -10.882866315805124
2023-04-08 22:27:45,439 - INFO - training.closure - iteration 336: loss = -10.897298559336033
2023-04-08 22:27:48,139 - INFO - training.closure - iteration 337: loss = -10.900428871346682
2023-04-08 22:27:50,916 - INFO - training.closure - iteration 338: loss = -10.91005640232995
2023-04-08 22:27:53,583 - INFO - training.closure - iteration 339: loss = -10.911960344637844
2023-04-08 22:27:56,217 - INFO - training.closure - iteration 340: loss = -10.914397250424468
2023-04-08 22:27:58,913 - INFO - training.closure - iteration 341: loss = -10.920848760179087
2023-04-08 22:28:02,030 - INFO - training.closure - iteration 342: loss = -10.908774066592695
2023-04-08 22:28:04,878 - INFO - training.closure - iteration 343: loss = -10.923769961787375
2023-04-08 22:28:07,523 - INFO - training.closure - iteration 344: loss = -10.929326429913491
2023-04-08 22:28:10,245 - INFO - training.closure - iteration 345: loss = -10.934888800221545
2023-04-08 22:28:12,979 - INFO - training.closure - iteration 346: loss = -10.938887951857286
2023-04-08 22:28:15,629 - INFO - training.closure - iteration 347: loss = -10.94320334610038
2023-04-08 22:28:18,252 - INFO - training.closure - iteration 348: loss = -10.944634296300599
2023-04-08 22:28:20,903 - INFO - training.closure - iteration 349: loss = -10.063284878914862
2023-04-08 22:28:23,615 - INFO - training.closure - iteration 350: loss = -10.923418706205418
2023-04-08 22:28:26,306 - INFO - training.closure - iteration 351: loss = -10.945838767159508
2023-04-08 22:28:28,963 - INFO - training.closure - iteration 352: loss = -10.948267757743185
2023-04-08 22:28:31,613 - INFO - training.closure - iteration 353: loss = -10.953839549158896
2023-04-08 22:28:34,234 - INFO - training.closure - iteration 354: loss = -10.957710966329774
2023-04-08 22:28:36,885 - INFO - training.closure - iteration 355: loss = -10.960950623464479
2023-04-08 22:28:39,586 - INFO - training.closure - iteration 356: loss = -10.957914887819697
2023-04-08 22:28:42,244 - INFO - training.closure - iteration 357: loss = -10.961873170659231
2023-04-08 22:28:44,939 - INFO - training.closure - iteration 358: loss = -10.962169080918681
2023-04-08 22:28:47,586 - INFO - training.closure - iteration 359: loss = -10.963653378800323
2023-04-08 22:28:50,226 - INFO - training.closure - iteration 360: loss = -10.964250691073179
2023-04-08 22:28:52,843 - INFO - training.closure - iteration 361: loss = -10.964708365291497
2023-04-08 22:28:55,556 - INFO - training.closure - iteration 362: loss = -10.965738175449403
2023-04-08 22:28:58,170 - INFO - training.closure - iteration 363: loss = -10.965067966280063
2023-04-08 22:29:00,827 - INFO - training.closure - iteration 364: loss = -10.966666887255816
2023-04-08 22:29:03,464 - INFO - training.closure - iteration 365: loss = -10.970463133316592
2023-04-08 22:29:06,091 - INFO - training.closure - iteration 366: loss = -10.977430107056545
2023-04-08 22:29:08,757 - INFO - training.closure - iteration 367: loss = -10.985755755361488
2023-04-08 22:29:11,411 - INFO - training.closure - iteration 368: loss = -10.970826477686856
2023-04-08 22:29:14,130 - INFO - training.closure - iteration 369: loss = -10.987557664570144
2023-04-08 22:29:16,792 - INFO - training.closure - iteration 370: loss = -10.98970104202984
2023-04-08 22:29:19,474 - INFO - training.closure - iteration 371: loss = -10.990830967651398
2023-04-08 22:29:22,107 - INFO - training.closure - iteration 372: loss = -10.991131610459277
2023-04-08 22:29:24,937 - INFO - training.closure - iteration 373: loss = -10.991805251503807
2023-04-08 22:29:27,597 - INFO - training.closure - iteration 374: loss = -10.992992258804556
2023-04-08 22:29:30,324 - INFO - training.closure - iteration 375: loss = -10.974951032195658
2023-04-08 22:29:32,946 - INFO - training.closure - iteration 376: loss = -10.993381654058862
2023-04-08 22:29:35,597 - INFO - training.closure - iteration 377: loss = -10.994228935021138
2023-04-08 22:29:38,320 - INFO - training.closure - iteration 378: loss = -10.995046363287624
2023-04-08 22:29:40,988 - INFO - training.closure - iteration 379: loss = -10.995488831244973
2023-04-08 22:29:43,680 - INFO - training.closure - iteration 380: loss = -10.99585646312228
2023-04-08 22:29:46,400 - INFO - training.closure - iteration 381: loss = -10.996196072478018
2023-04-08 22:29:49,059 - INFO - training.closure - iteration 382: loss = -10.996383694977617
2023-04-08 22:29:51,691 - INFO - training.closure - iteration 383: loss = -10.997142756017652
2023-04-08 22:29:54,407 - INFO - training.closure - iteration 384: loss = -10.997752332708597
2023-04-08 22:29:57,207 - INFO - training.closure - iteration 385: loss = -10.998713630444598
2023-04-08 22:29:59,893 - INFO - training.closure - iteration 386: loss = -10.999562110588386
2023-04-08 22:30:02,640 - INFO - training.closure - iteration 387: loss = -11.000230477345454
2023-04-08 22:30:05,495 - INFO - training.closure - iteration 388: loss = -11.00085809900472
2023-04-08 22:30:08,238 - INFO - training.closure - iteration 389: loss = -11.001499853747863
2023-04-08 22:30:10,929 - INFO - training.closure - iteration 390: loss = -11.003067177259116
2023-04-08 22:30:13,669 - INFO - training.closure - iteration 391: loss = -10.996805868064916
2023-04-08 22:30:16,327 - INFO - training.closure - iteration 392: loss = -11.003406296403384
2023-04-08 22:30:18,962 - INFO - training.closure - iteration 393: loss = -11.004977564834789
2023-04-08 22:30:21,704 - INFO - training.closure - iteration 394: loss = -11.007017511821786
2023-04-08 22:30:24,480 - INFO - training.closure - iteration 395: loss = -11.01002710739144
2023-04-08 22:30:27,137 - INFO - training.closure - iteration 396: loss = -11.01402899151162
2023-04-08 22:30:29,850 - INFO - training.closure - iteration 397: loss = -11.016986189404161
2023-04-08 22:30:32,495 - INFO - training.closure - iteration 398: loss = -11.017572737963846
2023-04-08 22:30:35,150 - INFO - training.closure - iteration 399: loss = -11.019818088909915
2023-04-08 22:30:37,887 - INFO - training.closure - iteration 400: loss = -11.021112089467
2023-04-08 22:30:40,660 - INFO - training.closure - iteration 401: loss = -11.02397230524473
2023-04-08 22:30:43,284 - INFO - training.closure - iteration 402: loss = -11.02691340486837
2023-04-08 22:30:45,943 - INFO - training.closure - iteration 403: loss = -11.031082352112254
2023-04-08 22:30:48,592 - INFO - training.closure - iteration 404: loss = -10.993941705275727
2023-04-08 22:30:51,224 - INFO - training.closure - iteration 405: loss = -11.033990222957147
2023-04-08 22:30:53,929 - INFO - training.closure - iteration 406: loss = -11.03888501713475
2023-04-08 22:30:56,803 - INFO - training.closure - iteration 407: loss = -11.041453026644266
2023-04-08 22:30:59,461 - INFO - training.closure - iteration 408: loss = -11.050630603991973
2023-04-08 22:31:02,132 - INFO - training.closure - iteration 409: loss = -11.059244097409426
2023-04-08 22:31:04,822 - INFO - training.closure - iteration 410: loss = -11.071838581147667
2023-04-08 22:31:07,457 - INFO - training.closure - iteration 411: loss = -11.081766706472937
2023-04-08 22:31:10,110 - INFO - training.closure - iteration 412: loss = -11.09007780172799
2023-04-08 22:31:12,824 - INFO - training.closure - iteration 413: loss = -11.091116454062423
2023-04-08 22:31:15,480 - INFO - training.closure - iteration 414: loss = -11.093773640228443
2023-04-08 22:31:18,130 - INFO - training.closure - iteration 415: loss = -11.100353015490867
2023-04-08 22:31:20,761 - INFO - training.closure - iteration 416: loss = -11.112999831017069
2023-04-08 22:31:23,417 - INFO - training.closure - iteration 417: loss = -11.11594567306187
2023-04-08 22:31:26,178 - INFO - training.closure - iteration 418: loss = -11.122597660531465
2023-04-08 22:31:28,962 - INFO - training.closure - iteration 419: loss = -11.124911910630463
2023-04-08 22:31:31,652 - INFO - training.closure - iteration 420: loss = -11.128515296550736
2023-04-08 22:31:34,300 - INFO - training.closure - iteration 421: loss = -10.861400183123052
2023-04-08 22:31:36,950 - INFO - training.closure - iteration 422: loss = -11.129480597517372
2023-04-08 22:31:39,758 - INFO - training.closure - iteration 423: loss = -11.13297000742985
2023-04-08 22:31:42,422 - INFO - training.closure - iteration 424: loss = -11.136702780299647
2023-04-08 22:31:45,083 - INFO - training.closure - iteration 425: loss = -11.140694601411504
2023-04-08 22:31:47,804 - INFO - training.closure - iteration 426: loss = -11.141768985369183
2023-04-08 22:31:50,419 - INFO - training.closure - iteration 427: loss = -11.14457876822362
2023-04-08 22:31:53,072 - INFO - training.closure - iteration 428: loss = -11.146449569478289
2023-04-08 22:31:55,730 - INFO - training.closure - iteration 429: loss = -11.150639702307156
2023-04-08 22:31:58,376 - INFO - training.closure - iteration 430: loss = -11.154459601240479
2023-04-08 22:32:01,154 - INFO - training.closure - iteration 431: loss = -11.157551366236742
2023-04-08 22:32:03,874 - INFO - training.closure - iteration 432: loss = -11.1621564242203
2023-04-08 22:32:06,516 - INFO - training.closure - iteration 433: loss = -11.166553589560262
2023-04-08 22:32:09,183 - INFO - training.closure - iteration 434: loss = -11.175398028850438
2023-04-08 22:32:11,832 - INFO - training.closure - iteration 435: loss = -11.101817543106218
2023-04-08 22:32:14,623 - INFO - training.closure - iteration 436: loss = -11.181744721668885
2023-04-08 22:32:17,259 - INFO - training.closure - iteration 437: loss = -11.193308600746448
2023-04-08 22:32:20,115 - INFO - training.closure - iteration 438: loss = -11.206944834061566
2023-04-08 22:32:22,825 - INFO - training.closure - iteration 439: loss = -11.166407254841339
2023-04-08 22:32:25,517 - INFO - training.closure - iteration 440: loss = -11.20938439120156
2023-04-08 22:32:28,169 - INFO - training.closure - iteration 441: loss = -11.213771223459528
2023-04-08 22:32:30,960 - INFO - training.closure - iteration 442: loss = -11.220334732779552
2023-04-08 22:32:33,617 - INFO - training.closure - iteration 443: loss = -11.22940693590009
2023-04-08 22:32:36,276 - INFO - training.closure - iteration 444: loss = -11.208850042862824
2023-04-08 22:32:39,108 - INFO - training.closure - iteration 445: loss = -11.233198135752776
2023-04-08 22:32:41,753 - INFO - training.closure - iteration 446: loss = -11.238518941953535
2023-04-08 22:32:44,407 - INFO - training.closure - iteration 447: loss = -11.240577812091798
2023-04-08 22:32:47,064 - INFO - training.closure - iteration 448: loss = -11.243542097793599
2023-04-08 22:32:49,710 - INFO - training.closure - iteration 449: loss = -11.245322650375124
2023-04-08 22:32:52,360 - INFO - training.closure - iteration 450: loss = -11.248750195402916
2023-04-08 22:32:55,094 - INFO - training.closure - iteration 451: loss = -11.253867285831491
2023-04-08 22:32:57,799 - INFO - training.closure - iteration 452: loss = -11.18558857637008
2023-04-08 22:33:00,449 - INFO - training.closure - iteration 453: loss = -11.256456573343293
2023-04-08 22:33:03,072 - INFO - training.closure - iteration 454: loss = -11.262656299268587
2023-04-08 22:33:05,709 - INFO - training.closure - iteration 455: loss = -11.274068234895626
2023-04-08 22:33:08,346 - INFO - training.closure - iteration 456: loss = -11.297588942634732
2023-04-08 22:33:11,050 - INFO - training.closure - iteration 457: loss = -11.313607116102782
2023-04-08 22:33:13,728 - INFO - training.closure - iteration 458: loss = -11.313874385311173
2023-04-08 22:33:16,362 - INFO - training.closure - iteration 459: loss = -11.325754327982773
2023-04-08 22:33:19,019 - INFO - training.closure - iteration 460: loss = -11.334402375973713
2023-04-08 22:33:21,654 - INFO - training.closure - iteration 461: loss = -11.341502189370154
2023-04-08 22:33:24,309 - INFO - training.closure - iteration 462: loss = -11.346824633623825
2023-04-08 22:33:26,972 - INFO - training.closure - iteration 463: loss = -11.351197692977859
2023-04-08 22:33:29,683 - INFO - training.closure - iteration 464: loss = -11.357003412342443
2023-04-08 22:33:32,332 - INFO - training.closure - iteration 465: loss = -11.362563933959201
2023-04-08 22:33:34,961 - INFO - training.closure - iteration 466: loss = -11.369622284073602
2023-04-08 22:33:37,586 - INFO - training.closure - iteration 467: loss = -11.372641209655228
2023-04-08 22:33:40,275 - INFO - training.closure - iteration 468: loss = -11.3750017611581
2023-04-08 22:33:42,918 - INFO - training.closure - iteration 469: loss = -11.376535892301614
2023-04-08 22:33:45,761 - INFO - training.closure - iteration 470: loss = -11.377521275391938
2023-04-08 22:33:48,705 - INFO - training.closure - iteration 471: loss = -11.37974261972171
2023-04-08 22:33:51,328 - INFO - training.closure - iteration 472: loss = -11.375132795098635
2023-04-08 22:33:53,975 - INFO - training.closure - iteration 473: loss = -11.380099128677731
2023-04-08 22:33:56,623 - INFO - training.closure - iteration 474: loss = -11.381414538746625
2023-04-08 22:33:59,276 - INFO - training.closure - iteration 475: loss = -11.385750924438597
2023-04-08 22:34:02,016 - INFO - training.closure - iteration 476: loss = -11.388702568705202
2023-04-08 22:34:04,665 - INFO - training.closure - iteration 477: loss = -11.391726378341094
2023-04-08 22:34:07,292 - INFO - training.closure - iteration 478: loss = -11.395311739687529
2023-04-08 22:34:09,929 - INFO - training.closure - iteration 479: loss = -11.396887661012729
2023-04-08 22:34:12,596 - INFO - training.closure - iteration 480: loss = -11.40111442133646
2023-04-08 22:34:15,306 - INFO - training.closure - iteration 481: loss = -11.402479975602922
2023-04-08 22:34:17,932 - INFO - training.closure - iteration 482: loss = -11.4033160006481
2023-04-08 22:34:20,674 - INFO - training.closure - iteration 483: loss = -11.40379971645562
2023-04-08 22:34:23,370 - INFO - training.closure - iteration 484: loss = -11.404200553877462
2023-04-08 22:34:26,144 - INFO - training.closure - iteration 485: loss = -11.402193744103169
2023-04-08 22:34:28,780 - INFO - training.closure - iteration 486: loss = -11.404687246201544
2023-04-08 22:34:31,545 - INFO - training.closure - iteration 487: loss = -11.406064502530882
2023-04-08 22:34:34,209 - INFO - training.closure - iteration 488: loss = -11.383272962667053
2023-04-08 22:34:36,937 - INFO - training.closure - iteration 489: loss = -11.408298474590369
2023-04-08 22:34:39,570 - INFO - training.closure - iteration 490: loss = -11.410921292688405
2023-04-08 22:34:42,207 - INFO - training.closure - iteration 491: loss = -11.412949818800943
2023-04-08 22:34:44,863 - INFO - training.closure - iteration 492: loss = -11.414170562909678
2023-04-08 22:34:47,519 - INFO - training.closure - iteration 493: loss = -11.415784801665291
2023-04-08 22:34:50,145 - INFO - training.closure - iteration 494: loss = -11.39711023245997
2023-04-08 22:34:52,873 - INFO - training.closure - iteration 495: loss = -11.416403157894099
2023-04-08 22:34:55,583 - INFO - training.closure - iteration 496: loss = -11.419226531703064
2023-04-08 22:34:58,280 - INFO - training.closure - iteration 497: loss = -11.422040917985044
2023-04-08 22:35:00,943 - INFO - training.closure - iteration 498: loss = -11.425469655591566
2023-04-08 22:35:03,727 - INFO - training.closure - iteration 499: loss = -11.427516476550995
2023-04-08 22:35:06,386 - INFO - training.closure - iteration 500: loss = -11.428379009146543
2023-04-08 22:35:09,303 - INFO - training.closure - iteration 501: loss = -11.430142697086822
2023-04-08 22:35:12,074 - INFO - training.closure - iteration 502: loss = -11.43135848749922
2023-04-08 22:35:14,797 - INFO - training.closure - iteration 503: loss = -11.434088015435828
2023-04-08 22:35:17,519 - INFO - training.closure - iteration 504: loss = -11.433497844630605
2023-04-08 22:35:20,148 - INFO - training.closure - iteration 505: loss = -11.435196574612617
2023-04-08 22:35:22,798 - INFO - training.closure - iteration 506: loss = -11.435933928848245
2023-04-08 22:35:25,415 - INFO - training.closure - iteration 507: loss = -11.429102103639856
2023-04-08 22:35:28,148 - INFO - training.closure - iteration 508: loss = -11.436142422656658
2023-04-08 22:35:30,977 - INFO - training.closure - iteration 509: loss = -11.436828608164987
2023-04-08 22:35:33,626 - INFO - training.closure - iteration 510: loss = -11.43795801637688
2023-04-08 22:35:36,320 - INFO - training.closure - iteration 511: loss = -11.438653585774873
2023-04-08 22:35:38,977 - INFO - training.closure - iteration 512: loss = -11.43895769704188
2023-04-08 22:35:41,624 - INFO - training.closure - iteration 513: loss = -11.439813684264944
2023-04-08 22:35:44,401 - INFO - training.closure - iteration 514: loss = -11.442704025715765
2023-04-08 22:35:47,083 - INFO - training.closure - iteration 515: loss = -11.444305867658404
2023-04-08 22:35:49,770 - INFO - training.closure - iteration 516: loss = -11.446542097725263
2023-04-08 22:35:52,399 - INFO - training.closure - iteration 517: loss = -11.446134817782625
2023-04-08 22:35:55,062 - INFO - training.closure - iteration 518: loss = -11.447495455403729
2023-04-08 22:35:57,716 - INFO - training.closure - iteration 519: loss = -11.449484904909815
2023-04-08 22:36:00,359 - INFO - training.closure - iteration 520: loss = -11.452710330066912
2023-04-08 22:36:03,085 - INFO - training.closure - iteration 521: loss = -11.454322976250397
2023-04-08 22:36:05,733 - INFO - training.closure - iteration 522: loss = -11.457684411799681
2023-04-08 22:36:08,464 - INFO - training.closure - iteration 523: loss = -11.461260397463835
2023-04-08 22:36:11,088 - INFO - training.closure - iteration 524: loss = -11.465100270812439
2023-04-08 22:36:13,752 - INFO - training.closure - iteration 525: loss = -11.468204676728249
2023-04-08 22:36:16,426 - INFO - training.closure - iteration 526: loss = -11.470183515746957
2023-04-08 22:36:19,238 - INFO - training.closure - iteration 527: loss = -11.47260203128819
2023-04-08 22:36:21,898 - INFO - training.closure - iteration 528: loss = -11.473674861189938
2023-04-08 22:36:24,556 - INFO - training.closure - iteration 529: loss = -11.474898916644504
2023-04-08 22:36:27,213 - INFO - training.closure - iteration 530: loss = -11.445105934253883
2023-04-08 22:36:30,057 - INFO - training.closure - iteration 531: loss = -11.478566511150762
2023-04-08 22:36:32,697 - INFO - training.closure - iteration 532: loss = -11.484148789355785
2023-04-08 22:36:35,429 - INFO - training.closure - iteration 533: loss = -11.486475670253958
2023-04-08 22:36:38,080 - INFO - training.closure - iteration 534: loss = -11.49080908517153
2023-04-08 22:36:40,701 - INFO - training.closure - iteration 535: loss = -11.49333733810787
2023-04-08 22:36:43,356 - INFO - training.closure - iteration 536: loss = -11.496713251859926
2023-04-08 22:36:46,019 - INFO - training.closure - iteration 537: loss = -11.502647286746175
2023-04-08 22:36:48,700 - INFO - training.closure - iteration 538: loss = -11.507227057172086
2023-04-08 22:36:51,472 - INFO - training.closure - iteration 539: loss = -11.51251577938106
2023-04-08 22:36:54,202 - INFO - training.closure - iteration 540: loss = -11.515345156737403
2023-04-08 22:36:56,855 - INFO - training.closure - iteration 541: loss = -11.517952310670251
2023-04-08 22:36:59,499 - INFO - training.closure - iteration 542: loss = -11.52274019342044
2023-04-08 22:37:02,148 - INFO - training.closure - iteration 543: loss = -11.532690204301904
2023-04-08 22:37:04,809 - INFO - training.closure - iteration 544: loss = -11.405049931339347
2023-04-08 22:37:07,448 - INFO - training.closure - iteration 545: loss = -11.53604141159312
2023-04-08 22:37:10,167 - INFO - training.closure - iteration 546: loss = -11.549705837937847
2023-04-08 22:37:12,846 - INFO - training.closure - iteration 547: loss = -11.561892165343835
2023-04-08 22:37:15,506 - INFO - training.closure - iteration 548: loss = -11.540786387190906
2023-04-08 22:37:18,161 - INFO - training.closure - iteration 549: loss = -11.56787150927966
2023-04-08 22:37:20,810 - INFO - training.closure - iteration 550: loss = -11.574156079065087
2023-04-08 22:37:23,438 - INFO - training.closure - iteration 551: loss = -11.544992030419428
2023-04-08 22:37:26,146 - INFO - training.closure - iteration 552: loss = -11.584245655969562
2023-04-08 22:37:28,790 - INFO - training.closure - iteration 553: loss = -11.50665133975216
2023-04-08 22:37:31,434 - INFO - training.closure - iteration 554: loss = -11.58961573142118
2023-04-08 22:37:34,100 - INFO - training.closure - iteration 555: loss = -11.592649879423888
2023-04-08 22:37:36,746 - INFO - training.closure - iteration 556: loss = -11.595249420114738
2023-04-08 22:37:39,400 - INFO - training.closure - iteration 557: loss = -11.598085269616368
2023-04-08 22:37:42,055 - INFO - training.closure - iteration 558: loss = -11.605961982811523
2023-04-08 22:37:44,775 - INFO - training.closure - iteration 559: loss = -11.642653739555376
2023-04-08 22:37:47,420 - INFO - training.closure - iteration 560: loss = -11.62000948963221
2023-04-08 22:37:50,125 - INFO - training.closure - iteration 561: loss = -11.656973053774053
2023-04-08 22:37:52,792 - INFO - training.closure - iteration 562: loss = -11.669516119279514
2023-04-08 22:37:55,452 - INFO - training.closure - iteration 563: loss = -11.668745084785517
2023-04-08 22:37:58,179 - INFO - training.closure - iteration 564: loss = -11.67489812947507
2023-04-08 22:38:00,916 - INFO - training.closure - iteration 565: loss = -11.681412570568597
2023-04-08 22:38:03,631 - INFO - training.closure - iteration 566: loss = -11.689158051673374
2023-04-08 22:38:06,478 - INFO - training.closure - iteration 567: loss = -11.704911075991394
2023-04-08 22:38:09,133 - INFO - training.closure - iteration 568: loss = -11.714222213128025
2023-04-08 22:38:11,884 - INFO - training.closure - iteration 569: loss = -11.73011288016185
2023-04-08 22:38:14,549 - INFO - training.closure - iteration 570: loss = -11.74954450154772
2023-04-08 22:38:17,277 - INFO - training.closure - iteration 571: loss = -11.752291414070282
2023-04-08 22:38:20,130 - INFO - training.closure - iteration 572: loss = -11.766009567058644
2023-04-08 22:38:22,780 - INFO - training.closure - iteration 573: loss = -11.793104076773638
2023-04-08 22:38:25,439 - INFO - training.closure - iteration 574: loss = -11.804040231464445
2023-04-08 22:38:28,088 - INFO - training.closure - iteration 575: loss = -11.835123300123268
2023-04-08 22:38:30,714 - INFO - training.closure - iteration 576: loss = -11.844033763405477
2023-04-08 22:38:33,367 - INFO - training.closure - iteration 577: loss = -11.852306603003294
2023-04-08 22:38:36,074 - INFO - training.closure - iteration 578: loss = -11.859496312473208
2023-04-08 22:38:38,707 - INFO - training.closure - iteration 579: loss = -11.86277234910358
2023-04-08 22:38:41,392 - INFO - training.closure - iteration 580: loss = -11.866618965811732
2023-04-08 22:38:44,094 - INFO - training.closure - iteration 581: loss = -11.873239384923707
2023-04-08 22:38:46,780 - INFO - training.closure - iteration 582: loss = -11.880676402055805
2023-04-08 22:38:49,491 - INFO - training.closure - iteration 583: loss = -11.890788233920226
2023-04-08 22:38:52,222 - INFO - training.closure - iteration 584: loss = -11.89162673052638
2023-04-08 22:38:54,845 - INFO - training.closure - iteration 585: loss = -11.895546792605142
2023-04-08 22:38:57,603 - INFO - training.closure - iteration 586: loss = -11.898895286063654
2023-04-08 22:39:00,233 - INFO - training.closure - iteration 587: loss = -11.904240783704541
2023-04-08 22:39:02,883 - INFO - training.closure - iteration 588: loss = -11.908015213626161
2023-04-08 22:39:05,536 - INFO - training.closure - iteration 589: loss = -11.909742661886899
2023-04-08 22:39:08,272 - INFO - training.closure - iteration 590: loss = -11.91087737623835
2023-04-08 22:39:10,899 - INFO - training.closure - iteration 591: loss = -11.912386500485031
2023-04-08 22:39:13,677 - INFO - training.closure - iteration 592: loss = -11.916655102579192
2023-04-08 22:39:16,300 - INFO - training.closure - iteration 593: loss = -11.918648430654308
2023-04-08 22:39:18,962 - INFO - training.closure - iteration 594: loss = -11.924810707928422
2023-04-08 22:39:21,604 - INFO - training.closure - iteration 595: loss = -11.934545387952854
2023-04-08 22:39:24,238 - INFO - training.closure - iteration 596: loss = -11.93940006020916
2023-04-08 22:39:26,971 - INFO - training.closure - iteration 597: loss = -11.941106006739016
2023-04-08 22:39:29,818 - INFO - training.closure - iteration 598: loss = -11.94397925109182
2023-04-08 22:39:32,689 - INFO - training.closure - iteration 599: loss = -11.945544466567696
2023-04-08 22:39:35,367 - INFO - training.closure - iteration 600: loss = -11.94884348336793
2023-04-08 22:39:38,014 - INFO - training.closure - iteration 601: loss = -11.951561894034883
2023-04-08 22:39:40,754 - INFO - training.closure - iteration 602: loss = -11.956880307012664
2023-04-08 22:39:43,505 - INFO - training.closure - iteration 603: loss = -11.959888311804328
2023-04-08 22:39:46,289 - INFO - training.closure - iteration 604: loss = -11.9636902138447
2023-04-08 22:39:49,019 - INFO - training.closure - iteration 605: loss = -11.963782267598278
2023-04-08 22:39:51,681 - INFO - training.closure - iteration 606: loss = -11.966697216531621
2023-04-08 22:39:54,310 - INFO - training.closure - iteration 607: loss = -11.97070773028114
2023-04-08 22:39:57,090 - INFO - training.closure - iteration 608: loss = -11.942679371549126
2023-04-08 22:39:59,891 - INFO - training.closure - iteration 609: loss = -11.972135002106729
2023-04-08 22:40:02,538 - INFO - training.closure - iteration 610: loss = -11.975755208759733
2023-04-08 22:40:05,190 - INFO - training.closure - iteration 611: loss = -11.976622313268448
2023-04-08 22:40:07,839 - INFO - training.closure - iteration 612: loss = -11.977232094106153
2023-04-08 22:40:10,509 - INFO - training.closure - iteration 613: loss = -11.978702459935112
2023-04-08 22:40:13,282 - INFO - training.closure - iteration 614: loss = -11.979400308249936
2023-04-08 22:40:16,016 - INFO - training.closure - iteration 615: loss = -11.979751443711516
2023-04-08 22:40:18,747 - INFO - training.closure - iteration 616: loss = -11.98050919003338
2023-04-08 22:40:21,400 - INFO - training.closure - iteration 617: loss = -11.982254265037547
2023-04-08 22:40:24,053 - INFO - training.closure - iteration 618: loss = -11.983901173388386
2023-04-08 22:40:26,705 - INFO - training.closure - iteration 619: loss = -11.98582550300483
2023-04-08 22:40:29,366 - INFO - training.closure - iteration 620: loss = -11.985548341060497
2023-04-08 22:40:32,066 - INFO - training.closure - iteration 621: loss = -11.986670062815534
2023-04-08 22:40:34,822 - INFO - training.closure - iteration 622: loss = -11.987574094029332
2023-04-08 22:40:37,457 - INFO - training.closure - iteration 623: loss = -11.98817932326504
2023-04-08 22:40:40,113 - INFO - training.closure - iteration 624: loss = -11.988733317679635
2023-04-08 22:40:42,741 - INFO - training.closure - iteration 625: loss = -11.989218667104524
2023-04-08 22:40:45,481 - INFO - training.closure - iteration 626: loss = -11.990498343889577
2023-04-08 22:40:48,135 - INFO - training.closure - iteration 627: loss = -11.992314801690892
2023-04-08 22:40:50,854 - INFO - training.closure - iteration 628: loss = -11.907973256391134
2023-04-08 22:40:53,477 - INFO - training.closure - iteration 629: loss = -11.992975295744014
2023-04-08 22:40:56,119 - INFO - training.closure - iteration 630: loss = -10.46132672800588
2023-04-08 22:40:58,770 - INFO - training.closure - iteration 631: loss = -11.9507632715433
2023-04-08 22:41:01,467 - INFO - training.closure - iteration 632: loss = -11.995075925441764
2023-04-08 22:41:04,104 - INFO - training.closure - iteration 633: loss = -11.998053739396926
2023-04-08 22:41:06,722 - INFO - training.closure - iteration 634: loss = -11.998521108967319
2023-04-08 22:41:09,482 - INFO - training.closure - iteration 635: loss = -12.000130791197247
2023-04-08 22:41:12,144 - INFO - training.closure - iteration 636: loss = -12.000584582803127
2023-04-08 22:41:14,793 - INFO - training.closure - iteration 637: loss = -12.000900731382888
2023-04-08 22:41:17,440 - INFO - training.closure - iteration 638: loss = -12.001295374048983
2023-04-08 22:41:20,121 - INFO - training.closure - iteration 639: loss = -12.001543091629262
2023-04-08 22:41:22,772 - INFO - training.closure - iteration 640: loss = -12.00177040267826
2023-04-08 22:41:25,508 - INFO - training.closure - iteration 641: loss = -12.001897769092315
2023-04-08 22:41:28,174 - INFO - training.closure - iteration 642: loss = -12.002129051930034
2023-04-08 22:41:30,814 - INFO - training.closure - iteration 643: loss = -12.002168237770178
2023-04-08 22:41:33,445 - INFO - training.closure - iteration 644: loss = -12.002197263089595
2023-04-08 22:41:36,098 - INFO - training.closure - iteration 645: loss = -12.002253516953271
2023-04-08 22:41:38,753 - INFO - training.closure - iteration 646: loss = -12.002316351181356
2023-04-08 22:41:41,532 - INFO - training.closure - iteration 647: loss = -12.002399565795075
2023-04-08 22:41:44,363 - INFO - training.closure - iteration 648: loss = -12.002461791714088
2023-04-08 22:41:46,991 - INFO - training.closure - iteration 649: loss = -12.002583517301922
2023-04-08 22:41:49,609 - INFO - training.closure - iteration 650: loss = -12.002874665801066
2023-04-08 22:41:52,274 - INFO - training.closure - iteration 651: loss = -12.003201754754219
2023-04-08 22:41:54,896 - INFO - training.closure - iteration 652: loss = -12.003892904359128
2023-04-08 22:41:57,516 - INFO - training.closure - iteration 653: loss = -12.004951431039403
2023-04-08 22:42:00,269 - INFO - training.closure - iteration 654: loss = -12.005921044060646
2023-04-08 22:42:02,901 - INFO - training.closure - iteration 655: loss = -12.006818211090145
2023-04-08 22:42:05,529 - INFO - training.closure - iteration 656: loss = -12.007372032301596
2023-04-08 22:42:08,174 - INFO - training.closure - iteration 657: loss = -12.007592596468694
2023-04-08 22:42:10,800 - INFO - training.closure - iteration 658: loss = -12.007750661414146
2023-04-08 22:42:13,504 - INFO - training.closure - iteration 659: loss = -12.008142991114456
2023-04-08 22:42:16,244 - INFO - training.closure - iteration 660: loss = -12.002262616446766
2023-04-08 22:42:18,947 - INFO - training.closure - iteration 661: loss = -12.008277165584973
2023-04-08 22:42:21,737 - INFO - training.closure - iteration 662: loss = -12.008938301037606
2023-04-08 22:42:24,610 - INFO - training.closure - iteration 663: loss = -12.009218326004813
2023-04-08 22:42:27,232 - INFO - training.closure - iteration 664: loss = -12.009402989124332
2023-04-08 22:42:29,872 - INFO - training.closure - iteration 665: loss = -12.009994072504574
2023-04-08 22:42:32,634 - INFO - training.closure - iteration 666: loss = -12.010441247467114
2023-04-08 22:42:35,285 - INFO - training.closure - iteration 667: loss = -12.010877231748875
2023-04-08 22:42:37,921 - INFO - training.closure - iteration 668: loss = -12.011201733938115
2023-04-08 22:42:40,550 - INFO - training.closure - iteration 669: loss = -12.011664697728003
2023-04-08 22:42:43,232 - INFO - training.closure - iteration 670: loss = -12.011734495840525
2023-04-08 22:42:46,004 - INFO - training.closure - iteration 671: loss = -12.011987351746367
2023-04-08 22:42:48,638 - INFO - training.closure - iteration 672: loss = -12.012000668839699
2023-04-08 22:42:51,389 - INFO - training.closure - iteration 673: loss = -12.012013072801565
2023-04-08 22:42:54,051 - INFO - training.closure - iteration 674: loss = -12.012057991417068
2023-04-08 22:42:56,711 - INFO - training.closure - iteration 675: loss = -12.01211312753895
2023-04-08 22:42:59,380 - INFO - training.closure - iteration 676: loss = -12.012202789306325
2023-04-08 22:43:02,032 - INFO - training.closure - iteration 677: loss = -12.01228162546154
2023-04-08 22:43:04,661 - INFO - training.closure - iteration 678: loss = -12.012135883409757
2023-04-08 22:43:07,375 - INFO - training.closure - iteration 679: loss = -12.012306059401087
2023-04-08 22:43:10,158 - INFO - training.closure - iteration 680: loss = -12.011895632709255
2023-04-08 22:43:12,947 - INFO - training.closure - iteration 681: loss = -12.012408408953785
2023-04-08 22:43:17,193 - INFO - training.closure - iteration 682: loss = -12.012500342192084
2023-04-08 22:43:19,973 - INFO - training.closure - iteration 683: loss = -12.012534949852022
2023-04-08 22:43:22,699 - INFO - training.closure - iteration 684: loss = -12.012543264772582
2023-04-08 22:43:25,556 - INFO - training.closure - iteration 685: loss = -12.01256789225303
2023-04-08 22:43:28,306 - INFO - training.closure - iteration 686: loss = -12.012625672313757
2023-04-08 22:43:30,943 - INFO - training.closure - iteration 687: loss = -12.012752408160658
2023-04-08 22:43:33,610 - INFO - training.closure - iteration 688: loss = -12.012908858276571
2023-04-08 22:43:38,415 - INFO - training.closure - iteration 689: loss = -12.013148602349641
2023-04-08 22:43:42,251 - INFO - training.closure - iteration 690: loss = -12.013329963675375
2023-04-08 22:43:46,529 - INFO - training.closure - iteration 691: loss = -12.013437737821501
2023-04-08 22:43:50,491 - INFO - training.closure - iteration 692: loss = -12.01357594798175
2023-04-08 22:43:53,521 - INFO - training.closure - iteration 693: loss = -12.013711333572857
2023-04-08 22:43:56,420 - INFO - training.closure - iteration 694: loss = -12.013831324131598
2023-04-08 22:43:59,281 - INFO - training.closure - iteration 695: loss = -12.013818970910526
2023-04-08 22:44:02,136 - INFO - training.closure - iteration 696: loss = -12.013877266615456
2023-04-08 22:44:04,986 - INFO - training.closure - iteration 697: loss = -12.013928608167637
2023-04-08 22:44:07,920 - INFO - training.closure - iteration 698: loss = -12.01398643822007
2023-04-08 22:44:10,695 - INFO - training.closure - iteration 699: loss = -12.014319956152548
2023-04-08 22:44:13,621 - INFO - training.closure - iteration 700: loss = -12.014515925667983
2023-04-08 22:44:16,455 - INFO - training.closure - iteration 701: loss = -12.014616063280148
2023-04-08 22:44:19,437 - INFO - training.closure - iteration 702: loss = -12.01478297632148
2023-04-08 22:44:22,211 - INFO - training.closure - iteration 703: loss = -12.014825562850291
2023-04-08 22:44:25,026 - INFO - training.closure - iteration 704: loss = -12.015006348230049
2023-04-08 22:44:27,846 - INFO - training.closure - iteration 705: loss = -12.015183724370761
2023-04-08 22:44:31,312 - INFO - training.closure - iteration 706: loss = -12.015524288462291
2023-04-08 22:44:34,176 - INFO - training.closure - iteration 707: loss = -12.016047045204322
2023-04-08 22:44:36,969 - INFO - training.closure - iteration 708: loss = -11.846284226469564
2023-04-08 22:44:39,842 - INFO - training.closure - iteration 709: loss = -12.016017105924076
2023-04-08 22:44:42,647 - INFO - training.closure - iteration 710: loss = -12.016095741101694
2023-04-08 22:44:45,503 - INFO - training.closure - iteration 711: loss = -12.01640152811891
2023-04-08 22:44:48,368 - INFO - training.closure - iteration 712: loss = -12.016546015763407
2023-04-08 22:44:51,104 - INFO - training.closure - iteration 713: loss = -12.016715551769723
2023-04-08 22:44:53,954 - INFO - training.closure - iteration 714: loss = -12.017001066566507
2023-04-08 22:44:56,717 - INFO - training.closure - iteration 715: loss = -11.926709596974483
2023-04-08 22:44:59,400 - INFO - training.closure - iteration 716: loss = -12.015959200276965
2023-04-08 22:45:02,197 - INFO - training.closure - iteration 717: loss = -12.017065871222936
2023-04-08 22:45:04,870 - INFO - training.closure - iteration 718: loss = -12.017533494964026
2023-04-08 22:45:07,694 - INFO - training.closure - iteration 719: loss = -12.018258916197729
2023-04-08 22:45:10,390 - INFO - training.closure - iteration 720: loss = -12.018259959748494
2023-04-08 22:45:13,106 - INFO - training.closure - iteration 721: loss = -12.018431017289164
2023-04-08 22:45:15,782 - INFO - training.closure - iteration 722: loss = -12.018349888989821
2023-04-08 22:45:18,537 - INFO - training.closure - iteration 723: loss = -12.018646126972497
2023-04-08 22:45:21,344 - INFO - training.closure - iteration 724: loss = -12.018835922879116
2023-04-08 22:45:24,074 - INFO - training.closure - iteration 725: loss = -12.018959630799952
2023-04-08 22:45:26,825 - INFO - training.closure - iteration 726: loss = -12.019166900193255
2023-04-08 22:45:29,617 - INFO - training.closure - iteration 727: loss = -12.019284655796568
2023-04-08 22:45:32,415 - INFO - training.closure - iteration 728: loss = -12.018473704452862
2023-04-08 22:45:35,112 - INFO - training.closure - iteration 729: loss = -12.0196844328103
2023-04-08 22:45:37,976 - INFO - training.closure - iteration 730: loss = -12.020177193126747
2023-04-08 22:45:40,757 - INFO - training.closure - iteration 731: loss = -12.020696205427885
2023-04-08 22:45:43,523 - INFO - training.closure - iteration 732: loss = -12.020833257051862
2023-04-08 22:45:46,232 - INFO - training.closure - iteration 733: loss = -12.020927917052301
2023-04-08 22:45:48,925 - INFO - training.closure - iteration 734: loss = -12.021032071083376
2023-04-08 22:45:51,694 - INFO - training.closure - iteration 735: loss = -12.021248310001972
2023-04-08 22:45:54,517 - INFO - training.closure - iteration 736: loss = -12.021503812697805
2023-04-08 22:45:57,276 - INFO - training.closure - iteration 737: loss = -12.021800217144978
2023-04-08 22:46:00,130 - INFO - training.closure - iteration 738: loss = -12.021943676502973
2023-04-08 22:46:02,827 - INFO - training.closure - iteration 739: loss = -12.02201806108677
2023-04-08 22:46:05,602 - INFO - training.closure - iteration 740: loss = -12.022048632085973
2023-04-08 22:46:08,403 - INFO - training.closure - iteration 741: loss = -12.022102685850676
2023-04-08 22:46:11,141 - INFO - training.closure - iteration 742: loss = -12.022213044866007
2023-04-08 22:46:13,821 - INFO - training.closure - iteration 743: loss = -12.022335657536253
2023-04-08 22:46:16,605 - INFO - training.closure - iteration 744: loss = -12.021953950811167
2023-04-08 22:46:19,298 - INFO - training.closure - iteration 745: loss = -12.022365699810415
2023-04-08 22:46:21,979 - INFO - training.closure - iteration 746: loss = -12.022459905505901
2023-04-08 22:46:24,715 - INFO - training.closure - iteration 747: loss = -12.022551155982335
2023-04-08 22:46:27,403 - INFO - training.closure - iteration 748: loss = -12.02260056381677
2023-04-08 22:46:30,172 - INFO - training.closure - iteration 749: loss = -12.020392865729505
2023-04-08 22:46:32,906 - INFO - training.closure - iteration 750: loss = -12.022625036768604
2023-04-08 22:46:35,598 - INFO - training.closure - iteration 751: loss = -12.022714164638607
2023-04-08 22:46:38,273 - INFO - training.closure - iteration 752: loss = -12.022786515403435
2023-04-08 22:46:40,947 - INFO - training.closure - iteration 753: loss = -12.022861281800747
2023-04-08 22:46:43,645 - INFO - training.closure - iteration 754: loss = -12.022900197033511
2023-04-08 22:46:46,551 - INFO - training.closure - iteration 755: loss = -12.023015791109799
2023-04-08 22:46:49,241 - INFO - training.closure - iteration 756: loss = -12.021459996314782
2023-04-08 22:46:51,982 - INFO - training.closure - iteration 757: loss = -12.023064536948148
2023-04-08 22:46:54,759 - INFO - training.closure - iteration 758: loss = -12.023229489803018
2023-04-08 22:46:57,464 - INFO - training.closure - iteration 759: loss = -12.023324205131322
2023-04-08 22:47:00,230 - INFO - training.closure - iteration 760: loss = -12.023414994480817
2023-04-08 22:47:03,142 - INFO - training.closure - iteration 761: loss = -12.023483496898585
2023-04-08 22:47:05,849 - INFO - training.closure - iteration 762: loss = -12.023548339513251
2023-04-08 22:47:08,599 - INFO - training.closure - iteration 763: loss = -12.023386544213459
2023-04-08 22:47:11,349 - INFO - training.closure - iteration 764: loss = -12.02356201315358
2023-04-08 22:47:14,099 - INFO - training.closure - iteration 765: loss = -12.023592878610657
2023-04-08 22:47:16,852 - INFO - training.closure - iteration 766: loss = -12.023638111943164
2023-04-08 22:47:19,573 - INFO - training.closure - iteration 767: loss = -12.023694791670657
2023-04-08 22:47:22,423 - INFO - training.closure - iteration 768: loss = -12.023742465061307
2023-04-08 22:47:25,248 - INFO - training.closure - iteration 769: loss = -12.023715109587613
2023-04-08 22:47:27,994 - INFO - training.closure - iteration 770: loss = -12.023750729564739
2023-04-08 22:47:30,726 - INFO - training.closure - iteration 771: loss = -12.023767304176276
2023-04-08 22:47:33,471 - INFO - training.closure - iteration 772: loss = -12.023777452575157
2023-04-08 22:47:36,231 - INFO - training.closure - iteration 773: loss = -12.023798362150568
2023-04-08 22:47:39,030 - INFO - training.closure - iteration 774: loss = -12.023816095907259
2023-04-08 22:47:41,748 - INFO - training.closure - iteration 775: loss = -12.023855945485488
2023-04-08 22:47:44,609 - INFO - training.closure - iteration 776: loss = -12.023402634996394
2023-04-08 22:47:47,438 - INFO - training.closure - iteration 777: loss = -12.02386535384008
2023-04-08 22:47:50,175 - INFO - training.closure - iteration 778: loss = -12.023913678934289
2023-04-08 22:47:52,905 - INFO - training.closure - iteration 779: loss = -12.023950436086551
2023-04-08 22:47:55,747 - INFO - training.closure - iteration 780: loss = -12.023970788275806
2023-04-08 22:47:58,453 - INFO - training.closure - iteration 781: loss = -12.023984257637103
2023-04-08 22:48:01,206 - INFO - training.closure - iteration 782: loss = -12.024026678689495
2023-04-08 22:48:03,909 - INFO - training.closure - iteration 783: loss = -12.024122848910519
2023-04-08 22:48:06,616 - INFO - training.closure - iteration 784: loss = -12.024242419095547
2023-04-08 22:48:09,355 - INFO - training.closure - iteration 785: loss = -12.001979750622713
2023-04-08 22:48:12,155 - INFO - training.closure - iteration 786: loss = -12.024289358165158
2023-04-08 22:48:15,085 - INFO - training.closure - iteration 787: loss = -12.024401422418006
2023-04-08 22:48:17,864 - INFO - training.closure - iteration 788: loss = -12.024537018941935
2023-04-08 22:48:20,575 - INFO - training.closure - iteration 789: loss = -12.024625726035568
2023-04-08 22:48:23,303 - INFO - training.closure - iteration 790: loss = -12.024247540740543
2023-04-08 22:48:26,037 - INFO - training.closure - iteration 791: loss = -12.024704202107474
2023-04-08 22:48:28,771 - INFO - training.closure - iteration 792: loss = -12.02483833954134
2023-04-08 22:48:31,579 - INFO - training.closure - iteration 793: loss = -12.025184114655545
2023-04-08 22:48:34,281 - INFO - training.closure - iteration 794: loss = -12.025354462103902
2023-04-08 22:48:37,054 - INFO - training.closure - iteration 795: loss = -12.021666866989797
2023-04-08 22:48:39,712 - INFO - training.closure - iteration 796: loss = -12.025407354381997
2023-04-08 22:48:42,377 - INFO - training.closure - iteration 797: loss = -12.02576858933217
2023-04-08 22:48:45,117 - INFO - training.closure - iteration 798: loss = -12.025393225329712
2023-04-08 22:48:47,924 - INFO - training.closure - iteration 799: loss = -12.025940798914096
2023-04-08 22:48:50,639 - INFO - training.closure - iteration 800: loss = -12.026175214269566
2023-04-08 22:48:53,451 - INFO - training.closure - iteration 801: loss = -12.02628522336061
2023-04-08 22:48:56,173 - INFO - training.closure - iteration 802: loss = -12.026322840119075
2023-04-08 22:48:58,915 - INFO - training.closure - iteration 803: loss = -12.026350511897721
2023-04-08 22:49:01,752 - INFO - training.closure - iteration 804: loss = -12.026420161706271
2023-04-08 22:49:04,482 - INFO - training.closure - iteration 805: loss = -12.026594031581663
2023-04-08 22:49:07,322 - INFO - training.closure - iteration 806: loss = -12.026843434284427
2023-04-08 22:49:10,071 - INFO - training.closure - iteration 807: loss = -12.027163082675223
2023-04-08 22:49:12,782 - INFO - training.closure - iteration 808: loss = -12.027553446751202
2023-04-08 22:49:15,507 - INFO - training.closure - iteration 809: loss = -12.027744314804586
2023-04-08 22:49:18,181 - INFO - training.closure - iteration 810: loss = -12.027885632889639
2023-04-08 22:49:20,912 - INFO - training.closure - iteration 811: loss = -12.02794816978919
2023-04-08 22:49:23,700 - INFO - training.closure - iteration 812: loss = -12.028007686546545
2023-04-08 22:49:26,367 - INFO - training.closure - iteration 813: loss = -12.028071569706466
2023-04-08 22:49:29,022 - INFO - training.closure - iteration 814: loss = -12.028241159520835
2023-04-08 22:49:31,727 - INFO - training.closure - iteration 815: loss = -12.028441191459663
2023-04-08 22:49:34,486 - INFO - training.closure - iteration 816: loss = -12.028635226305488
2023-04-08 22:49:37,333 - INFO - training.closure - iteration 817: loss = -12.028748061657595
2023-04-08 22:49:40,133 - INFO - training.closure - iteration 818: loss = -12.028845129384031
2023-04-08 22:49:42,854 - INFO - training.closure - iteration 819: loss = -12.029074800670159
2023-04-08 22:49:45,599 - INFO - training.closure - iteration 820: loss = -12.029428431128355
2023-04-08 22:49:48,365 - INFO - training.closure - iteration 821: loss = -12.03000561281444
2023-04-08 22:49:51,495 - INFO - training.closure - iteration 822: loss = -12.028035216042078
2023-04-08 22:49:54,216 - INFO - training.closure - iteration 823: loss = -12.030623394413116
2023-04-08 22:49:57,029 - INFO - training.closure - iteration 824: loss = -12.031737018503186
2023-04-08 22:49:59,806 - INFO - training.closure - iteration 825: loss = -12.032948892288537
2023-04-08 22:50:02,541 - INFO - training.closure - iteration 826: loss = -12.033480878054835
2023-04-08 22:50:05,301 - INFO - training.closure - iteration 827: loss = -12.033914459509631
2023-04-08 22:50:08,035 - INFO - training.closure - iteration 828: loss = -12.034104545969559
2023-04-08 22:50:10,836 - INFO - training.closure - iteration 829: loss = -12.034296798365286
2023-04-08 22:50:13,596 - INFO - training.closure - iteration 830: loss = -12.034166889175577
2023-04-08 22:50:16,404 - INFO - training.closure - iteration 831: loss = -12.03506964624533
2023-04-08 22:50:19,111 - INFO - training.closure - iteration 832: loss = -12.035804763968143
2023-04-08 22:50:21,813 - INFO - training.closure - iteration 833: loss = -12.036429404477458
2023-04-08 22:50:24,523 - INFO - training.closure - iteration 834: loss = -12.036953929991059
2023-04-08 22:50:27,249 - INFO - training.closure - iteration 835: loss = -12.035631801362129
2023-04-08 22:50:29,969 - INFO - training.closure - iteration 836: loss = -12.037141173832072
2023-04-08 22:50:32,786 - INFO - training.closure - iteration 837: loss = -12.037358798361574
2023-04-08 22:50:35,538 - INFO - training.closure - iteration 838: loss = -12.03780503088917
2023-04-08 22:50:38,252 - INFO - training.closure - iteration 839: loss = -12.038264907573392
2023-04-08 22:50:40,934 - INFO - training.closure - iteration 840: loss = -12.037927867210348
2023-04-08 22:50:43,637 - INFO - training.closure - iteration 841: loss = -12.038522760826078
2023-04-08 22:50:46,399 - INFO - training.closure - iteration 842: loss = -12.039054091146275
2023-04-08 22:50:49,239 - INFO - training.closure - iteration 843: loss = -12.039694458956502
2023-04-08 22:50:52,303 - INFO - training.closure - iteration 844: loss = -12.039937677640967
2023-04-08 22:50:55,476 - INFO - training.closure - iteration 845: loss = -12.0401736243054
2023-04-08 22:50:59,026 - INFO - training.closure - iteration 846: loss = -12.040805780822804
2023-04-08 22:51:02,350 - INFO - training.closure - iteration 847: loss = -12.04140409412626
2023-04-08 22:51:05,107 - INFO - training.closure - iteration 848: loss = -12.043249486355588
2023-04-08 22:51:07,986 - INFO - training.closure - iteration 849: loss = -12.043751419160712
2023-04-08 22:51:10,907 - INFO - training.closure - iteration 850: loss = -12.04459375733781
2023-04-08 22:51:13,597 - INFO - training.closure - iteration 851: loss = -10.559624873545687
2023-04-08 22:51:16,331 - INFO - training.closure - iteration 852: loss = -11.971296196315778
2023-04-08 22:51:19,074 - INFO - training.closure - iteration 853: loss = -12.044787548788126
2023-04-08 22:51:21,738 - INFO - training.closure - iteration 854: loss = -12.04546236278653
2023-04-08 22:51:24,468 - INFO - training.closure - iteration 855: loss = -12.049186328443477
2023-04-08 22:51:27,221 - INFO - training.closure - iteration 856: loss = -12.051537557809372
2023-04-08 22:51:29,900 - INFO - training.closure - iteration 857: loss = -12.037211036351259
2023-04-08 22:51:32,598 - INFO - training.closure - iteration 858: loss = -12.053347856808553
2023-04-08 22:51:35,365 - INFO - training.closure - iteration 859: loss = -12.055691417452495
2023-04-08 22:51:38,078 - INFO - training.closure - iteration 860: loss = -12.059145446267078
2023-04-08 22:51:40,825 - INFO - training.closure - iteration 861: loss = -12.061782874917444
2023-04-08 22:51:43,575 - INFO - training.closure - iteration 862: loss = -12.062519619123954
2023-04-08 22:51:46,347 - INFO - training.closure - iteration 863: loss = -12.0630354744898
2023-04-08 22:51:49,304 - INFO - training.closure - iteration 864: loss = -12.063354960554145
2023-04-08 22:51:52,327 - INFO - training.closure - iteration 865: loss = -12.063675497553508
2023-04-08 22:51:55,051 - INFO - training.closure - iteration 866: loss = -12.06429016998555
2023-04-08 22:51:57,764 - INFO - training.closure - iteration 867: loss = -12.065143138135138
2023-04-08 22:52:00,435 - INFO - training.closure - iteration 868: loss = -12.066448334234634
2023-04-08 22:52:03,266 - INFO - training.closure - iteration 869: loss = -12.067900530314944
2023-04-08 22:52:05,964 - INFO - training.closure - iteration 870: loss = -12.068431749345258
2023-04-08 22:52:08,806 - INFO - training.closure - iteration 871: loss = -12.069125678107042
2023-04-08 22:52:11,494 - INFO - training.closure - iteration 872: loss = -12.069422263654726
2023-04-08 22:52:14,223 - INFO - training.closure - iteration 873: loss = -12.07048164209337
2023-04-08 22:52:16,981 - INFO - training.closure - iteration 874: loss = -12.06822314382628
2023-04-08 22:52:19,809 - INFO - training.closure - iteration 875: loss = -12.071136444968129
2023-04-08 22:52:22,488 - INFO - training.closure - iteration 876: loss = -12.071622105885066
2023-04-08 22:52:25,212 - INFO - training.closure - iteration 877: loss = -12.073036928043729
2023-04-08 22:52:27,908 - INFO - training.closure - iteration 878: loss = -12.073276229025947
2023-04-08 22:52:30,638 - INFO - training.closure - iteration 879: loss = -12.07388589586568
2023-04-08 22:52:33,484 - INFO - training.closure - iteration 880: loss = -12.074452277682303
2023-04-08 22:52:36,227 - INFO - training.closure - iteration 881: loss = -12.074836815407608
2023-04-08 22:52:39,085 - INFO - training.closure - iteration 882: loss = -12.075369818817673
2023-04-08 22:52:41,809 - INFO - training.closure - iteration 883: loss = -12.075806359246073
2023-04-08 22:52:44,657 - INFO - training.closure - iteration 884: loss = -12.076786798673158
2023-04-08 22:52:47,428 - INFO - training.closure - iteration 885: loss = -12.077118810314829
2023-04-08 22:52:50,154 - INFO - training.closure - iteration 886: loss = -12.077313238735119
2023-04-08 22:52:52,991 - INFO - training.closure - iteration 887: loss = -12.077602938981698
2023-04-08 22:52:55,801 - INFO - training.closure - iteration 888: loss = -12.0763448120709
2023-04-08 22:52:58,508 - INFO - training.closure - iteration 889: loss = -12.078309656533012
2023-04-08 22:53:01,228 - INFO - training.closure - iteration 890: loss = -12.079045190554307
2023-04-08 22:53:04,023 - INFO - training.closure - iteration 891: loss = -12.079772624026226
2023-04-08 22:53:06,762 - INFO - training.closure - iteration 892: loss = -12.079510432609837
2023-04-08 22:53:09,528 - INFO - training.closure - iteration 893: loss = -12.079989119828562
2023-04-08 22:53:12,322 - INFO - training.closure - iteration 894: loss = -12.08040684070561
2023-04-08 22:53:15,033 - INFO - training.closure - iteration 895: loss = -12.08064517675167
2023-04-08 22:53:17,780 - INFO - training.closure - iteration 896: loss = -12.081183749799855
2023-04-08 22:53:20,533 - INFO - training.closure - iteration 897: loss = -12.081640501131357
2023-04-08 22:53:23,290 - INFO - training.closure - iteration 898: loss = -12.081913407354211
2023-04-08 22:53:26,033 - INFO - training.closure - iteration 899: loss = -12.082316949884696
2023-04-08 22:53:28,778 - INFO - training.closure - iteration 900: loss = -12.082882547567007
2023-04-08 22:53:31,531 - INFO - training.closure - iteration 901: loss = -12.083422922805497
2023-04-08 22:53:34,241 - INFO - training.closure - iteration 902: loss = -12.08380371772277
2023-04-08 22:53:36,953 - INFO - training.closure - iteration 903: loss = -12.084187956731174
2023-04-08 22:53:39,632 - INFO - training.closure - iteration 904: loss = -12.084447531368468
2023-04-08 22:53:42,413 - INFO - training.closure - iteration 905: loss = -12.08480564163907
2023-04-08 22:53:45,240 - INFO - training.closure - iteration 906: loss = -12.08552350444981
2023-04-08 22:53:48,079 - INFO - training.closure - iteration 907: loss = -12.08609091441102
2023-04-08 22:53:50,782 - INFO - training.closure - iteration 908: loss = -12.086029433990817
2023-04-08 22:53:53,536 - INFO - training.closure - iteration 909: loss = -12.086149205217268
2023-04-08 22:53:56,306 - INFO - training.closure - iteration 910: loss = -12.0862964953429
2023-04-08 22:53:59,142 - INFO - training.closure - iteration 911: loss = -12.086412536802293
2023-04-08 22:54:01,848 - INFO - training.closure - iteration 912: loss = -12.08651920415113
2023-04-08 22:54:04,666 - INFO - training.closure - iteration 913: loss = -12.086680460968836
2023-04-08 22:54:07,371 - INFO - training.closure - iteration 914: loss = -12.086646601201952
2023-04-08 22:54:10,115 - INFO - training.closure - iteration 915: loss = -12.086819748383206
2023-04-08 22:54:12,784 - INFO - training.closure - iteration 916: loss = -12.087039914683743
2023-04-08 22:54:15,550 - INFO - training.closure - iteration 917: loss = -12.087358232698438
2023-04-08 22:54:18,272 - INFO - training.closure - iteration 918: loss = -12.087622103853388
2023-04-08 22:54:20,944 - INFO - training.closure - iteration 919: loss = -12.08804279970223
2023-04-08 22:54:23,768 - INFO - training.closure - iteration 920: loss = -12.088283682278714
2023-04-08 22:54:26,516 - INFO - training.closure - iteration 921: loss = -12.08849794408109
2023-04-08 22:54:29,244 - INFO - training.closure - iteration 922: loss = -12.088623201664417
2023-04-08 22:54:31,985 - INFO - training.closure - iteration 923: loss = -12.088815892663135
2023-04-08 22:54:34,748 - INFO - training.closure - iteration 924: loss = -12.088928388717767
2023-04-08 22:54:37,506 - INFO - training.closure - iteration 925: loss = -12.089154747027184
2023-04-08 22:54:40,276 - INFO - training.closure - iteration 926: loss = -12.089334520038104
2023-04-08 22:54:43,069 - INFO - training.closure - iteration 927: loss = -12.089502468478486
2023-04-08 22:54:45,798 - INFO - training.closure - iteration 928: loss = -12.089334081221654
2023-04-08 22:54:48,618 - INFO - training.closure - iteration 929: loss = -12.08963111583997
2023-04-08 22:54:51,398 - INFO - training.closure - iteration 930: loss = -12.089774401752734
2023-04-08 22:54:54,062 - INFO - training.closure - iteration 931: loss = -12.089875506177275
2023-04-08 22:54:56,832 - INFO - training.closure - iteration 932: loss = -12.08973835801228
2023-04-08 22:54:59,535 - INFO - training.closure - iteration 933: loss = -12.089908624948809
2023-04-08 22:55:02,293 - INFO - training.closure - iteration 934: loss = -12.08999169347874
2023-04-08 22:55:05,140 - INFO - training.closure - iteration 935: loss = -12.09003046552591
2023-04-08 22:55:07,827 - INFO - training.closure - iteration 936: loss = -12.090101372246629
2023-04-08 22:55:10,619 - INFO - training.closure - iteration 937: loss = -12.09014627129633
2023-04-08 22:55:13,353 - INFO - training.closure - iteration 938: loss = -12.090285446036358
2023-04-08 22:55:16,149 - INFO - training.closure - iteration 939: loss = -12.088556826112551
2023-04-08 22:55:18,898 - INFO - training.closure - iteration 940: loss = -12.090341992562678
2023-04-08 22:55:21,596 - INFO - training.closure - iteration 941: loss = -12.090626401544409
2023-04-08 22:55:24,321 - INFO - training.closure - iteration 942: loss = -12.09117862815198
2023-04-08 22:55:27,047 - INFO - training.closure - iteration 943: loss = -12.091802057471899
2023-04-08 22:55:29,812 - INFO - training.closure - iteration 944: loss = -12.092076425836975
2023-04-08 22:55:32,565 - INFO - training.closure - iteration 945: loss = -12.092583298009956
2023-04-08 22:55:35,380 - INFO - training.closure - iteration 946: loss = -12.092732063325993
2023-04-08 22:55:38,109 - INFO - training.closure - iteration 947: loss = -12.092871761724243
2023-04-08 22:55:40,792 - INFO - training.closure - iteration 948: loss = -12.09309068106685
2023-04-08 22:55:43,547 - INFO - training.closure - iteration 949: loss = -12.09373136278592
2023-04-08 22:55:46,374 - INFO - training.closure - iteration 950: loss = -12.089755006741665
2023-04-08 22:55:49,233 - INFO - training.closure - iteration 951: loss = -12.093846913586159
2023-04-08 22:55:52,113 - INFO - training.closure - iteration 952: loss = -12.093684466545696
2023-04-08 22:55:54,787 - INFO - training.closure - iteration 953: loss = -12.09412788168429
2023-04-08 22:55:57,454 - INFO - training.closure - iteration 954: loss = -12.094314977941721
2023-04-08 22:56:00,203 - INFO - training.closure - iteration 955: loss = -12.094448031347962
2023-04-08 22:56:02,905 - INFO - training.closure - iteration 956: loss = -12.094531461026335
2023-04-08 22:56:05,630 - INFO - training.closure - iteration 957: loss = -12.09465153062007
2023-04-08 22:56:08,456 - INFO - training.closure - iteration 958: loss = -12.094787883076272
2023-04-08 22:56:11,213 - INFO - training.closure - iteration 959: loss = -12.073497555947215
2023-04-08 22:56:13,887 - INFO - training.closure - iteration 960: loss = -12.094824090417625
2023-04-08 22:56:16,590 - INFO - training.closure - iteration 961: loss = -12.094992402631721
2023-04-08 22:56:19,370 - INFO - training.closure - iteration 962: loss = -12.095237734313448
2023-04-08 22:56:22,177 - INFO - training.closure - iteration 963: loss = -12.095424294197539
2023-04-08 22:56:24,953 - INFO - training.closure - iteration 964: loss = -12.095568113261638
2023-04-08 22:56:27,675 - INFO - training.closure - iteration 965: loss = -12.095672451464292
2023-04-08 22:56:30,392 - INFO - training.closure - iteration 966: loss = -12.095770095498583
2023-04-08 22:56:33,151 - INFO - training.closure - iteration 967: loss = -12.095826634851113
2023-04-08 22:56:35,967 - INFO - training.closure - iteration 968: loss = -12.095842837458004
2023-04-08 22:56:38,687 - INFO - training.closure - iteration 969: loss = -12.095866084289696
2023-04-08 22:56:41,485 - INFO - training.closure - iteration 970: loss = -12.095956561054257
2023-04-08 22:56:44,215 - INFO - training.closure - iteration 971: loss = -12.096010137073089
2023-04-08 22:56:46,926 - INFO - training.closure - iteration 972: loss = -12.095722252566395
2023-04-08 22:56:49,659 - INFO - training.closure - iteration 973: loss = -12.096066312877662
2023-04-08 22:56:52,461 - INFO - training.closure - iteration 974: loss = -12.096108608046592
2023-04-08 22:56:55,212 - INFO - training.closure - iteration 975: loss = -12.096123331954258
2023-04-08 22:56:57,919 - INFO - training.closure - iteration 976: loss = -12.096146574610962
2023-04-08 22:57:00,711 - INFO - training.closure - iteration 977: loss = -12.096193626376962
2023-04-08 22:57:03,452 - INFO - training.closure - iteration 978: loss = -12.096294394644149
2023-04-08 22:57:06,152 - INFO - training.closure - iteration 979: loss = -12.096430057555875
2023-04-08 22:57:08,820 - INFO - training.closure - iteration 980: loss = -12.09659870977656
2023-04-08 22:57:11,514 - INFO - training.closure - iteration 981: loss = -12.096725924814839
2023-04-08 22:57:14,237 - INFO - training.closure - iteration 982: loss = -12.09682673323859
2023-04-08 22:57:17,014 - INFO - training.closure - iteration 983: loss = -12.096899046730561
2023-04-08 22:57:19,765 - INFO - training.closure - iteration 984: loss = -12.097008190009534
2023-04-08 22:57:22,511 - INFO - training.closure - iteration 985: loss = -12.097320152839831
2023-04-08 22:57:25,312 - INFO - training.closure - iteration 986: loss = -12.09754412882127
2023-04-08 22:57:28,089 - INFO - training.closure - iteration 987: loss = -12.097665066579477
2023-04-08 22:57:30,811 - INFO - training.closure - iteration 988: loss = -12.097733444250755
2023-04-08 22:57:33,651 - INFO - training.closure - iteration 989: loss = -12.09778632995603
2023-04-08 22:57:36,429 - INFO - training.closure - iteration 990: loss = -12.09781455205368
2023-04-08 22:57:39,125 - INFO - training.closure - iteration 991: loss = -12.097874575384367
2023-04-08 22:57:41,844 - INFO - training.closure - iteration 992: loss = -12.097906647572444
2023-04-08 22:57:44,636 - INFO - training.closure - iteration 993: loss = -12.097648091342622
2023-04-08 22:57:47,367 - INFO - training.closure - iteration 994: loss = -12.097915248344409
2023-04-08 22:57:50,073 - INFO - training.closure - iteration 995: loss = -12.097937982607768
2023-04-08 22:57:52,873 - INFO - training.closure - iteration 996: loss = -12.097955240275487
2023-04-08 22:57:55,573 - INFO - training.closure - iteration 997: loss = -12.097986406867793
2023-04-08 22:57:58,306 - INFO - training.closure - iteration 998: loss = -12.098020583101519
2023-04-08 22:58:01,016 - INFO - training.closure - iteration 999: loss = -12.098087052621224
2023-04-08 22:58:03,755 - INFO - training.closure - iteration 1000: loss = -12.09815764788824
2023-04-08 22:58:06,479 - INFO - training.closure - iteration 1001: loss = -12.09826374503665
2023-04-08 22:58:09,288 - INFO - training.closure - iteration 1002: loss = -12.098464556444954
2023-04-08 22:58:12,065 - INFO - training.closure - iteration 1003: loss = -12.098679386483434
2023-04-08 22:58:14,814 - INFO - training.closure - iteration 1004: loss = -12.098952674289002
2023-04-08 22:58:17,661 - INFO - training.closure - iteration 1005: loss = -12.099179750650782
2023-04-08 22:58:20,374 - INFO - training.closure - iteration 1006: loss = -12.099064476774231
2023-04-08 22:58:23,157 - INFO - training.closure - iteration 1007: loss = -12.099272508047244
2023-04-08 22:58:25,900 - INFO - training.closure - iteration 1008: loss = -12.099359201280688
2023-04-08 22:58:28,570 - INFO - training.closure - iteration 1009: loss = -12.099391141471333
2023-04-08 22:58:31,266 - INFO - training.closure - iteration 1010: loss = -12.09942916209097
2023-04-08 22:58:33,970 - INFO - training.closure - iteration 1011: loss = -12.099472621276785
2023-04-08 22:58:36,653 - INFO - training.closure - iteration 1012: loss = -12.099519054546597
2023-04-08 22:58:39,386 - INFO - training.closure - iteration 1013: loss = -12.09959587722695
2023-04-08 22:58:42,099 - INFO - training.closure - iteration 1014: loss = -12.099699519655609
2023-04-08 22:58:44,908 - INFO - training.closure - iteration 1015: loss = -12.07454030574779
2023-04-08 22:58:47,882 - INFO - training.closure - iteration 1016: loss = -12.09970976814438
2023-04-08 22:58:50,667 - INFO - training.closure - iteration 1017: loss = -12.099822029836485
2023-04-08 22:58:53,387 - INFO - training.closure - iteration 1018: loss = -12.099966496983264
2023-04-08 22:58:56,080 - INFO - training.closure - iteration 1019: loss = -12.10003271314158
2023-04-08 22:58:58,785 - INFO - training.closure - iteration 1020: loss = -12.10005114750011
2023-04-08 22:59:01,604 - INFO - training.closure - iteration 1021: loss = -12.100074933743535
2023-04-08 22:59:04,351 - INFO - training.closure - iteration 1022: loss = -12.100198393570562
2023-04-08 22:59:07,113 - INFO - training.closure - iteration 1023: loss = -12.1002897880862
2023-04-08 22:59:09,815 - INFO - training.closure - iteration 1024: loss = -12.100443293602282
2023-04-08 22:59:12,531 - INFO - training.closure - iteration 1025: loss = -12.100535327173983
2023-04-08 22:59:15,276 - INFO - training.closure - iteration 1026: loss = -12.100581091457352
2023-04-08 22:59:18,170 - INFO - training.closure - iteration 1027: loss = -12.100620610773605
2023-04-08 22:59:20,868 - INFO - training.closure - iteration 1028: loss = -12.100638899170153
2023-04-08 22:59:23,542 - INFO - training.closure - iteration 1029: loss = -12.10064874599384
2023-04-08 22:59:26,205 - INFO - training.closure - iteration 1030: loss = -12.100663130923973
2023-04-08 22:59:28,897 - INFO - training.closure - iteration 1031: loss = -12.100691707272214
2023-04-08 22:59:31,564 - INFO - training.closure - iteration 1032: loss = -12.100693733011044
2023-04-08 22:59:34,291 - INFO - training.closure - iteration 1033: loss = -12.100717538758428
2023-04-08 22:59:37,146 - INFO - training.closure - iteration 1034: loss = -12.100754383946464
2023-04-08 22:59:39,882 - INFO - training.closure - iteration 1035: loss = -12.100812167908039
2023-04-08 22:59:42,687 - INFO - training.closure - iteration 1036: loss = -12.100821245823967
2023-04-08 22:59:45,416 - INFO - training.closure - iteration 1037: loss = -12.100865210321173
2023-04-08 22:59:48,152 - INFO - training.closure - iteration 1038: loss = -12.10092900552349
2023-04-08 22:59:50,894 - INFO - training.closure - iteration 1039: loss = -12.101155083749429
2023-04-08 22:59:53,666 - INFO - training.closure - iteration 1040: loss = -12.101260492725778
2023-04-08 22:59:56,420 - INFO - training.closure - iteration 1041: loss = -12.101389575753846
2023-04-08 22:59:59,124 - INFO - training.closure - iteration 1042: loss = -12.101434058595572
2023-04-08 23:00:01,871 - INFO - training.closure - iteration 1043: loss = -12.101497497226225
2023-04-08 23:00:04,563 - INFO - training.closure - iteration 1044: loss = -12.101514197785175
2023-04-08 23:00:07,331 - INFO - training.closure - iteration 1045: loss = -12.101575988778736
2023-04-08 23:00:10,194 - INFO - training.closure - iteration 1046: loss = -12.101599154731515
2023-04-08 23:00:12,917 - INFO - training.closure - iteration 1047: loss = -12.101619093393001
2023-04-08 23:00:15,666 - INFO - training.closure - iteration 1048: loss = -12.101648749762301
2023-04-08 23:00:18,411 - INFO - training.closure - iteration 1049: loss = -12.101670543870348
2023-04-08 23:00:21,152 - INFO - training.closure - iteration 1050: loss = -12.101683510416446
2023-04-08 23:00:23,903 - INFO - training.closure - iteration 1051: loss = -12.10169481565022
2023-04-08 23:00:26,628 - INFO - training.closure - iteration 1052: loss = -12.10170591871572
2023-04-08 23:00:29,492 - INFO - training.closure - iteration 1053: loss = -12.101635660594088
2023-04-08 23:00:32,351 - INFO - training.closure - iteration 1054: loss = -12.10171271594199
2023-04-08 23:00:35,048 - INFO - training.closure - iteration 1055: loss = -12.101719284251413
2023-04-08 23:00:37,785 - INFO - training.closure - iteration 1056: loss = -12.101723863782436
2023-04-08 23:00:40,443 - INFO - training.closure - iteration 1057: loss = -12.101746349602344
2023-04-08 23:00:43,094 - INFO - training.closure - iteration 1058: loss = -12.101763149755294
2023-04-08 23:00:45,903 - INFO - training.closure - iteration 1059: loss = -12.101838227191987
2023-04-08 23:00:48,566 - INFO - training.closure - iteration 1060: loss = -12.101904685317452
2023-04-08 23:00:51,202 - INFO - training.closure - iteration 1061: loss = -12.101998758656789
2023-04-08 23:00:53,887 - INFO - training.closure - iteration 1062: loss = -12.102115222206557
2023-04-08 23:00:56,594 - INFO - training.closure - iteration 1063: loss = -12.102197931701621
2023-04-08 23:00:59,238 - INFO - training.closure - iteration 1064: loss = -12.102263759810782
2023-04-08 23:01:02,037 - INFO - training.closure - iteration 1065: loss = -12.1023205357066
2023-04-08 23:01:04,730 - INFO - training.closure - iteration 1066: loss = -12.10238660049688
2023-04-08 23:01:07,402 - INFO - training.closure - iteration 1067: loss = -12.102519301331736
2023-04-08 23:01:10,033 - INFO - training.closure - iteration 1068: loss = -12.102667435778534
2023-04-08 23:01:12,680 - INFO - training.closure - iteration 1069: loss = -12.10256203921925
2023-04-08 23:01:15,389 - INFO - training.closure - iteration 1070: loss = -12.102782499724134
2023-04-08 23:01:18,082 - INFO - training.closure - iteration 1071: loss = -12.102964295526204
2023-04-08 23:01:20,905 - INFO - training.closure - iteration 1072: loss = -12.10082100304994
2023-04-08 23:01:23,563 - INFO - training.closure - iteration 1073: loss = -12.103005631072332
2023-04-08 23:01:26,235 - INFO - training.closure - iteration 1074: loss = -12.10308405822137
2023-04-08 23:01:28,892 - INFO - training.closure - iteration 1075: loss = -12.103216434548072
2023-04-08 23:01:31,548 - INFO - training.closure - iteration 1076: loss = -12.103482566593732
2023-04-08 23:01:34,221 - INFO - training.closure - iteration 1077: loss = -12.103664836602878
2023-04-08 23:01:36,963 - INFO - training.closure - iteration 1078: loss = -12.103853826993415
2023-04-08 23:01:39,705 - INFO - training.closure - iteration 1079: loss = -12.104003261753299
2023-04-08 23:01:42,400 - INFO - training.closure - iteration 1080: loss = -12.104074012397524
2023-04-08 23:01:45,037 - INFO - training.closure - iteration 1081: loss = -12.104155887097125
2023-04-08 23:01:47,718 - INFO - training.closure - iteration 1082: loss = -12.104189556530343
2023-04-08 23:01:50,415 - INFO - training.closure - iteration 1083: loss = -12.104280726798796
2023-04-08 23:01:53,160 - INFO - training.closure - iteration 1084: loss = -12.104318019534176
2023-04-08 23:01:55,819 - INFO - training.closure - iteration 1085: loss = -12.104379725784451
2023-04-08 23:01:58,481 - INFO - training.closure - iteration 1086: loss = -12.104441934795906
2023-04-08 23:02:01,148 - INFO - training.closure - iteration 1087: loss = -12.104554360022313
2023-04-08 23:02:03,806 - INFO - training.closure - iteration 1088: loss = -12.104730462720767
2023-04-08 23:02:06,519 - INFO - training.closure - iteration 1089: loss = -12.10361571437411
2023-04-08 23:02:09,187 - INFO - training.closure - iteration 1090: loss = -12.105003919155024
2023-04-08 23:02:11,921 - INFO - training.closure - iteration 1091: loss = -12.105481753085598
2023-04-08 23:02:14,584 - INFO - training.closure - iteration 1092: loss = -12.10596398336526
2023-04-08 23:02:17,256 - INFO - training.closure - iteration 1093: loss = -12.106227572058344
2023-04-08 23:02:19,913 - INFO - training.closure - iteration 1094: loss = -12.106524779555926
2023-04-08 23:02:22,696 - INFO - training.closure - iteration 1095: loss = -12.106696094804075
2023-04-08 23:02:25,341 - INFO - training.closure - iteration 1096: loss = -12.1068431540143
2023-04-08 23:02:28,090 - INFO - training.closure - iteration 1097: loss = -12.10689212084245
2023-04-08 23:02:30,761 - INFO - training.closure - iteration 1098: loss = -12.107112326873471
2023-04-08 23:02:33,413 - INFO - training.closure - iteration 1099: loss = -12.107208675440525
2023-04-08 23:02:36,097 - INFO - training.closure - iteration 1100: loss = -12.1073196040408
2023-04-08 23:02:38,774 - INFO - training.closure - iteration 1101: loss = -12.107421932886549
2023-04-08 23:02:41,442 - INFO - training.closure - iteration 1102: loss = -12.107513637255042
2023-04-08 23:02:44,165 - INFO - training.closure - iteration 1103: loss = -12.107600492028897
2023-04-08 23:02:46,812 - INFO - training.closure - iteration 1104: loss = -12.107712306964364
2023-04-08 23:02:49,468 - INFO - training.closure - iteration 1105: loss = -12.107845060368735
2023-04-08 23:02:52,115 - INFO - training.closure - iteration 1106: loss = -12.108069175950837
2023-04-08 23:02:54,746 - INFO - training.closure - iteration 1107: loss = -12.108265065128323
2023-04-08 23:02:57,405 - INFO - training.closure - iteration 1108: loss = -12.108582662234502
2023-04-08 23:03:00,104 - INFO - training.closure - iteration 1109: loss = -12.098376847753766
2023-04-08 23:03:02,870 - INFO - training.closure - iteration 1110: loss = -12.109234335300432
2023-04-08 23:03:05,531 - INFO - training.closure - iteration 1111: loss = -12.1103593890635
2023-04-08 23:03:08,170 - INFO - training.closure - iteration 1112: loss = -12.114200124991381
2023-04-08 23:03:10,832 - INFO - training.closure - iteration 1113: loss = -12.113127859152813
2023-04-08 23:03:13,493 - INFO - training.closure - iteration 1114: loss = -12.11478682269415
2023-04-08 23:03:16,165 - INFO - training.closure - iteration 1115: loss = -12.115520755698942
2023-04-08 23:03:18,932 - INFO - training.closure - iteration 1116: loss = -12.116068021586578
2023-04-08 23:03:21,589 - INFO - training.closure - iteration 1117: loss = -12.116625623462433
2023-04-08 23:03:24,255 - INFO - training.closure - iteration 1118: loss = -12.116904835555607
2023-04-08 23:03:26,923 - INFO - training.closure - iteration 1119: loss = -12.116118448984931
2023-04-08 23:03:29,583 - INFO - training.closure - iteration 1120: loss = -12.117100703814785
2023-04-08 23:03:32,397 - INFO - training.closure - iteration 1121: loss = -12.117400445214418
2023-04-08 23:03:35,134 - INFO - training.closure - iteration 1122: loss = -12.118174857714578
2023-04-08 23:03:37,857 - INFO - training.closure - iteration 1123: loss = -12.118880674312493
2023-04-08 23:03:40,512 - INFO - training.closure - iteration 1124: loss = -12.119624538392745
2023-04-08 23:03:43,187 - INFO - training.closure - iteration 1125: loss = -12.120433998859198
2023-04-08 23:03:45,849 - INFO - training.closure - iteration 1126: loss = -12.121310208758974
2023-04-08 23:03:48,573 - INFO - training.closure - iteration 1127: loss = -12.098229800939212
2023-04-08 23:03:51,249 - INFO - training.closure - iteration 1128: loss = -12.122234170182
2023-04-08 23:03:54,061 - INFO - training.closure - iteration 1129: loss = -12.124424350793467
2023-04-08 23:03:56,723 - INFO - training.closure - iteration 1130: loss = -12.123221118292962
2023-04-08 23:03:59,417 - INFO - training.closure - iteration 1131: loss = -12.125731793040028
2023-04-08 23:04:02,084 - INFO - training.closure - iteration 1132: loss = -12.12636257725264
2023-04-08 23:04:04,792 - INFO - training.closure - iteration 1133: loss = -12.12669933563104
2023-04-08 23:04:07,476 - INFO - training.closure - iteration 1134: loss = -12.127284432099884
2023-04-08 23:07:16,831 - INFO - main.experiment - train - RMSE_b at iteration 0 = 0.09468121373636243
2023-04-08 23:07:16,831 - INFO - main.experiment - train - RMSE_b at last iteration = 0.05571695789682669
2023-04-08 23:07:16,831 - INFO - main.experiment - train - RMSE_a at iteration 0 = 0.20525574100595914
2023-04-08 23:07:16,831 - INFO - main.experiment - train - RMSE_a at last iteration = 0.05116654714842976
2023-04-08 23:07:16,831 - INFO - main.experiment - train - LOGPDF_b at iteration 0 = 86.06819691740364
2023-04-08 23:07:16,831 - INFO - main.experiment - train - LOGPDF_b at last iteration = -2.988587558195693
2023-04-08 23:07:16,831 - INFO - main.experiment - train - LOGPDF_a at iteration 0 = 97247.2284323137
2023-04-08 23:07:16,831 - INFO - main.experiment - train - LOGPDF_a at last iteration = -3.1268308197813006
2023-04-08 23:07:16,831 - INFO - main.experiment - train - LOSS at iteration 0 = 97333.2966292311
2023-04-08 23:07:16,831 - INFO - main.experiment - train - LOSS at last iteration = -6.115418377976994
2023-04-08 23:07:57,010 - INFO - main.experiment - test - RMSE_b at iteration 0 = 0.09407961853295221
2023-04-08 23:07:57,010 - INFO - main.experiment - test - RMSE_b at last iteration = 0.08613300391960613
2023-04-08 23:07:57,010 - INFO - main.experiment - test - RMSE_a at iteration 0 = 0.048492429484383
2023-04-08 23:07:57,010 - INFO - main.experiment - test - RMSE_a at last iteration = 0.0526185438742513
2023-04-08 23:07:57,010 - INFO - main.experiment - test - LOGPDF_b at iteration 0 = 13.004583638141135
2023-04-08 23:07:57,010 - INFO - main.experiment - test - LOGPDF_b at last iteration = 126.57203152429729
2023-04-08 23:07:57,010 - INFO - main.experiment - test - LOGPDF_a at iteration 0 = -3.5317761414283604
2023-04-08 23:07:57,010 - INFO - main.experiment - test - LOGPDF_a at last iteration = 275.0876849012048
2023-04-08 23:07:57,010 - INFO - main.experiment - test - LOSS at iteration 0 = 9.472807496712775
2023-04-08 23:07:57,010 - INFO - main.experiment - test - LOSS at last iteration = 401.6597164255021
2023-04-08 23:07:57,011 - INFO - main.experiment - deep = 5 - plot = True - sigma0 = 0.1
2023-04-08 23:07:57,016 - INFO - training.pre_train_full - empirical mean of x0: 3.0003199843299018
2023-04-08 23:07:57,032 - INFO - training.pre_train_full - initial loss: 12.288520642601817
2023-04-08 23:07:57,053 - INFO - training.closure0 - iteration 0: loss = 12.288520642601817
2023-04-08 23:07:57,072 - INFO - training.closure0 - iteration 1: loss = 6.573871729520528
2023-04-08 23:07:57,088 - INFO - training.closure0 - iteration 2: loss = 5.703951156499713
2023-04-08 23:07:57,103 - INFO - training.closure0 - iteration 3: loss = 4.950793364431133
2023-04-08 23:07:57,117 - INFO - training.closure0 - iteration 4: loss = 4.604157097102442
2023-04-08 23:07:57,131 - INFO - training.closure0 - iteration 5: loss = 4.228016409001814
2023-04-08 23:07:57,146 - INFO - training.closure0 - iteration 6: loss = 3.5415307190345695
2023-04-08 23:07:57,162 - INFO - training.closure0 - iteration 7: loss = 2.6648459715742074
2023-04-08 23:07:57,176 - INFO - training.closure0 - iteration 8: loss = 54849.81672073748
2023-04-08 23:07:57,190 - INFO - training.closure0 - iteration 9: loss = 615.2155099036529
2023-04-08 23:07:57,205 - INFO - training.closure0 - iteration 10: loss = 15.834862662334954
2023-04-08 23:07:57,219 - INFO - training.closure0 - iteration 11: loss = 2.703701788693233
2023-04-08 23:07:57,235 - INFO - training.closure0 - iteration 12: loss = 2.4683045591620694
2023-04-08 23:07:57,255 - INFO - training.closure0 - iteration 13: loss = 2.379416327476303
2023-04-08 23:07:57,274 - INFO - training.closure0 - iteration 14: loss = 2.2261866585801644
2023-04-08 23:07:57,292 - INFO - training.closure0 - iteration 15: loss = 5.942344209712385
2023-04-08 23:07:57,308 - INFO - training.closure0 - iteration 16: loss = 1.7046439940174518
2023-04-08 23:07:57,324 - INFO - training.closure0 - iteration 17: loss = 2.867382416645583
2023-04-08 23:07:57,341 - INFO - training.closure0 - iteration 18: loss = 1.6443631379239991
2023-04-08 23:07:57,361 - INFO - training.closure0 - iteration 19: loss = 1.6236972566141916
2023-04-08 23:07:57,381 - INFO - training.closure0 - iteration 20: loss = 1.5850251731228022
2023-04-08 23:07:57,405 - INFO - training.closure0 - iteration 21: loss = 1.335939302143261
2023-04-08 23:07:57,422 - INFO - training.closure0 - iteration 22: loss = 1.3307385666317995
2023-04-08 23:07:57,439 - INFO - training.closure0 - iteration 23: loss = 1.2753675901110961
2023-04-08 23:07:57,458 - INFO - training.closure0 - iteration 24: loss = 1.5380468817835973
2023-04-08 23:07:57,476 - INFO - training.closure0 - iteration 25: loss = 1.2351876736106886
2023-04-08 23:07:57,495 - INFO - training.closure0 - iteration 26: loss = 1.156011427816231
2023-04-08 23:07:57,513 - INFO - training.closure0 - iteration 27: loss = 1.3612149253531838
2023-04-08 23:07:57,531 - INFO - training.closure0 - iteration 28: loss = 0.9039144674341302
2023-04-08 23:07:57,546 - INFO - training.closure0 - iteration 29: loss = 18.328437454915676
2023-04-08 23:07:57,560 - INFO - training.closure0 - iteration 30: loss = 1.14027335784517
2023-04-08 23:07:57,574 - INFO - training.closure0 - iteration 31: loss = 0.7722224391905093
2023-04-08 23:07:57,589 - INFO - training.closure0 - iteration 32: loss = 113.30970527509916
2023-04-08 23:07:57,603 - INFO - training.closure0 - iteration 33: loss = 1.2860907497992802
2023-04-08 23:07:57,617 - INFO - training.closure0 - iteration 34: loss = 0.07933991925416868
2023-04-08 23:07:57,632 - INFO - training.closure0 - iteration 35: loss = 3.451959858618975e+20
2023-04-08 23:07:57,647 - INFO - training.closure0 - iteration 36: loss = 4.2663185928327204e+18
2023-04-08 23:07:57,663 - INFO - training.closure0 - iteration 37: loss = 5.323397265869329e+16
2023-04-08 23:07:57,689 - INFO - training.closure0 - iteration 38: loss = 17596032930830.258
2023-04-08 23:07:57,750 - INFO - training.closure0 - iteration 39: loss = 449047715.67314035
2023-04-08 23:07:57,775 - INFO - training.closure0 - iteration 40: loss = 523473.99880644365
2023-04-08 23:07:57,794 - INFO - training.closure0 - iteration 41: loss = 4220.620293369865
2023-04-08 23:07:57,812 - INFO - training.closure0 - iteration 42: loss = 100.65314509181759
2023-04-08 23:07:57,830 - INFO - training.closure0 - iteration 43: loss = 3.3387213643664797
2023-04-08 23:07:57,846 - INFO - training.closure0 - iteration 44: loss = 0.053589861404462105
2023-04-08 23:07:57,864 - INFO - training.closure0 - iteration 45: loss = 0.026403102611448372
2023-04-08 23:07:57,881 - INFO - training.closure0 - iteration 46: loss = -0.31504553229837085
2023-04-08 23:07:57,899 - INFO - training.closure0 - iteration 47: loss = -0.5109795599512073
2023-04-08 23:07:57,916 - INFO - training.closure0 - iteration 48: loss = -0.5661199818536518
2023-04-08 23:07:57,933 - INFO - training.closure0 - iteration 49: loss = -0.6973337788463462
2023-04-08 23:07:57,946 - INFO - training.closure0 - iteration 50: loss = -0.3916212850128218
2023-04-08 23:07:57,960 - INFO - training.closure0 - iteration 51: loss = -0.9030882233854274
2023-04-08 23:07:57,978 - INFO - training.closure0 - iteration 52: loss = -0.9572776996001988
2023-04-08 23:07:57,995 - INFO - training.closure0 - iteration 53: loss = -0.9949023164463194
2023-04-08 23:07:58,012 - INFO - training.closure0 - iteration 54: loss = -1.0215697047416208
2023-04-08 23:07:58,032 - INFO - training.closure0 - iteration 55: loss = -1.0950954920039233
2023-04-08 23:07:58,050 - INFO - training.closure0 - iteration 56: loss = -1.2562043670469252
2023-04-08 23:07:58,073 - INFO - training.closure0 - iteration 57: loss = -1.4028144212487403
2023-04-08 23:07:58,094 - INFO - training.closure0 - iteration 58: loss = 17.349418384136367
2023-04-08 23:07:58,112 - INFO - training.closure0 - iteration 59: loss = -1.100675093638874
2023-04-08 23:07:58,130 - INFO - training.closure0 - iteration 60: loss = -1.5564364864278852
2023-04-08 23:07:58,156 - INFO - training.closure0 - iteration 61: loss = -1.5895936637462753
2023-04-08 23:07:58,175 - INFO - training.closure0 - iteration 62: loss = -1.6346245853137984
2023-04-08 23:07:58,194 - INFO - training.closure0 - iteration 63: loss = -1.666750905675848
2023-04-08 23:07:58,212 - INFO - training.closure0 - iteration 64: loss = -1.7204794883137573
2023-04-08 23:07:58,229 - INFO - training.closure0 - iteration 65: loss = -1.756657089707975
2023-04-08 23:07:58,246 - INFO - training.closure0 - iteration 66: loss = -1.8152965458437391
2023-04-08 23:07:58,261 - INFO - training.closure0 - iteration 67: loss = -1.8429081273851988
2023-04-08 23:07:58,277 - INFO - training.closure0 - iteration 68: loss = -1.8503104733348548
2023-04-08 23:07:58,293 - INFO - training.closure0 - iteration 69: loss = -1.851691595648746
2023-04-08 23:07:58,308 - INFO - training.closure0 - iteration 70: loss = -1.8517311470667983
2023-04-08 23:07:58,323 - INFO - training.closure0 - iteration 71: loss = -1.8517408266273305
2023-04-08 23:07:58,341 - INFO - training.closure0 - iteration 72: loss = -1.8517414730031825
2023-04-08 23:07:58,357 - INFO - training.closure0 - iteration 73: loss = -1.851741489228173
2023-04-08 23:07:58,372 - INFO - training.closure0 - iteration 74: loss = -1.8517414899107485
2023-04-08 23:07:58,387 - INFO - training.closure0 - iteration 75: loss = -1.8517414899231879
2023-04-08 23:07:58,402 - INFO - training.closure0 - iteration 76: loss = -1.8517414899232842
2023-04-08 23:07:58,409 - INFO - training.pre_train_full - a0 mean: [2.99442535 3.00621463]
2023-04-08 23:07:58,410 - INFO - training.pre_train_full - a0 var: [0.01004094 0.00844174]
2023-04-08 23:07:58,412 - INFO - training.pre_train_full - a0 covar: [[0.010040941426847168, -0.0005508687775297212], [-0.0005508687775297212, 0.00844174493437062]]
2023-04-08 23:08:22,791 - INFO - training.closure - iteration 0: loss = 5674.535377918826
2023-04-08 23:08:25,371 - INFO - training.closure - iteration 1: loss = 228.74539631925384
2023-04-08 23:08:27,854 - INFO - training.closure - iteration 2: loss = 198.14552263009415
2023-04-08 23:08:30,443 - INFO - training.closure - iteration 3: loss = 100.99621152711964
2023-04-08 23:08:32,959 - INFO - training.closure - iteration 4: loss = 85.07475256341036
2023-04-08 23:08:35,664 - INFO - training.closure - iteration 5: loss = 78.80770857162484
2023-04-08 23:08:38,198 - INFO - training.closure - iteration 6: loss = 52.33595274417369
2023-04-08 23:08:40,731 - INFO - training.closure - iteration 7: loss = 41.108933512696886
2023-04-08 23:08:43,283 - INFO - training.closure - iteration 8: loss = 25.192499756577394
2023-04-08 23:08:45,856 - INFO - training.closure - iteration 9: loss = 13.879743044175218
2023-04-08 23:08:48,538 - INFO - training.closure - iteration 10: loss = 8.665707930615982
2023-04-08 23:08:51,133 - INFO - training.closure - iteration 11: loss = 5.789852771453562
2023-04-08 23:08:53,717 - INFO - training.closure - iteration 12: loss = 4.622773414729279
2023-04-08 23:08:56,303 - INFO - training.closure - iteration 13: loss = 4.111806798908004
2023-04-08 23:08:58,866 - INFO - training.closure - iteration 14: loss = 3.083893785042383
2023-04-08 23:09:01,436 - INFO - training.closure - iteration 15: loss = 2.2760456061128065
2023-04-08 23:09:04,029 - INFO - training.closure - iteration 16: loss = 2.0376036119533607
2023-04-08 23:09:06,673 - INFO - training.closure - iteration 17: loss = 1.8977309582927824
2023-04-08 23:09:09,287 - INFO - training.closure - iteration 18: loss = 1.7585507596621377
2023-04-08 23:09:11,863 - INFO - training.closure - iteration 19: loss = 1.6798301743540691
2023-04-08 23:09:14,466 - INFO - training.closure - iteration 20: loss = 1.5239482812565759
2023-04-08 23:09:17,210 - INFO - training.closure - iteration 21: loss = 1.4191316714683802
2023-04-08 23:09:19,832 - INFO - training.closure - iteration 22: loss = 1.359911631637102
2023-04-08 23:09:22,492 - INFO - training.closure - iteration 23: loss = 1.3217450541489275
2023-04-08 23:09:25,065 - INFO - training.closure - iteration 24: loss = 1.2939942022755309
2023-04-08 23:09:27,675 - INFO - training.closure - iteration 25: loss = 1.2751990465148622
2023-04-08 23:09:30,255 - INFO - training.closure - iteration 26: loss = 1.257555853257032
2023-04-08 23:09:32,818 - INFO - training.closure - iteration 27: loss = 1.2182890399232273
2023-04-08 23:09:35,414 - INFO - training.closure - iteration 28: loss = 1.1665350262802272
2023-04-08 23:09:38,077 - INFO - training.closure - iteration 29: loss = 1.1270530032691577
2023-04-08 23:09:40,708 - INFO - training.closure - iteration 30: loss = 1.084268917955912
2023-04-08 23:09:43,303 - INFO - training.closure - iteration 31: loss = 1.0276160590728232
2023-04-08 23:09:45,886 - INFO - training.closure - iteration 32: loss = 0.9826467577652265
2023-04-08 23:09:48,473 - INFO - training.closure - iteration 33: loss = 0.9318356097672735
2023-04-08 23:09:51,048 - INFO - training.closure - iteration 34: loss = 0.9121985976003697
2023-04-08 23:09:53,643 - INFO - training.closure - iteration 35: loss = 0.8341501633425408
2023-04-08 23:09:56,329 - INFO - training.closure - iteration 36: loss = 0.8000064869803745
2023-04-08 23:09:58,957 - INFO - training.closure - iteration 37: loss = 0.7956016747179067
2023-04-08 23:10:01,546 - INFO - training.closure - iteration 38: loss = 0.7841988691955879
2023-04-08 23:10:04,130 - INFO - training.closure - iteration 39: loss = 0.7794187117286315
2023-04-08 23:10:06,908 - INFO - training.closure - iteration 40: loss = 0.7622937931572507
2023-04-08 23:10:09,502 - INFO - training.closure - iteration 41: loss = 0.7493295944148222
2023-04-08 23:10:12,218 - INFO - training.closure - iteration 42: loss = 0.6935695209421806
2023-04-08 23:10:14,862 - INFO - training.closure - iteration 43: loss = 0.6777438450154833
2023-04-08 23:10:17,462 - INFO - training.closure - iteration 44: loss = 0.6671374649030186
2023-04-08 23:10:20,051 - INFO - training.closure - iteration 45: loss = 0.6550154055741428
2023-04-08 23:10:22,722 - INFO - training.closure - iteration 46: loss = 0.6429981693031122
2023-04-08 23:10:25,308 - INFO - training.closure - iteration 47: loss = 0.6044223814241583
2023-04-08 23:10:27,998 - INFO - training.closure - iteration 48: loss = 0.5972703498981793
2023-04-08 23:10:30,623 - INFO - training.closure - iteration 49: loss = 0.5676018319212337
2023-04-08 23:10:33,202 - INFO - training.closure - iteration 50: loss = 0.42803332366440294
2023-04-08 23:10:35,805 - INFO - training.closure - iteration 51: loss = 0.7456777280651117
2023-04-08 23:10:38,408 - INFO - training.closure - iteration 52: loss = 0.32638767269196545
2023-04-08 23:10:41,045 - INFO - training.closure - iteration 53: loss = 156.87472646050347
2023-04-08 23:10:43,657 - INFO - training.closure - iteration 54: loss = 4.091709206072593
2023-04-08 23:10:46,345 - INFO - training.closure - iteration 55: loss = 0.388825955708072
2023-04-08 23:10:48,939 - INFO - training.closure - iteration 56: loss = 0.28328008197390386
2023-04-08 23:10:51,502 - INFO - training.closure - iteration 57: loss = 0.42738935403606115
2023-04-08 23:10:54,108 - INFO - training.closure - iteration 58: loss = 0.18631301172314974
2023-04-08 23:10:56,702 - INFO - training.closure - iteration 59: loss = 1.0602652153035428
2023-04-08 23:10:59,464 - INFO - training.closure - iteration 60: loss = -0.006965903818596519
2023-04-08 23:11:02,165 - INFO - training.closure - iteration 61: loss = -0.08223656797202383
2023-04-08 23:11:04,751 - INFO - training.closure - iteration 62: loss = 4.423682662155667
2023-04-08 23:11:07,443 - INFO - training.closure - iteration 63: loss = -0.1067351625115538
2023-04-08 23:11:10,121 - INFO - training.closure - iteration 64: loss = -0.17399090705292175
2023-04-08 23:11:12,708 - INFO - training.closure - iteration 65: loss = 56304.56548681253
2023-04-08 23:11:15,330 - INFO - training.closure - iteration 66: loss = 88.02980200843001
2023-04-08 23:11:18,027 - INFO - training.closure - iteration 67: loss = 2.1701712214757967
2023-04-08 23:11:20,651 - INFO - training.closure - iteration 68: loss = -0.18804755822879127
2023-04-08 23:11:23,518 - INFO - training.closure - iteration 69: loss = -0.23150710623048698
2023-04-08 23:11:26,077 - INFO - training.closure - iteration 70: loss = -0.0026235163088630387
2023-04-08 23:11:28,659 - INFO - training.closure - iteration 71: loss = -0.3152913278020446
2023-04-08 23:11:31,236 - INFO - training.closure - iteration 72: loss = -0.4772813521761339
2023-04-08 23:11:33,818 - INFO - training.closure - iteration 73: loss = 0.23751672497435394
2023-04-08 23:11:36,480 - INFO - training.closure - iteration 74: loss = -0.7511035518874113
2023-04-08 23:11:39,060 - INFO - training.closure - iteration 75: loss = 2.0337822530494303
2023-04-08 23:11:41,643 - INFO - training.closure - iteration 76: loss = -0.7845422388078364
2023-04-08 23:11:44,206 - INFO - training.closure - iteration 77: loss = -0.6247389693566953
2023-04-08 23:11:46,774 - INFO - training.closure - iteration 78: loss = -0.9542447440857317
2023-04-08 23:11:49,351 - INFO - training.closure - iteration 79: loss = -1.1321045626454969
2023-04-08 23:11:51,985 - INFO - training.closure - iteration 80: loss = -1.1531064159915947
2023-04-08 23:11:54,554 - INFO - training.closure - iteration 81: loss = -1.2302470112855863
2023-04-08 23:11:57,100 - INFO - training.closure - iteration 82: loss = -0.8051681252233855
2023-04-08 23:11:59,683 - INFO - training.closure - iteration 83: loss = -1.3070874860119785
2023-04-08 23:12:02,254 - INFO - training.closure - iteration 84: loss = -1.3330333272176655
2023-04-08 23:12:04,811 - INFO - training.closure - iteration 85: loss = -1.4038524971860016
2023-04-08 23:12:07,594 - INFO - training.closure - iteration 86: loss = -1.458555999602803
2023-04-08 23:12:10,149 - INFO - training.closure - iteration 87: loss = -1.5139607171025329
2023-04-08 23:12:12,708 - INFO - training.closure - iteration 88: loss = -1.5749647241350604
2023-04-08 23:12:15,431 - INFO - training.closure - iteration 89: loss = -1.6732365987926943
2023-04-08 23:12:18,080 - INFO - training.closure - iteration 90: loss = -1.7841671398175631
2023-04-08 23:12:20,672 - INFO - training.closure - iteration 91: loss = -1.859839715788805
2023-04-08 23:12:23,256 - INFO - training.closure - iteration 92: loss = -1.948205094312681
2023-04-08 23:12:25,911 - INFO - training.closure - iteration 93: loss = -1.9867139690182987
2023-04-08 23:12:28,561 - INFO - training.closure - iteration 94: loss = -2.0134789125714425
2023-04-08 23:12:31,140 - INFO - training.closure - iteration 95: loss = -2.0855539184058034
2023-04-08 23:12:33,726 - INFO - training.closure - iteration 96: loss = -2.1514467211087327
2023-04-08 23:12:36,296 - INFO - training.closure - iteration 97: loss = -2.247520461288731
2023-04-08 23:12:38,885 - INFO - training.closure - iteration 98: loss = -2.31374018420697
2023-04-08 23:12:41,547 - INFO - training.closure - iteration 99: loss = -2.3536370391595387
2023-04-08 23:12:44,129 - INFO - training.closure - iteration 100: loss = 4.212307866107417
2023-04-08 23:12:46,701 - INFO - training.closure - iteration 101: loss = -2.2005144178893836
2023-04-08 23:12:49,280 - INFO - training.closure - iteration 102: loss = -2.3600372069624855
2023-04-08 23:12:51,857 - INFO - training.closure - iteration 103: loss = -2.3808218919360318
2023-04-08 23:12:54,446 - INFO - training.closure - iteration 104: loss = -2.4137555555065147
2023-04-08 23:12:57,218 - INFO - training.closure - iteration 105: loss = -2.4392940586286116
2023-04-08 23:12:59,773 - INFO - training.closure - iteration 106: loss = -2.463119438344836
2023-04-08 23:13:02,381 - INFO - training.closure - iteration 107: loss = -2.512692694712456
2023-04-08 23:13:05,022 - INFO - training.closure - iteration 108: loss = -2.636853440806443
2023-04-08 23:13:07,607 - INFO - training.closure - iteration 109: loss = -3.0177633980474905
2023-04-08 23:13:10,326 - INFO - training.closure - iteration 110: loss = -3.12897953800627
2023-04-08 23:13:12,948 - INFO - training.closure - iteration 111: loss = -3.256047443202924
2023-04-08 23:13:15,587 - INFO - training.closure - iteration 112: loss = -2.707116097917644
2023-04-08 23:13:18,174 - INFO - training.closure - iteration 113: loss = -3.352754021236764
2023-04-08 23:13:20,819 - INFO - training.closure - iteration 114: loss = -3.471559844659837
2023-04-08 23:13:23,451 - INFO - training.closure - iteration 115: loss = -3.4323241611405924
2023-04-08 23:13:26,011 - INFO - training.closure - iteration 116: loss = -3.519312071280225
2023-04-08 23:13:28,635 - INFO - training.closure - iteration 117: loss = -3.5470606842535966
2023-04-08 23:13:31,274 - INFO - training.closure - iteration 118: loss = -3.619170829542022
2023-04-08 23:13:33,851 - INFO - training.closure - iteration 119: loss = -3.6483580710339893
2023-04-08 23:13:36,408 - INFO - training.closure - iteration 120: loss = -3.698990405785449
2023-04-08 23:13:38,976 - INFO - training.closure - iteration 121: loss = -3.730549936317762
2023-04-08 23:13:41,543 - INFO - training.closure - iteration 122: loss = -3.7417641730253415
2023-04-08 23:13:44,105 - INFO - training.closure - iteration 123: loss = -3.7592581885899032
2023-04-08 23:13:46,767 - INFO - training.closure - iteration 124: loss = -3.800030580729289
2023-04-08 23:13:49,354 - INFO - training.closure - iteration 125: loss = -3.849312937262822
2023-04-08 23:13:51,937 - INFO - training.closure - iteration 126: loss = -3.8820681447536414
2023-04-08 23:13:54,507 - INFO - training.closure - iteration 127: loss = -3.928613192434998
2023-04-08 23:13:57,105 - INFO - training.closure - iteration 128: loss = -4.036835058025767
2023-04-08 23:13:59,709 - INFO - training.closure - iteration 129: loss = -4.249833692167134
2023-04-08 23:14:02,296 - INFO - training.closure - iteration 130: loss = -4.250994043411446
2023-04-08 23:14:04,945 - INFO - training.closure - iteration 131: loss = -4.532184980986774
2023-04-08 23:14:07,521 - INFO - training.closure - iteration 132: loss = -4.642676564778064
2023-04-08 23:14:10,101 - INFO - training.closure - iteration 133: loss = -4.770161200974375
2023-04-08 23:14:12,654 - INFO - training.closure - iteration 134: loss = -4.8009654253976635
2023-04-08 23:14:15,325 - INFO - training.closure - iteration 135: loss = -4.843561169761028
2023-04-08 23:14:17,947 - INFO - training.closure - iteration 136: loss = -4.852620769390992
2023-04-08 23:14:20,679 - INFO - training.closure - iteration 137: loss = -4.863509448136593
2023-04-08 23:14:23,309 - INFO - training.closure - iteration 138: loss = -4.879667790969114
2023-04-08 23:14:25,946 - INFO - training.closure - iteration 139: loss = -4.892756787978352
2023-04-08 23:14:28,619 - INFO - training.closure - iteration 140: loss = -4.931475606864018
2023-04-08 23:14:31,249 - INFO - training.closure - iteration 141: loss = -5.044980410814065
2023-04-08 23:14:33,883 - INFO - training.closure - iteration 142: loss = -5.152647421590896
2023-04-08 23:14:36,606 - INFO - training.closure - iteration 143: loss = -4.622592057358645
2023-04-08 23:14:39,248 - INFO - training.closure - iteration 144: loss = -5.238131121020375
2023-04-08 23:14:41,874 - INFO - training.closure - iteration 145: loss = -5.312080588597199
2023-04-08 23:14:44,539 - INFO - training.closure - iteration 146: loss = -5.412465888497928
2023-04-08 23:14:47,188 - INFO - training.closure - iteration 147: loss = -5.550906891045127
2023-04-08 23:14:49,827 - INFO - training.closure - iteration 148: loss = -5.641924947784393
2023-04-08 23:14:52,474 - INFO - training.closure - iteration 149: loss = -5.677883256998769
2023-04-08 23:14:55,192 - INFO - training.closure - iteration 150: loss = -5.607088645333802
2023-04-08 23:14:57,832 - INFO - training.closure - iteration 151: loss = -5.734446084017939
2023-04-08 23:15:00,476 - INFO - training.closure - iteration 152: loss = -5.7584893151160585
2023-04-08 23:15:03,117 - INFO - training.closure - iteration 153: loss = -5.749602905576925
2023-04-08 23:15:05,758 - INFO - training.closure - iteration 154: loss = -5.767549630276923
2023-04-08 23:15:08,404 - INFO - training.closure - iteration 155: loss = -5.773182406679757
2023-04-08 23:15:11,134 - INFO - training.closure - iteration 156: loss = -5.784387277813507
2023-04-08 23:15:13,750 - INFO - training.closure - iteration 157: loss = -5.80201114493485
2023-04-08 23:15:16,393 - INFO - training.closure - iteration 158: loss = -5.836017926669225
2023-04-08 23:15:19,041 - INFO - training.closure - iteration 159: loss = -5.870806586385831
2023-04-08 23:15:21,659 - INFO - training.closure - iteration 160: loss = -5.8989536466006465
2023-04-08 23:15:24,372 - INFO - training.closure - iteration 161: loss = -5.913035410843259
2023-04-08 23:15:27,093 - INFO - training.closure - iteration 162: loss = 15.548803236075932
2023-04-08 23:15:29,730 - INFO - training.closure - iteration 163: loss = -5.7583466477599625
2023-04-08 23:15:32,347 - INFO - training.closure - iteration 164: loss = -5.943313055267112
2023-04-08 23:15:34,997 - INFO - training.closure - iteration 165: loss = -6.02635056553395
2023-04-08 23:15:37,624 - INFO - training.closure - iteration 166: loss = -6.135594545941252
2023-04-08 23:15:40,242 - INFO - training.closure - iteration 167: loss = -6.164269698765911
2023-04-08 23:15:42,891 - INFO - training.closure - iteration 168: loss = -6.185842215057368
2023-04-08 23:15:45,662 - INFO - training.closure - iteration 169: loss = -6.238679687382743
2023-04-08 23:15:48,336 - INFO - training.closure - iteration 170: loss = -6.25074134236588
2023-04-08 23:15:50,977 - INFO - training.closure - iteration 171: loss = -6.267559665128275
2023-04-08 23:15:53,618 - INFO - training.closure - iteration 172: loss = -6.282280726192991
2023-04-08 23:15:56,376 - INFO - training.closure - iteration 173: loss = -6.289958221040486
2023-04-08 23:15:59,112 - INFO - training.closure - iteration 174: loss = -6.295414049721932
2023-04-08 23:16:01,931 - INFO - training.closure - iteration 175: loss = -6.297682580675152
2023-04-08 23:16:04,560 - INFO - training.closure - iteration 176: loss = -6.285375549693691
2023-04-08 23:16:07,236 - INFO - training.closure - iteration 177: loss = -6.300524733028678
2023-04-08 23:16:09,977 - INFO - training.closure - iteration 178: loss = -6.305729158214923
2023-04-08 23:16:12,627 - INFO - training.closure - iteration 179: loss = -6.3152402175402695
2023-04-08 23:16:15,257 - INFO - training.closure - iteration 180: loss = -6.326078953770525
2023-04-08 23:16:17,948 - INFO - training.closure - iteration 181: loss = -6.344505442620784
2023-04-08 23:16:20,661 - INFO - training.closure - iteration 182: loss = -6.296004778358384
2023-04-08 23:16:23,274 - INFO - training.closure - iteration 183: loss = -6.350820109271672
2023-04-08 23:16:25,897 - INFO - training.closure - iteration 184: loss = -6.363402900951048
2023-04-08 23:16:28,709 - INFO - training.closure - iteration 185: loss = -6.376853651034464
2023-04-08 23:16:31,355 - INFO - training.closure - iteration 186: loss = -6.393163794904108
2023-04-08 23:16:33,992 - INFO - training.closure - iteration 187: loss = -4.735785315004922
2023-04-08 23:16:36,818 - INFO - training.closure - iteration 188: loss = -6.410293881397913
2023-04-08 23:16:39,466 - INFO - training.closure - iteration 189: loss = -6.4596579948835355
2023-04-08 23:16:42,303 - INFO - training.closure - iteration 190: loss = -6.488223514797617
2023-04-08 23:16:45,061 - INFO - training.closure - iteration 191: loss = -6.508082475419664
2023-04-08 23:16:47,673 - INFO - training.closure - iteration 192: loss = -6.463069114113995
2023-04-08 23:16:50,281 - INFO - training.closure - iteration 193: loss = -6.510372663419783
2023-04-08 23:16:52,992 - INFO - training.closure - iteration 194: loss = -6.516957229929153
2023-04-08 23:16:55,635 - INFO - training.closure - iteration 195: loss = -6.5291970235667485
2023-04-08 23:16:58,332 - INFO - training.closure - iteration 196: loss = -6.539201285011132
2023-04-08 23:17:01,014 - INFO - training.closure - iteration 197: loss = -6.5485999603542755
2023-04-08 23:17:03,668 - INFO - training.closure - iteration 198: loss = -6.546535611044744
2023-04-08 23:17:06,335 - INFO - training.closure - iteration 199: loss = -6.550790310228601
2023-04-08 23:17:09,064 - INFO - training.closure - iteration 200: loss = -6.553369056001829
2023-04-08 23:17:11,712 - INFO - training.closure - iteration 201: loss = -6.553238202775679
2023-04-08 23:17:14,372 - INFO - training.closure - iteration 202: loss = -6.553902198514155
2023-04-08 23:17:17,020 - INFO - training.closure - iteration 203: loss = -6.555215141773908
2023-04-08 23:17:19,645 - INFO - training.closure - iteration 204: loss = -6.556549742976706
2023-04-08 23:17:22,298 - INFO - training.closure - iteration 205: loss = -6.559513988029563
2023-04-08 23:17:24,909 - INFO - training.closure - iteration 206: loss = -6.564647578220121
2023-04-08 23:17:27,616 - INFO - training.closure - iteration 207: loss = -6.568057349051523
2023-04-08 23:17:30,247 - INFO - training.closure - iteration 208: loss = -6.569623497595984
2023-04-08 23:17:32,879 - INFO - training.closure - iteration 209: loss = -6.570508740962801
2023-04-08 23:17:35,633 - INFO - training.closure - iteration 210: loss = -6.5719045056430705
2023-04-08 23:17:38,479 - INFO - training.closure - iteration 211: loss = -6.575851205200511
2023-04-08 23:17:41,308 - INFO - training.closure - iteration 212: loss = -6.581603394538408
2023-04-08 23:17:44,057 - INFO - training.closure - iteration 213: loss = -6.591190593634563
2023-04-08 23:17:46,708 - INFO - training.closure - iteration 214: loss = -6.1148932535429745
2023-04-08 23:17:49,396 - INFO - training.closure - iteration 215: loss = -6.596674639224437
2023-04-08 23:17:52,015 - INFO - training.closure - iteration 216: loss = -6.612803971295884
2023-04-08 23:17:54,643 - INFO - training.closure - iteration 217: loss = -6.639736966206556
2023-04-08 23:17:57,255 - INFO - training.closure - iteration 218: loss = -6.656575357465955
2023-04-08 23:17:59,944 - INFO - training.closure - iteration 219: loss = -6.6638988369801915
2023-04-08 23:18:02,603 - INFO - training.closure - iteration 220: loss = -6.677070499341026
2023-04-08 23:18:05,210 - INFO - training.closure - iteration 221: loss = -6.683495951074224
2023-04-08 23:18:07,877 - INFO - training.closure - iteration 222: loss = -6.573996197208429
2023-04-08 23:18:10,525 - INFO - training.closure - iteration 223: loss = -6.705862668558494
2023-04-08 23:18:13,171 - INFO - training.closure - iteration 224: loss = -6.716595456932488
2023-04-08 23:18:15,800 - INFO - training.closure - iteration 225: loss = -6.721286860373084
2023-04-08 23:18:18,547 - INFO - training.closure - iteration 226: loss = -6.732021058357656
2023-04-08 23:18:21,291 - INFO - training.closure - iteration 227: loss = 22.970108900456
2023-04-08 23:18:24,037 - INFO - training.closure - iteration 228: loss = -5.7164938030510815
2023-04-08 23:18:26,735 - INFO - training.closure - iteration 229: loss = -6.723222803747783
2023-04-08 23:18:29,374 - INFO - training.closure - iteration 230: loss = -6.735679930008619
2023-04-08 23:18:32,072 - INFO - training.closure - iteration 231: loss = -6.724749872289218
2023-04-08 23:18:34,777 - INFO - training.closure - iteration 232: loss = -6.746688850918401
2023-04-08 23:18:37,398 - INFO - training.closure - iteration 233: loss = -6.754400455817846
2023-04-08 23:18:40,024 - INFO - training.closure - iteration 234: loss = -6.7582900761437275
2023-04-08 23:18:42,727 - INFO - training.closure - iteration 235: loss = -6.7607301319222985
2023-04-08 23:18:45,369 - INFO - training.closure - iteration 236: loss = -6.7630429663997855
2023-04-08 23:18:48,016 - INFO - training.closure - iteration 237: loss = -6.7702938003268605
2023-04-08 23:18:50,720 - INFO - training.closure - iteration 238: loss = -6.780134057656084
2023-04-08 23:18:53,341 - INFO - training.closure - iteration 239: loss = -6.78684211965604
2023-04-08 23:18:55,954 - INFO - training.closure - iteration 240: loss = -6.791210935850513
2023-04-08 23:18:58,565 - INFO - training.closure - iteration 241: loss = -6.795138797417999
2023-04-08 23:19:01,199 - INFO - training.closure - iteration 242: loss = -6.796220798444792
2023-04-08 23:19:03,995 - INFO - training.closure - iteration 243: loss = -6.797276002892691
2023-04-08 23:19:06,707 - INFO - training.closure - iteration 244: loss = -6.800242190229166
2023-04-08 23:19:09,424 - INFO - training.closure - iteration 245: loss = -6.80111316700492
2023-04-08 23:19:12,183 - INFO - training.closure - iteration 246: loss = -6.802299100898275
2023-04-08 23:19:14,816 - INFO - training.closure - iteration 247: loss = -6.803642592201848
2023-04-08 23:19:17,450 - INFO - training.closure - iteration 248: loss = -6.806057385127064
2023-04-08 23:19:20,065 - INFO - training.closure - iteration 249: loss = -6.81138649983458
2023-04-08 23:19:22,718 - INFO - training.closure - iteration 250: loss = -6.810764817283072
2023-04-08 23:19:25,453 - INFO - training.closure - iteration 251: loss = -6.813466354583925
2023-04-08 23:19:28,086 - INFO - training.closure - iteration 252: loss = -6.819868284327242
2023-04-08 23:19:30,725 - INFO - training.closure - iteration 253: loss = -6.823500002929659
2023-04-08 23:19:33,376 - INFO - training.closure - iteration 254: loss = -6.8260039953864045
2023-04-08 23:19:36,027 - INFO - training.closure - iteration 255: loss = -6.828263124338855
2023-04-08 23:19:38,729 - INFO - training.closure - iteration 256: loss = -6.844237446775826
2023-04-08 23:19:41,449 - INFO - training.closure - iteration 257: loss = -6.867225100377656
2023-04-08 23:19:44,085 - INFO - training.closure - iteration 258: loss = 1029.596906931964
2023-04-08 23:19:46,790 - INFO - training.closure - iteration 259: loss = 14.45355197564791
2023-04-08 23:19:49,440 - INFO - training.closure - iteration 260: loss = -6.236100367925299
2023-04-08 23:19:52,097 - INFO - training.closure - iteration 261: loss = -6.85702614771405
2023-04-08 23:19:54,745 - INFO - training.closure - iteration 262: loss = -6.870245564085726
2023-04-08 23:19:57,396 - INFO - training.closure - iteration 263: loss = -6.878445569458026
2023-04-08 23:20:00,265 - INFO - training.closure - iteration 264: loss = -6.900086896435935
2023-04-08 23:20:03,059 - INFO - training.closure - iteration 265: loss = -6.929307939177172
2023-04-08 23:20:05,777 - INFO - training.closure - iteration 266: loss = -6.9110096002189785
2023-04-08 23:20:08,400 - INFO - training.closure - iteration 267: loss = -6.942882237078842
2023-04-08 23:20:11,056 - INFO - training.closure - iteration 268: loss = -6.962737046649898
2023-04-08 23:20:13,688 - INFO - training.closure - iteration 269: loss = -6.975489274971983
2023-04-08 23:20:16,390 - INFO - training.closure - iteration 270: loss = -6.973382718832075
2023-04-08 23:20:19,099 - INFO - training.closure - iteration 271: loss = -6.98049107114075
2023-04-08 23:20:21,935 - INFO - training.closure - iteration 272: loss = -6.986575096357631
2023-04-08 23:20:24,619 - INFO - training.closure - iteration 273: loss = -6.992459531710806
2023-04-08 23:20:27,241 - INFO - training.closure - iteration 274: loss = -6.995790321078531
2023-04-08 23:20:29,935 - INFO - training.closure - iteration 275: loss = -7.000807958644485
2023-04-08 23:20:32,653 - INFO - training.closure - iteration 276: loss = -7.015854564255753
2023-04-08 23:20:35,328 - INFO - training.closure - iteration 277: loss = -7.027698028789431
2023-04-08 23:20:37,982 - INFO - training.closure - iteration 278: loss = -7.048996323603783
2023-04-08 23:20:40,619 - INFO - training.closure - iteration 279: loss = -7.078084069433723
2023-04-08 23:20:43,266 - INFO - training.closure - iteration 280: loss = -6.6926319376268655
2023-04-08 23:20:46,008 - INFO - training.closure - iteration 281: loss = -7.086185586527609
2023-04-08 23:20:48,637 - INFO - training.closure - iteration 282: loss = -7.076810843947349
2023-04-08 23:20:51,323 - INFO - training.closure - iteration 283: loss = -7.09569009837705
2023-04-08 23:20:54,032 - INFO - training.closure - iteration 284: loss = -7.072559827992883
2023-04-08 23:20:56,684 - INFO - training.closure - iteration 285: loss = -7.101676657310316
2023-04-08 23:20:59,338 - INFO - training.closure - iteration 286: loss = -7.113506806021016
2023-04-08 23:21:02,008 - INFO - training.closure - iteration 287: loss = -7.1263993740551665
2023-04-08 23:21:04,654 - INFO - training.closure - iteration 288: loss = -7.138590605338986
2023-04-08 23:21:07,347 - INFO - training.closure - iteration 289: loss = -7.150303862696989
2023-04-08 23:21:09,982 - INFO - training.closure - iteration 290: loss = -7.1462421548369415
2023-04-08 23:21:12,622 - INFO - training.closure - iteration 291: loss = -7.16085092193066
2023-04-08 23:21:15,315 - INFO - training.closure - iteration 292: loss = -7.160596957983879
2023-04-08 23:21:17,938 - INFO - training.closure - iteration 293: loss = -7.175889515575562
2023-04-08 23:21:20,557 - INFO - training.closure - iteration 294: loss = -7.15782047797428
2023-04-08 23:21:23,280 - INFO - training.closure - iteration 295: loss = -7.179895376619623
2023-04-08 23:21:25,906 - INFO - training.closure - iteration 296: loss = -7.186279977990452
2023-04-08 23:21:28,525 - INFO - training.closure - iteration 297: loss = -7.191134793400927
2023-04-08 23:21:31,134 - INFO - training.closure - iteration 298: loss = -7.192939506395466
2023-04-08 23:21:33,794 - INFO - training.closure - iteration 299: loss = -7.172233205152562
2023-04-08 23:21:36,422 - INFO - training.closure - iteration 300: loss = -7.2021174075089895
2023-04-08 23:21:39,072 - INFO - training.closure - iteration 301: loss = -7.207025946105747
2023-04-08 23:21:41,775 - INFO - training.closure - iteration 302: loss = -7.211958062252215
2023-04-08 23:21:44,444 - INFO - training.closure - iteration 303: loss = -7.225662444717786
2023-04-08 23:21:47,116 - INFO - training.closure - iteration 304: loss = -7.250711790487006
2023-04-08 23:21:49,741 - INFO - training.closure - iteration 305: loss = -7.230012681419124
2023-04-08 23:21:52,387 - INFO - training.closure - iteration 306: loss = -7.268875078533011
2023-04-08 23:21:55,020 - INFO - training.closure - iteration 307: loss = -7.318182591191334
2023-04-08 23:21:57,736 - INFO - training.closure - iteration 308: loss = -7.366033122623831
2023-04-08 23:22:00,374 - INFO - training.closure - iteration 309: loss = -7.294878750634866
2023-04-08 23:22:02,985 - INFO - training.closure - iteration 310: loss = -7.372834563939428
2023-04-08 23:22:05,624 - INFO - training.closure - iteration 311: loss = -7.3820192791486505
2023-04-08 23:22:08,249 - INFO - training.closure - iteration 312: loss = -7.39022834906641
2023-04-08 23:22:10,889 - INFO - training.closure - iteration 313: loss = -7.401665897970413
2023-04-08 23:22:13,615 - INFO - training.closure - iteration 314: loss = -7.416798402930574
2023-04-08 23:22:16,250 - INFO - training.closure - iteration 315: loss = -7.436031665265125
2023-04-08 23:22:18,987 - INFO - training.closure - iteration 316: loss = -7.438226904827417
2023-04-08 23:22:21,616 - INFO - training.closure - iteration 317: loss = -7.44505197828207
2023-04-08 23:22:24,270 - INFO - training.closure - iteration 318: loss = -7.447077749433772
2023-04-08 23:22:26,906 - INFO - training.closure - iteration 319: loss = -7.448179382967399
2023-04-08 23:22:29,545 - INFO - training.closure - iteration 320: loss = -7.4493189296043365
2023-04-08 23:22:32,259 - INFO - training.closure - iteration 321: loss = -7.45190289906515
2023-04-08 23:22:34,902 - INFO - training.closure - iteration 322: loss = -7.458550063441409
2023-04-08 23:22:37,548 - INFO - training.closure - iteration 323: loss = -7.465897354383233
2023-04-08 23:22:40,162 - INFO - training.closure - iteration 324: loss = -7.470571728963787
2023-04-08 23:22:42,803 - INFO - training.closure - iteration 325: loss = -7.469323168426039
2023-04-08 23:22:45,499 - INFO - training.closure - iteration 326: loss = -7.473249901763156
2023-04-08 23:22:48,183 - INFO - training.closure - iteration 327: loss = -7.475058205430184
2023-04-08 23:22:50,831 - INFO - training.closure - iteration 328: loss = -7.477480135301047
2023-04-08 23:22:53,455 - INFO - training.closure - iteration 329: loss = -7.478899082556449
2023-04-08 23:22:56,074 - INFO - training.closure - iteration 330: loss = -7.479884725710501
2023-04-08 23:22:58,721 - INFO - training.closure - iteration 331: loss = -7.481909991489871
2023-04-08 23:23:01,350 - INFO - training.closure - iteration 332: loss = -7.483978176093837
2023-04-08 23:23:04,112 - INFO - training.closure - iteration 333: loss = -7.486237221730271
2023-04-08 23:23:06,724 - INFO - training.closure - iteration 334: loss = -7.487553758349409
2023-04-08 23:23:09,339 - INFO - training.closure - iteration 335: loss = -7.488721644917105
2023-04-08 23:23:11,996 - INFO - training.closure - iteration 336: loss = -7.489277038382564
2023-04-08 23:23:14,605 - INFO - training.closure - iteration 337: loss = -7.490203621204202
2023-04-08 23:23:17,254 - INFO - training.closure - iteration 338: loss = -7.491223477904164
2023-04-08 23:23:19,859 - INFO - training.closure - iteration 339: loss = -7.4923188557579365
2023-04-08 23:23:22,604 - INFO - training.closure - iteration 340: loss = -7.494331871120357
2023-04-08 23:23:25,253 - INFO - training.closure - iteration 341: loss = -7.495766910753794
2023-04-08 23:23:27,858 - INFO - training.closure - iteration 342: loss = -7.48997366782409
2023-04-08 23:23:30,473 - INFO - training.closure - iteration 343: loss = -7.498530209132303
2023-04-08 23:23:33,099 - INFO - training.closure - iteration 344: loss = -7.501532019451478
2023-04-08 23:23:35,706 - INFO - training.closure - iteration 345: loss = -7.506175283130209
2023-04-08 23:23:38,496 - INFO - training.closure - iteration 346: loss = -7.509486373958
2023-04-08 23:23:41,185 - INFO - training.closure - iteration 347: loss = -7.511101071121946
2023-04-08 23:23:43,813 - INFO - training.closure - iteration 348: loss = -7.511992734473418
2023-04-08 23:23:46,546 - INFO - training.closure - iteration 349: loss = -7.516375166117587
2023-04-08 23:23:49,191 - INFO - training.closure - iteration 350: loss = -7.519033806375565
2023-04-08 23:23:51,850 - INFO - training.closure - iteration 351: loss = -7.519283423936823
2023-04-08 23:23:54,582 - INFO - training.closure - iteration 352: loss = -7.520349106896059
2023-04-08 23:23:57,222 - INFO - training.closure - iteration 353: loss = -7.520961501598535
2023-04-08 23:23:59,863 - INFO - training.closure - iteration 354: loss = -7.521459292985709
2023-04-08 23:24:02,507 - INFO - training.closure - iteration 355: loss = -7.522268461911586
2023-04-08 23:24:05,133 - INFO - training.closure - iteration 356: loss = -7.521677407967879
2023-04-08 23:24:07,744 - INFO - training.closure - iteration 357: loss = -7.522716420646103
2023-04-08 23:24:10,358 - INFO - training.closure - iteration 358: loss = -7.523062637168881
2023-04-08 23:24:13,061 - INFO - training.closure - iteration 359: loss = -7.523392719939183
2023-04-08 23:24:15,679 - INFO - training.closure - iteration 360: loss = -7.5237776068096665
2023-04-08 23:24:18,313 - INFO - training.closure - iteration 361: loss = -7.524118452072203
2023-04-08 23:24:20,944 - INFO - training.closure - iteration 362: loss = -7.524609547116999
2023-04-08 23:24:23,566 - INFO - training.closure - iteration 363: loss = -7.525149734202855
2023-04-08 23:24:26,200 - INFO - training.closure - iteration 364: loss = -7.525601385639772
2023-04-08 23:24:28,949 - INFO - training.closure - iteration 365: loss = -7.525970186095268
2023-04-08 23:24:31,591 - INFO - training.closure - iteration 366: loss = -7.52714888414669
2023-04-08 23:24:34,220 - INFO - training.closure - iteration 367: loss = -7.528549500719093
2023-04-08 23:24:36,852 - INFO - training.closure - iteration 368: loss = -7.5298371894899345
2023-04-08 23:24:39,461 - INFO - training.closure - iteration 369: loss = -7.532065657244899
2023-04-08 23:24:42,078 - INFO - training.closure - iteration 370: loss = -7.533327169804986
2023-04-08 23:24:44,805 - INFO - training.closure - iteration 371: loss = -7.5367514349711575
2023-04-08 23:24:47,502 - INFO - training.closure - iteration 372: loss = -7.538568550428004
2023-04-08 23:24:50,112 - INFO - training.closure - iteration 373: loss = -7.535275942722068
2023-04-08 23:24:52,726 - INFO - training.closure - iteration 374: loss = -7.540642731521022
2023-04-08 23:24:55,344 - INFO - training.closure - iteration 375: loss = -7.543670552801151
2023-04-08 23:24:57,955 - INFO - training.closure - iteration 376: loss = -7.545977526771322
2023-04-08 23:25:00,570 - INFO - training.closure - iteration 377: loss = -7.547293133884794
2023-04-08 23:25:03,254 - INFO - training.closure - iteration 378: loss = -7.548020951283776
2023-04-08 23:25:06,004 - INFO - training.closure - iteration 379: loss = -7.551292657169101
2023-04-08 23:25:08,627 - INFO - training.closure - iteration 380: loss = -7.551837310187072
2023-04-08 23:25:11,255 - INFO - training.closure - iteration 381: loss = -7.553989688029214
2023-04-08 23:25:13,931 - INFO - training.closure - iteration 382: loss = -7.558396075935677
2023-04-08 23:25:16,573 - INFO - training.closure - iteration 383: loss = -7.555988933442603
2023-04-08 23:25:19,295 - INFO - training.closure - iteration 384: loss = -7.561854839436021
2023-04-08 23:25:21,936 - INFO - training.closure - iteration 385: loss = -7.564563791498291
2023-04-08 23:25:24,554 - INFO - training.closure - iteration 386: loss = -7.566410789094452
2023-04-08 23:25:27,161 - INFO - training.closure - iteration 387: loss = -7.568152016128514
2023-04-08 23:25:29,795 - INFO - training.closure - iteration 388: loss = -7.5689574808448326
2023-04-08 23:25:32,459 - INFO - training.closure - iteration 389: loss = -7.570223972559351
2023-04-08 23:25:35,171 - INFO - training.closure - iteration 390: loss = -7.571100250188461
2023-04-08 23:25:37,880 - INFO - training.closure - iteration 391: loss = -7.573732637656269
2023-04-08 23:25:40,511 - INFO - training.closure - iteration 392: loss = -7.57519844116603
2023-04-08 23:25:43,132 - INFO - training.closure - iteration 393: loss = -7.575792513815455
2023-04-08 23:25:45,756 - INFO - training.closure - iteration 394: loss = -7.576939855277212
2023-04-08 23:25:48,372 - INFO - training.closure - iteration 395: loss = -7.577534416112309
2023-04-08 23:25:50,982 - INFO - training.closure - iteration 396: loss = -7.578203260977759
2023-04-08 23:25:53,749 - INFO - training.closure - iteration 397: loss = -7.522043168471171
2023-04-08 23:25:56,393 - INFO - training.closure - iteration 398: loss = -7.5784813881526745
2023-04-08 23:25:59,040 - INFO - training.closure - iteration 399: loss = -7.579406157932592
2023-04-08 23:26:01,771 - INFO - training.closure - iteration 400: loss = -7.580147613003247
2023-04-08 23:26:04,417 - INFO - training.closure - iteration 401: loss = -7.580445001252164
2023-04-08 23:26:07,070 - INFO - training.closure - iteration 402: loss = -7.580733162952275
2023-04-08 23:26:09,781 - INFO - training.closure - iteration 403: loss = -7.580900149832354
2023-04-08 23:26:12,425 - INFO - training.closure - iteration 404: loss = -7.5811834725841205
2023-04-08 23:26:15,073 - INFO - training.closure - iteration 405: loss = -7.581466620099914
2023-04-08 23:26:17,694 - INFO - training.closure - iteration 406: loss = -7.581689024104161
2023-04-08 23:26:20,346 - INFO - training.closure - iteration 407: loss = -7.582717661668446
2023-04-08 23:26:22,994 - INFO - training.closure - iteration 408: loss = -7.585147256479865
2023-04-08 23:26:25,724 - INFO - training.closure - iteration 409: loss = -7.588639250518954
2023-04-08 23:26:28,370 - INFO - training.closure - iteration 410: loss = -7.596233568888642
2023-04-08 23:26:31,037 - INFO - training.closure - iteration 411: loss = -7.4303808928384445
2023-04-08 23:26:33,682 - INFO - training.closure - iteration 412: loss = -7.597962825551406
2023-04-08 23:26:36,340 - INFO - training.closure - iteration 413: loss = -7.603901390614501
2023-04-08 23:26:38,960 - INFO - training.closure - iteration 414: loss = -7.6082595327137295
2023-04-08 23:26:41,594 - INFO - training.closure - iteration 415: loss = -7.60161016090581
2023-04-08 23:26:44,373 - INFO - training.closure - iteration 416: loss = -7.61157465531904
2023-04-08 23:26:47,072 - INFO - training.closure - iteration 417: loss = -7.614528872562548
2023-04-08 23:26:49,697 - INFO - training.closure - iteration 418: loss = -7.616709320688917
2023-04-08 23:26:52,334 - INFO - training.closure - iteration 419: loss = -7.598167840904969
2023-04-08 23:26:54,983 - INFO - training.closure - iteration 420: loss = -7.61888655538784
2023-04-08 23:26:57,626 - INFO - training.closure - iteration 421: loss = -7.623678192298914
2023-04-08 23:27:00,339 - INFO - training.closure - iteration 422: loss = -7.637762204956856
2023-04-08 23:27:02,979 - INFO - training.closure - iteration 423: loss = -5.569098812302862
2023-04-08 23:27:05,630 - INFO - training.closure - iteration 424: loss = -7.6173612412072575
2023-04-08 23:27:08,272 - INFO - training.closure - iteration 425: loss = -7.651581200261391
2023-04-08 23:27:10,885 - INFO - training.closure - iteration 426: loss = -7.6604239001262435
2023-04-08 23:27:13,527 - INFO - training.closure - iteration 427: loss = -7.6612409328738575
2023-04-08 23:27:16,250 - INFO - training.closure - iteration 428: loss = -7.669877557073406
2023-04-08 23:27:18,942 - INFO - training.closure - iteration 429: loss = -7.679292808899715
2023-04-08 23:27:21,558 - INFO - training.closure - iteration 430: loss = -7.6856061099914825
2023-04-08 23:27:24,199 - INFO - training.closure - iteration 431: loss = -7.649438239236577
2023-04-08 23:27:26,854 - INFO - training.closure - iteration 432: loss = -7.686947318886532
2023-04-08 23:27:29,474 - INFO - training.closure - iteration 433: loss = -7.690358713903168
2023-04-08 23:27:32,085 - INFO - training.closure - iteration 434: loss = -7.692187614165121
2023-04-08 23:27:34,768 - INFO - training.closure - iteration 435: loss = -7.69524164662263
2023-04-08 23:27:37,420 - INFO - training.closure - iteration 436: loss = -7.701859355300611
2023-04-08 23:27:40,033 - INFO - training.closure - iteration 437: loss = -7.705650407445141
2023-04-08 23:27:42,661 - INFO - training.closure - iteration 438: loss = -7.709332549107362
2023-04-08 23:27:45,294 - INFO - training.closure - iteration 439: loss = -7.7108619143085
2023-04-08 23:27:47,921 - INFO - training.closure - iteration 440: loss = -7.712547254700293
2023-04-08 23:27:50,605 - INFO - training.closure - iteration 441: loss = -7.715814559468011
2023-04-08 23:27:53,288 - INFO - training.closure - iteration 442: loss = -7.717527427126024
2023-04-08 23:27:55,933 - INFO - training.closure - iteration 443: loss = -7.719451692787834
2023-04-08 23:27:58,656 - INFO - training.closure - iteration 444: loss = -7.7197928204251065
2023-04-08 23:28:01,281 - INFO - training.closure - iteration 445: loss = -7.721206263664156
2023-04-08 23:28:03,956 - INFO - training.closure - iteration 446: loss = -7.721499795191475
2023-04-08 23:28:06,668 - INFO - training.closure - iteration 447: loss = -7.721995339259654
2023-04-08 23:28:09,318 - INFO - training.closure - iteration 448: loss = -7.72221738431967
2023-04-08 23:28:11,958 - INFO - training.closure - iteration 449: loss = -7.72252385592283
2023-04-08 23:28:14,604 - INFO - training.closure - iteration 450: loss = -7.722990288327991
2023-04-08 23:28:17,242 - INFO - training.closure - iteration 451: loss = -7.724399308859523
2023-04-08 23:28:19,903 - INFO - training.closure - iteration 452: loss = -7.725053796651403
2023-04-08 23:28:22,565 - INFO - training.closure - iteration 453: loss = -7.725382689637017
2023-04-08 23:28:25,258 - INFO - training.closure - iteration 454: loss = -7.725595431131402
2023-04-08 23:28:27,894 - INFO - training.closure - iteration 455: loss = -7.726112801453166
2023-04-08 23:28:30,529 - INFO - training.closure - iteration 456: loss = -7.727511737514457
2023-04-08 23:28:33,142 - INFO - training.closure - iteration 457: loss = -7.728768419846479
2023-04-08 23:28:35,767 - INFO - training.closure - iteration 458: loss = -7.729615230882298
2023-04-08 23:28:38,393 - INFO - training.closure - iteration 459: loss = -7.730800822236668
2023-04-08 23:28:41,120 - INFO - training.closure - iteration 460: loss = -7.731871410038858
2023-04-08 23:28:43,763 - INFO - training.closure - iteration 461: loss = -7.733743236540698
2023-04-08 23:28:46,410 - INFO - training.closure - iteration 462: loss = -7.7355531519414615
2023-04-08 23:28:49,053 - INFO - training.closure - iteration 463: loss = -7.722627258448924
2023-04-08 23:28:51,669 - INFO - training.closure - iteration 464: loss = -7.737546811141133
2023-04-08 23:28:54,353 - INFO - training.closure - iteration 465: loss = -7.739265868241106
2023-04-08 23:28:57,075 - INFO - training.closure - iteration 466: loss = -7.74020263472727
2023-04-08 23:28:59,718 - INFO - training.closure - iteration 467: loss = -7.740455479035388
2023-04-08 23:29:02,847 - INFO - training.closure - iteration 468: loss = -7.740765268593066
2023-04-08 23:29:05,467 - INFO - training.closure - iteration 469: loss = -7.741062581921602
2023-04-08 23:29:08,110 - INFO - training.closure - iteration 470: loss = -7.741496064499473
2023-04-08 23:29:10,725 - INFO - training.closure - iteration 471: loss = -7.742213786913079
2023-04-08 23:29:13,372 - INFO - training.closure - iteration 472: loss = -7.743027475558
2023-04-08 23:29:16,064 - INFO - training.closure - iteration 473: loss = -7.743985227013541
2023-04-08 23:29:18,694 - INFO - training.closure - iteration 474: loss = -7.7433564079617385
2023-04-08 23:29:21,307 - INFO - training.closure - iteration 475: loss = -7.744476135077388
2023-04-08 23:29:23,950 - INFO - training.closure - iteration 476: loss = -7.745270614319421
2023-04-08 23:29:26,580 - INFO - training.closure - iteration 477: loss = -7.74579081717194
2023-04-08 23:29:29,196 - INFO - training.closure - iteration 478: loss = -7.74680305030842
2023-04-08 23:29:31,896 - INFO - training.closure - iteration 479: loss = -7.7477907765033045
2023-04-08 23:29:34,530 - INFO - training.closure - iteration 480: loss = -7.7487242266387355
2023-04-08 23:29:37,172 - INFO - training.closure - iteration 481: loss = -7.749881380513265
2023-04-08 23:29:39,821 - INFO - training.closure - iteration 482: loss = -7.751067503238377
2023-04-08 23:29:42,442 - INFO - training.closure - iteration 483: loss = -7.752312881670313
2023-04-08 23:29:45,217 - INFO - training.closure - iteration 484: loss = -7.752806584341911
2023-04-08 23:29:47,906 - INFO - training.closure - iteration 485: loss = -7.753564695175125
2023-04-08 23:29:50,530 - INFO - training.closure - iteration 486: loss = -7.755533181365998
2023-04-08 23:29:53,171 - INFO - training.closure - iteration 487: loss = -7.756030976701036
2023-04-08 23:29:55,809 - INFO - training.closure - iteration 488: loss = -7.757002235274102
2023-04-08 23:29:58,449 - INFO - training.closure - iteration 489: loss = -7.757123812579122
2023-04-08 23:30:01,095 - INFO - training.closure - iteration 490: loss = -7.757530154319248
2023-04-08 23:30:03,744 - INFO - training.closure - iteration 491: loss = -7.757712962796204
2023-04-08 23:30:06,458 - INFO - training.closure - iteration 492: loss = -7.7579459477957595
2023-04-08 23:30:09,087 - INFO - training.closure - iteration 493: loss = -7.758183402653481
2023-04-08 23:30:11,780 - INFO - training.closure - iteration 494: loss = -7.758411198343398
2023-04-08 23:30:14,404 - INFO - training.closure - iteration 495: loss = -7.758452340574658
2023-04-08 23:30:17,082 - INFO - training.closure - iteration 496: loss = -7.758491091558294
2023-04-08 23:30:19,740 - INFO - training.closure - iteration 497: loss = -7.7585135202248185
2023-04-08 23:30:22,442 - INFO - training.closure - iteration 498: loss = -7.758553613604121
2023-04-08 23:30:25,154 - INFO - training.closure - iteration 499: loss = -7.7585906365904656
2023-04-08 23:30:27,761 - INFO - training.closure - iteration 500: loss = -7.758643504907628
2023-04-08 23:30:30,392 - INFO - training.closure - iteration 501: loss = -7.758743177902644
2023-04-08 23:30:33,022 - INFO - training.closure - iteration 502: loss = -7.758984212318257
2023-04-08 23:30:35,716 - INFO - training.closure - iteration 503: loss = -7.759474419494238
2023-04-08 23:30:38,418 - INFO - training.closure - iteration 504: loss = -7.760547173421745
2023-04-08 23:30:41,048 - INFO - training.closure - iteration 505: loss = -7.742701524698099
2023-04-08 23:30:43,682 - INFO - training.closure - iteration 506: loss = -7.761074041879915
2023-04-08 23:30:46,417 - INFO - training.closure - iteration 507: loss = -7.762158595151933
2023-04-08 23:30:49,079 - INFO - training.closure - iteration 508: loss = -7.763642376390811
2023-04-08 23:30:51,717 - INFO - training.closure - iteration 509: loss = -7.76400908411007
2023-04-08 23:30:54,354 - INFO - training.closure - iteration 510: loss = -7.761953783697237
2023-04-08 23:30:57,067 - INFO - training.closure - iteration 511: loss = -7.7644585621906295
2023-04-08 23:30:59,702 - INFO - training.closure - iteration 512: loss = -7.765735446089206
2023-04-08 23:31:02,387 - INFO - training.closure - iteration 513: loss = -7.766930360544037
2023-04-08 23:31:05,007 - INFO - training.closure - iteration 514: loss = -7.768842891846713
2023-04-08 23:31:07,636 - INFO - training.closure - iteration 515: loss = -7.770275723699694
2023-04-08 23:31:10,259 - INFO - training.closure - iteration 516: loss = -7.772422636743451
2023-04-08 23:31:12,955 - INFO - training.closure - iteration 517: loss = -7.773270573764123
2023-04-08 23:31:15,603 - INFO - training.closure - iteration 518: loss = -7.773761516608112
2023-04-08 23:31:18,268 - INFO - training.closure - iteration 519: loss = -7.774197588901913
2023-04-08 23:31:20,885 - INFO - training.closure - iteration 520: loss = -7.771024162493214
2023-04-08 23:31:23,529 - INFO - training.closure - iteration 521: loss = -7.774444239663968
2023-04-08 23:31:26,157 - INFO - training.closure - iteration 522: loss = -7.774958660256154
2023-04-08 23:31:28,949 - INFO - training.closure - iteration 523: loss = -7.775127566239437
2023-04-08 23:31:31,878 - INFO - training.closure - iteration 524: loss = -7.775397541589969
2023-04-08 23:31:34,571 - INFO - training.closure - iteration 525: loss = -7.7756007361167
2023-04-08 23:31:37,198 - INFO - training.closure - iteration 526: loss = -7.775910071725115
2023-04-08 23:31:39,851 - INFO - training.closure - iteration 527: loss = -7.7764297964903975
2023-04-08 23:31:42,499 - INFO - training.closure - iteration 528: loss = -7.776975270541336
2023-04-08 23:31:45,138 - INFO - training.closure - iteration 529: loss = -7.772733252282688
2023-04-08 23:31:47,832 - INFO - training.closure - iteration 530: loss = -7.777230059494784
2023-04-08 23:31:50,461 - INFO - training.closure - iteration 531: loss = -7.7779182104572495
2023-04-08 23:31:53,075 - INFO - training.closure - iteration 532: loss = -7.778811069015565
2023-04-08 23:31:55,718 - INFO - training.closure - iteration 533: loss = -7.780751981119704
2023-04-08 23:31:58,359 - INFO - training.closure - iteration 534: loss = -7.782742630532768
2023-04-08 23:32:00,994 - INFO - training.closure - iteration 535: loss = -7.480433264184421
2023-04-08 23:32:03,704 - INFO - training.closure - iteration 536: loss = -7.782844210019627
2023-04-08 23:32:06,326 - INFO - training.closure - iteration 537: loss = -7.784113780310012
2023-04-08 23:32:08,972 - INFO - training.closure - iteration 538: loss = -7.784811676575919
2023-04-08 23:32:11,619 - INFO - training.closure - iteration 539: loss = -7.785101874058
2023-04-08 23:32:14,266 - INFO - training.closure - iteration 540: loss = -7.785461883469595
2023-04-08 23:32:16,910 - INFO - training.closure - iteration 541: loss = -7.785589054280508
2023-04-08 23:32:19,596 - INFO - training.closure - iteration 542: loss = -7.785727487833478
2023-04-08 23:32:22,233 - INFO - training.closure - iteration 543: loss = -7.785807121133505
2023-04-08 23:32:24,852 - INFO - training.closure - iteration 544: loss = -7.78471154051394
2023-04-08 23:32:27,459 - INFO - training.closure - iteration 545: loss = -7.785903207185977
2023-04-08 23:32:30,088 - INFO - training.closure - iteration 546: loss = -7.785974716698325
2023-04-08 23:32:32,757 - INFO - training.closure - iteration 547: loss = -7.786210372116833
2023-04-08 23:32:35,398 - INFO - training.closure - iteration 548: loss = -7.786418936738371
2023-04-08 23:32:38,097 - INFO - training.closure - iteration 549: loss = -7.786554370771493
2023-04-08 23:32:40,728 - INFO - training.closure - iteration 550: loss = -7.78690360791763
2023-04-08 23:32:43,364 - INFO - training.closure - iteration 551: loss = -7.7872488353467535
2023-04-08 23:32:45,971 - INFO - training.closure - iteration 552: loss = -7.787444948731304
2023-04-08 23:32:48,578 - INFO - training.closure - iteration 553: loss = -7.787625304511805
2023-04-08 23:32:51,184 - INFO - training.closure - iteration 554: loss = -7.787837023369885
2023-04-08 23:32:53,875 - INFO - training.closure - iteration 555: loss = -7.7882913971030625
2023-04-08 23:32:56,481 - INFO - training.closure - iteration 556: loss = -7.788845389066024
2023-04-08 23:32:59,086 - INFO - training.closure - iteration 557: loss = -7.7894048548880335
2023-04-08 23:33:01,698 - INFO - training.closure - iteration 558: loss = -7.789449874863859
2023-04-08 23:33:04,303 - INFO - training.closure - iteration 559: loss = -7.789871528932434
2023-04-08 23:33:06,935 - INFO - training.closure - iteration 560: loss = -7.7899674408472395
2023-04-08 23:33:09,644 - INFO - training.closure - iteration 561: loss = -7.7900542707883655
2023-04-08 23:33:12,340 - INFO - training.closure - iteration 562: loss = -7.789805775580932
2023-04-08 23:33:14,985 - INFO - training.closure - iteration 563: loss = -7.790082758209374
2023-04-08 23:33:17,651 - INFO - training.closure - iteration 564: loss = -7.790186064002463
2023-04-08 23:33:20,289 - INFO - training.closure - iteration 565: loss = -7.790245529391928
2023-04-08 23:33:22,926 - INFO - training.closure - iteration 566: loss = -7.790262196366839
2023-04-08 23:33:25,574 - INFO - training.closure - iteration 567: loss = -7.790283442304602
2023-04-08 23:33:28,330 - INFO - training.closure - iteration 568: loss = -7.7903067687543714
2023-04-08 23:33:30,978 - INFO - training.closure - iteration 569: loss = -7.79036111631431
2023-04-08 23:33:33,621 - INFO - training.closure - iteration 570: loss = -7.790445659548089
2023-04-08 23:33:36,323 - INFO - training.closure - iteration 571: loss = -7.790681442481407
2023-04-08 23:33:38,962 - INFO - training.closure - iteration 572: loss = -7.790982567599237
2023-04-08 23:33:41,591 - INFO - training.closure - iteration 573: loss = -7.790739327606664
2023-04-08 23:33:44,294 - INFO - training.closure - iteration 574: loss = -7.791142425581178
2023-04-08 23:33:46,922 - INFO - training.closure - iteration 575: loss = -7.791391305982062
2023-04-08 23:33:49,559 - INFO - training.closure - iteration 576: loss = -7.791528514779701
2023-04-08 23:33:52,166 - INFO - training.closure - iteration 577: loss = -7.791612063063927
2023-04-08 23:33:54,811 - INFO - training.closure - iteration 578: loss = -7.791746178016075
2023-04-08 23:33:57,437 - INFO - training.closure - iteration 579: loss = -7.789272553827758
2023-04-08 23:34:00,159 - INFO - training.closure - iteration 580: loss = -7.791802172688578
2023-04-08 23:34:02,771 - INFO - training.closure - iteration 581: loss = -7.791886009047854
2023-04-08 23:34:05,398 - INFO - training.closure - iteration 582: loss = -7.7919357714067115
2023-04-08 23:34:08,030 - INFO - training.closure - iteration 583: loss = -7.791956875062502
2023-04-08 23:34:10,769 - INFO - training.closure - iteration 584: loss = -7.791979337991414
2023-04-08 23:34:13,400 - INFO - training.closure - iteration 585: loss = -7.792036283768756
2023-04-08 23:34:16,082 - INFO - training.closure - iteration 586: loss = -7.792105937862122
2023-04-08 23:34:18,778 - INFO - training.closure - iteration 587: loss = -7.7922141037031984
2023-04-08 23:34:21,386 - INFO - training.closure - iteration 588: loss = -7.792318294056297
2023-04-08 23:34:24,029 - INFO - training.closure - iteration 589: loss = -7.79235433035776
2023-04-08 23:34:26,669 - INFO - training.closure - iteration 590: loss = -7.792383164536211
2023-04-08 23:34:29,304 - INFO - training.closure - iteration 591: loss = -7.792476891467928
2023-04-08 23:34:31,925 - INFO - training.closure - iteration 592: loss = -7.792585188796003
2023-04-08 23:34:34,635 - INFO - training.closure - iteration 593: loss = -7.7925794219735165
2023-04-08 23:34:37,284 - INFO - training.closure - iteration 594: loss = -7.792699784506501
2023-04-08 23:34:39,940 - INFO - training.closure - iteration 595: loss = -7.7928650349189335
2023-04-08 23:34:42,581 - INFO - training.closure - iteration 596: loss = -7.79302024757221
2023-04-08 23:34:45,217 - INFO - training.closure - iteration 597: loss = -7.793016335783751
2023-04-08 23:34:47,858 - INFO - training.closure - iteration 598: loss = -7.793097007077735
2023-04-08 23:34:50,607 - INFO - training.closure - iteration 599: loss = -7.7931758224507535
2023-04-08 23:34:53,265 - INFO - training.closure - iteration 600: loss = -7.79321961122992
2023-04-08 23:34:55,969 - INFO - training.closure - iteration 601: loss = -7.793422406216417
2023-04-08 23:34:58,586 - INFO - training.closure - iteration 602: loss = -7.793772110195159
2023-04-08 23:35:01,208 - INFO - training.closure - iteration 603: loss = -7.793929758280359
2023-04-08 23:35:03,848 - INFO - training.closure - iteration 604: loss = -7.794383813446165
2023-04-08 23:35:06,503 - INFO - training.closure - iteration 605: loss = -7.794597689644656
2023-04-08 23:35:09,237 - INFO - training.closure - iteration 606: loss = -7.794712467186946
2023-04-08 23:35:11,948 - INFO - training.closure - iteration 607: loss = -7.794762288304545
2023-04-08 23:35:14,565 - INFO - training.closure - iteration 608: loss = -7.794800984308427
2023-04-08 23:35:17,208 - INFO - training.closure - iteration 609: loss = -7.794888643103603
2023-04-08 23:35:19,849 - INFO - training.closure - iteration 610: loss = -7.794577400230997
2023-04-08 23:35:22,525 - INFO - training.closure - iteration 611: loss = -7.794907198754778
2023-04-08 23:35:25,294 - INFO - training.closure - iteration 612: loss = -7.794976318835312
2023-04-08 23:35:27,936 - INFO - training.closure - iteration 613: loss = -7.795051150992542
2023-04-08 23:35:30,594 - INFO - training.closure - iteration 614: loss = -7.795078487455512
2023-04-08 23:35:33,241 - INFO - training.closure - iteration 615: loss = -7.795093688158298
2023-04-08 23:35:35,900 - INFO - training.closure - iteration 616: loss = -7.795110947619
2023-04-08 23:35:38,510 - INFO - training.closure - iteration 617: loss = -7.795130381958246
2023-04-08 23:35:41,249 - INFO - training.closure - iteration 618: loss = -7.795114184643882
2023-04-08 23:35:43,899 - INFO - training.closure - iteration 619: loss = -7.795141941732516
2023-04-08 23:35:46,545 - INFO - training.closure - iteration 620: loss = -7.795163232547071
2023-04-08 23:35:49,160 - INFO - training.closure - iteration 621: loss = -7.795198636908819
2023-04-08 23:35:51,774 - INFO - training.closure - iteration 622: loss = -7.795186329014308
2023-04-08 23:35:54,396 - INFO - training.closure - iteration 623: loss = -7.795211841176526
2023-04-08 23:35:57,043 - INFO - training.closure - iteration 624: loss = -7.795239810187991
2023-04-08 23:35:59,730 - INFO - training.closure - iteration 625: loss = -7.795260004592485
2023-04-08 23:36:02,344 - INFO - training.closure - iteration 626: loss = -7.7952760544876565
2023-04-08 23:36:05,018 - INFO - training.closure - iteration 627: loss = -7.795294038096818
2023-04-08 23:36:07,659 - INFO - training.closure - iteration 628: loss = -7.7953339743898535
2023-04-08 23:36:10,310 - INFO - training.closure - iteration 629: loss = -7.7954085866800025
2023-04-08 23:36:12,929 - INFO - training.closure - iteration 630: loss = -7.792782990031485
2023-04-08 23:36:15,826 - INFO - training.closure - iteration 631: loss = -7.795431981971247
2023-04-08 23:36:18,470 - INFO - training.closure - iteration 632: loss = -7.795508913935378
2023-04-08 23:36:21,108 - INFO - training.closure - iteration 633: loss = -7.79552494521364
2023-04-08 23:36:23,775 - INFO - training.closure - iteration 634: loss = -7.795636860059885
2023-04-08 23:36:26,444 - INFO - training.closure - iteration 635: loss = -7.795667280976198
2023-04-08 23:36:29,088 - INFO - training.closure - iteration 636: loss = -7.7956874291300915
2023-04-08 23:36:31,787 - INFO - training.closure - iteration 637: loss = -7.795703472219418
2023-04-08 23:36:34,403 - INFO - training.closure - iteration 638: loss = -7.79571107440934
2023-04-08 23:36:37,010 - INFO - training.closure - iteration 639: loss = -7.795721835978548
2023-04-08 23:36:39,650 - INFO - training.closure - iteration 640: loss = -7.795742328892638
2023-04-08 23:36:42,259 - INFO - training.closure - iteration 641: loss = -7.795783147223393
2023-04-08 23:36:44,873 - INFO - training.closure - iteration 642: loss = -7.7958433702715055
2023-04-08 23:36:47,491 - INFO - training.closure - iteration 643: loss = -7.795814218690688
2023-04-08 23:36:50,261 - INFO - training.closure - iteration 644: loss = -7.795889827758547
2023-04-08 23:36:52,939 - INFO - training.closure - iteration 645: loss = -7.795964079411457
2023-04-08 23:36:55,591 - INFO - training.closure - iteration 646: loss = -7.795994996125744
2023-04-08 23:36:58,198 - INFO - training.closure - iteration 647: loss = -7.796010836893762
2023-04-08 23:37:00,828 - INFO - training.closure - iteration 648: loss = -7.796029040043583
2023-04-08 23:37:03,458 - INFO - training.closure - iteration 649: loss = -7.796050680845747
2023-04-08 23:37:06,177 - INFO - training.closure - iteration 650: loss = -7.796092017706724
2023-04-08 23:37:08,815 - INFO - training.closure - iteration 651: loss = -7.796124140074035
2023-04-08 23:37:11,449 - INFO - training.closure - iteration 652: loss = -7.7961647225982365
2023-04-08 23:37:14,090 - INFO - training.closure - iteration 653: loss = -7.79620558676353
2023-04-08 23:37:16,737 - INFO - training.closure - iteration 654: loss = -7.7962538390536364
2023-04-08 23:37:19,356 - INFO - training.closure - iteration 655: loss = -7.796291208434346
2023-04-08 23:37:22,042 - INFO - training.closure - iteration 656: loss = -7.796354042092034
2023-04-08 23:37:24,695 - INFO - training.closure - iteration 657: loss = -7.796399725557366
2023-04-08 23:37:27,349 - INFO - training.closure - iteration 658: loss = -7.796494595733675
2023-04-08 23:37:29,980 - INFO - training.closure - iteration 659: loss = -7.796616122667865
2023-04-08 23:37:32,586 - INFO - training.closure - iteration 660: loss = -7.7967672051702195
2023-04-08 23:37:35,220 - INFO - training.closure - iteration 661: loss = -7.796914976804192
2023-04-08 23:37:37,824 - INFO - training.closure - iteration 662: loss = -7.7970365370266475
2023-04-08 23:37:40,542 - INFO - training.closure - iteration 663: loss = -7.7971006279670805
2023-04-08 23:37:43,161 - INFO - training.closure - iteration 664: loss = -7.797138819514502
2023-04-08 23:37:45,769 - INFO - training.closure - iteration 665: loss = -7.79718110020481
2023-04-08 23:37:48,373 - INFO - training.closure - iteration 666: loss = -7.797231648250021
2023-04-08 23:37:51,047 - INFO - training.closure - iteration 667: loss = -7.79725487759212
2023-04-08 23:37:53,655 - INFO - training.closure - iteration 668: loss = -7.797290747276253
2023-04-08 23:37:56,381 - INFO - training.closure - iteration 669: loss = -7.797334006661224
2023-04-08 23:37:59,026 - INFO - training.closure - iteration 670: loss = -7.7974802884552
2023-04-08 23:38:01,662 - INFO - training.closure - iteration 671: loss = -7.797629767469283
2023-04-08 23:38:04,312 - INFO - training.closure - iteration 672: loss = -7.797937528895519
2023-04-08 23:38:06,959 - INFO - training.closure - iteration 673: loss = -7.798193935696139
2023-04-08 23:38:09,605 - INFO - training.closure - iteration 674: loss = -7.798596004312731
2023-04-08 23:38:12,331 - INFO - training.closure - iteration 675: loss = -7.795692110957833
2023-04-08 23:38:14,960 - INFO - training.closure - iteration 676: loss = -7.798749750739433
2023-04-08 23:38:17,577 - INFO - training.closure - iteration 677: loss = -7.798975099864961
2023-04-08 23:38:20,187 - INFO - training.closure - iteration 678: loss = -7.799329466452404
2023-04-08 23:38:22,795 - INFO - training.closure - iteration 679: loss = -7.7997464867413875
2023-04-08 23:38:25,414 - INFO - training.closure - iteration 680: loss = -7.797955486772573
2023-04-08 23:38:28,070 - INFO - training.closure - iteration 681: loss = -7.799834088459019
2023-04-08 23:38:30,772 - INFO - training.closure - iteration 682: loss = -7.800082186208568
2023-04-08 23:38:33,414 - INFO - training.closure - iteration 683: loss = -7.800264197991082
2023-04-08 23:38:36,055 - INFO - training.closure - iteration 684: loss = -7.800401553256452
2023-04-08 23:38:38,692 - INFO - training.closure - iteration 685: loss = -7.8005949493858235
2023-04-08 23:38:41,336 - INFO - training.closure - iteration 686: loss = -7.800910320538794
2023-04-08 23:38:43,996 - INFO - training.closure - iteration 687: loss = -7.79918298922029
2023-04-08 23:38:46,700 - INFO - training.closure - iteration 688: loss = -7.801262476821982
2023-04-08 23:38:49,343 - INFO - training.closure - iteration 689: loss = -7.802088952050049
2023-04-08 23:38:51,957 - INFO - training.closure - iteration 690: loss = -7.802483004159024
2023-04-08 23:38:54,572 - INFO - training.closure - iteration 691: loss = -7.802571515162746
2023-04-08 23:38:57,179 - INFO - training.closure - iteration 692: loss = -7.802707760433629
2023-04-08 23:38:59,801 - INFO - training.closure - iteration 693: loss = -7.802773055144083
2023-04-08 23:39:02,542 - INFO - training.closure - iteration 694: loss = -7.802863921345919
2023-04-08 23:39:05,227 - INFO - training.closure - iteration 695: loss = -7.803011506028391
2023-04-08 23:39:07,917 - INFO - training.closure - iteration 696: loss = -7.803024191280148
2023-04-08 23:39:10,576 - INFO - training.closure - iteration 697: loss = -7.803154877510454
2023-04-08 23:39:13,195 - INFO - training.closure - iteration 698: loss = -7.803253281180284
2023-04-08 23:39:15,839 - INFO - training.closure - iteration 699: loss = -7.803311898632711
2023-04-08 23:39:18,470 - INFO - training.closure - iteration 700: loss = -7.803422047422841
2023-04-08 23:39:21,167 - INFO - training.closure - iteration 701: loss = -7.803473776933084
2023-04-08 23:39:23,776 - INFO - training.closure - iteration 702: loss = -7.803518575135994
2023-04-08 23:39:26,439 - INFO - training.closure - iteration 703: loss = -7.80349639210572
2023-04-08 23:39:29,057 - INFO - training.closure - iteration 704: loss = -7.803577527967853
2023-04-08 23:39:31,699 - INFO - training.closure - iteration 705: loss = -7.801101134458978
2023-04-08 23:39:34,334 - INFO - training.closure - iteration 706: loss = -7.803652603042982
2023-04-08 23:39:37,055 - INFO - training.closure - iteration 707: loss = -7.803722056488166
2023-04-08 23:39:39,665 - INFO - training.closure - iteration 708: loss = -7.803961516091885
2023-04-08 23:39:42,283 - INFO - training.closure - iteration 709: loss = -7.804348379990646
2023-04-08 23:39:44,890 - INFO - training.closure - iteration 710: loss = -7.804238625184631
2023-04-08 23:39:47,567 - INFO - training.closure - iteration 711: loss = -7.80483102118661
2023-04-08 23:39:50,180 - INFO - training.closure - iteration 712: loss = -7.805770046074334
2023-04-08 23:39:52,867 - INFO - training.closure - iteration 713: loss = -7.805930852572049
2023-04-08 23:39:55,591 - INFO - training.closure - iteration 714: loss = -7.806222300583924
2023-04-08 23:39:58,283 - INFO - training.closure - iteration 715: loss = -7.80640661148839
2023-04-08 23:40:00,976 - INFO - training.closure - iteration 716: loss = -7.806565631263793
2023-04-08 23:40:03,622 - INFO - training.closure - iteration 717: loss = -7.8067731999112615
2023-04-08 23:40:06,246 - INFO - training.closure - iteration 718: loss = -7.806999724199311
2023-04-08 23:40:08,853 - INFO - training.closure - iteration 719: loss = -7.807122785542743
2023-04-08 23:40:11,541 - INFO - training.closure - iteration 720: loss = -7.807141682810199
2023-04-08 23:40:14,178 - INFO - training.closure - iteration 721: loss = -7.807306429719793
2023-04-08 23:40:16,820 - INFO - training.closure - iteration 722: loss = -7.807372804193464
2023-04-08 23:40:19,513 - INFO - training.closure - iteration 723: loss = -7.807469854186071
2023-04-08 23:40:22,156 - INFO - training.closure - iteration 724: loss = -7.804372037888893
2023-04-08 23:40:24,803 - INFO - training.closure - iteration 725: loss = -7.80749451665657
2023-04-08 23:40:27,519 - INFO - training.closure - iteration 726: loss = -7.807581615416419
2023-04-08 23:40:30,160 - INFO - training.closure - iteration 727: loss = -7.807754180171538
2023-04-08 23:40:32,779 - INFO - training.closure - iteration 728: loss = -7.807889848809381
2023-04-08 23:40:35,411 - INFO - training.closure - iteration 729: loss = -7.808169135826617
2023-04-08 23:40:38,137 - INFO - training.closure - iteration 730: loss = -7.808502528238149
2023-04-08 23:40:40,776 - INFO - training.closure - iteration 731: loss = -7.808895091893513
2023-04-08 23:40:43,488 - INFO - training.closure - iteration 732: loss = -7.809276517042624
2023-04-08 23:40:46,104 - INFO - training.closure - iteration 733: loss = -7.8095705926391625
2023-04-08 23:40:48,738 - INFO - training.closure - iteration 734: loss = -7.80967665022387
2023-04-08 23:40:51,426 - INFO - training.closure - iteration 735: loss = -7.809718468040105
2023-04-08 23:40:54,037 - INFO - training.closure - iteration 736: loss = -7.809713460758176
2023-04-08 23:40:56,655 - INFO - training.closure - iteration 737: loss = -7.809740674159659
2023-04-08 23:40:59,272 - INFO - training.closure - iteration 738: loss = -7.809773749944476
2023-04-08 23:41:01,993 - INFO - training.closure - iteration 739: loss = -7.809850581721351
2023-04-08 23:41:04,939 - INFO - training.closure - iteration 740: loss = -7.809946037407239
2023-04-08 23:41:07,620 - INFO - training.closure - iteration 741: loss = -7.810131615436558
2023-04-08 23:41:10,522 - INFO - training.closure - iteration 742: loss = -7.810291749219856
2023-04-08 23:41:13,163 - INFO - training.closure - iteration 743: loss = -7.810379837350306
2023-04-08 23:41:15,820 - INFO - training.closure - iteration 744: loss = -7.810522268303927
2023-04-08 23:41:18,661 - INFO - training.closure - iteration 745: loss = -7.810777426559686
2023-04-08 23:41:21,281 - INFO - training.closure - iteration 746: loss = -7.811083967250185
2023-04-08 23:41:23,963 - INFO - training.closure - iteration 747: loss = -7.785427568715943
2023-04-08 23:41:26,598 - INFO - training.closure - iteration 748: loss = -7.811230199585643
2023-04-08 23:41:29,227 - INFO - training.closure - iteration 749: loss = -7.811617153095139
2023-04-08 23:41:31,850 - INFO - training.closure - iteration 750: loss = -7.811834999295439
2023-04-08 23:41:34,575 - INFO - training.closure - iteration 751: loss = -7.811990479951723
2023-04-08 23:41:37,276 - INFO - training.closure - iteration 752: loss = -7.812134628870549
2023-04-08 23:41:39,903 - INFO - training.closure - iteration 753: loss = -7.8122767389101755
2023-04-08 23:41:42,534 - INFO - training.closure - iteration 754: loss = -7.812253210654502
2023-04-08 23:41:45,155 - INFO - training.closure - iteration 755: loss = -7.81234126780646
2023-04-08 23:41:47,786 - INFO - training.closure - iteration 756: loss = -7.812450891517244
2023-04-08 23:41:50,430 - INFO - training.closure - iteration 757: loss = -7.8124601767013555
2023-04-08 23:41:53,151 - INFO - training.closure - iteration 758: loss = -7.812558315289266
2023-04-08 23:41:55,875 - INFO - training.closure - iteration 759: loss = -7.812576463206376
2023-04-08 23:41:58,689 - INFO - training.closure - iteration 760: loss = -7.812605301667704
2023-04-08 23:42:01,324 - INFO - training.closure - iteration 761: loss = -7.8126366551309365
2023-04-08 23:42:04,000 - INFO - training.closure - iteration 762: loss = -7.8126681408290475
2023-04-08 23:42:06,712 - INFO - training.closure - iteration 763: loss = -7.812570835744594
2023-04-08 23:42:09,503 - INFO - training.closure - iteration 764: loss = -7.812693041811897
2023-04-08 23:42:12,173 - INFO - training.closure - iteration 765: loss = -7.812711101140256
2023-04-08 23:42:14,822 - INFO - training.closure - iteration 766: loss = -7.812739022938633
2023-04-08 23:42:17,531 - INFO - training.closure - iteration 767: loss = -7.8127448736652045
2023-04-08 23:42:20,184 - INFO - training.closure - iteration 768: loss = -7.812775944666489
2023-04-08 23:42:22,836 - INFO - training.closure - iteration 769: loss = -7.812787361312181
2023-04-08 23:42:25,571 - INFO - training.closure - iteration 770: loss = -7.812795816080428
2023-04-08 23:42:28,225 - INFO - training.closure - iteration 771: loss = -7.812804742002626
2023-04-08 23:42:30,878 - INFO - training.closure - iteration 772: loss = -7.812827064622985
2023-04-08 23:42:33,521 - INFO - training.closure - iteration 773: loss = -7.812512939863073
2023-04-08 23:42:36,452 - INFO - training.closure - iteration 774: loss = -7.812838111967761
2023-04-08 23:42:39,163 - INFO - training.closure - iteration 775: loss = -7.812883165988387
2023-04-08 23:42:41,994 - INFO - training.closure - iteration 776: loss = -7.812989001874579
2023-04-08 23:42:44,684 - INFO - training.closure - iteration 777: loss = -7.813007934560753
2023-04-08 23:42:47,328 - INFO - training.closure - iteration 778: loss = -7.813063643385217
2023-04-08 23:42:49,984 - INFO - training.closure - iteration 779: loss = -7.8131297511255315
2023-04-08 23:42:52,634 - INFO - training.closure - iteration 780: loss = -7.813147503144219
2023-04-08 23:42:55,251 - INFO - training.closure - iteration 781: loss = -7.8131569896329385
2023-04-08 23:42:57,869 - INFO - training.closure - iteration 782: loss = -7.813164964149717
2023-04-08 23:43:00,727 - INFO - training.closure - iteration 783: loss = -7.813146101071096
2023-04-08 23:43:03,375 - INFO - training.closure - iteration 784: loss = -7.813171701194875
2023-04-08 23:43:06,011 - INFO - training.closure - iteration 785: loss = -7.813205154355912
2023-04-08 23:43:08,861 - INFO - training.closure - iteration 786: loss = -7.813253150549542
2023-04-08 23:43:11,581 - INFO - training.closure - iteration 787: loss = -7.81339719194809
2023-04-08 23:43:14,227 - INFO - training.closure - iteration 788: loss = -7.813500734540197
2023-04-08 23:43:16,960 - INFO - training.closure - iteration 789: loss = -7.813601706455039
2023-04-08 23:43:19,575 - INFO - training.closure - iteration 790: loss = -7.81367915315886
2023-04-08 23:43:22,190 - INFO - training.closure - iteration 791: loss = -7.813746631971467
2023-04-08 23:43:24,810 - INFO - training.closure - iteration 792: loss = -7.8137736874925405
2023-04-08 23:43:27,463 - INFO - training.closure - iteration 793: loss = -7.813821899452792
2023-04-08 23:43:30,130 - INFO - training.closure - iteration 794: loss = -7.8138650126597815
2023-04-08 23:43:32,769 - INFO - training.closure - iteration 795: loss = -7.813911458731283
2023-04-08 23:43:35,496 - INFO - training.closure - iteration 796: loss = -7.814018189302122
2023-04-08 23:43:38,149 - INFO - training.closure - iteration 797: loss = -7.814128356269423
2023-04-08 23:43:40,878 - INFO - training.closure - iteration 798: loss = -7.814211687343352
2023-04-08 23:43:43,510 - INFO - training.closure - iteration 799: loss = -7.814307165307557
2023-04-08 23:43:46,130 - INFO - training.closure - iteration 800: loss = -7.814535422239489
2023-04-08 23:43:48,835 - INFO - training.closure - iteration 801: loss = -7.814482223636015
2023-04-08 23:43:51,556 - INFO - training.closure - iteration 802: loss = -7.814583394649063
2023-04-08 23:43:54,166 - INFO - training.closure - iteration 803: loss = -7.8146917225456844
2023-04-08 23:43:56,791 - INFO - training.closure - iteration 804: loss = -7.814750580304198
2023-04-08 23:43:59,448 - INFO - training.closure - iteration 805: loss = -7.814780086329986
2023-04-08 23:44:02,103 - INFO - training.closure - iteration 806: loss = -7.814902618756308
2023-04-08 23:44:04,759 - INFO - training.closure - iteration 807: loss = -7.803670944388513
2023-04-08 23:44:07,536 - INFO - training.closure - iteration 808: loss = -7.81492211554972
2023-04-08 23:44:10,195 - INFO - training.closure - iteration 809: loss = -7.815014790598728
2023-04-08 23:44:12,838 - INFO - training.closure - iteration 810: loss = -7.815039240267748
2023-04-08 23:44:15,474 - INFO - training.closure - iteration 811: loss = -7.8151115292554945
2023-04-08 23:44:18,113 - INFO - training.closure - iteration 812: loss = -7.815150856372701
2023-04-08 23:44:20,763 - INFO - training.closure - iteration 813: loss = -7.815259540037152
2023-04-08 23:44:23,412 - INFO - training.closure - iteration 814: loss = -7.815393672618351
2023-04-08 23:44:26,144 - INFO - training.closure - iteration 815: loss = -7.815651156072166
2023-04-08 23:44:28,797 - INFO - training.closure - iteration 816: loss = -7.815798070392583
2023-04-08 23:44:31,435 - INFO - training.closure - iteration 817: loss = -7.815892240010481
2023-04-08 23:44:34,091 - INFO - training.closure - iteration 818: loss = -7.81592817982519
2023-04-08 23:44:36,731 - INFO - training.closure - iteration 819: loss = -7.815946654970909
2023-04-08 23:44:39,369 - INFO - training.closure - iteration 820: loss = -7.816005551341232
2023-04-08 23:44:42,210 - INFO - training.closure - iteration 821: loss = -7.8160690394969246
2023-04-08 23:44:44,882 - INFO - training.closure - iteration 822: loss = -7.816210118778789
2023-04-08 23:44:47,499 - INFO - training.closure - iteration 823: loss = -7.8154246974596795
2023-04-08 23:44:50,257 - INFO - training.closure - iteration 824: loss = -7.816280522876875
2023-04-08 23:44:52,918 - INFO - training.closure - iteration 825: loss = -7.816430664997834
2023-04-08 23:44:55,641 - INFO - training.closure - iteration 826: loss = -7.81659947898456
2023-04-08 23:44:58,354 - INFO - training.closure - iteration 827: loss = -7.816729882154784
2023-04-08 23:45:01,139 - INFO - training.closure - iteration 828: loss = -7.816846884944196
2023-04-08 23:45:03,911 - INFO - training.closure - iteration 829: loss = -7.8169251278143115
2023-04-08 23:45:06,561 - INFO - training.closure - iteration 830: loss = -7.816971701578865
2023-04-08 23:45:09,190 - INFO - training.closure - iteration 831: loss = -7.816956163639119
2023-04-08 23:45:11,821 - INFO - training.closure - iteration 832: loss = -7.817004350157765
2023-04-08 23:45:14,478 - INFO - training.closure - iteration 833: loss = -7.817065051699442
2023-04-08 23:45:17,212 - INFO - training.closure - iteration 834: loss = -7.817095385296271
2023-04-08 23:45:19,843 - INFO - training.closure - iteration 835: loss = -7.817130726266978
2023-04-08 23:45:22,498 - INFO - training.closure - iteration 836: loss = -7.817147575952329
2023-04-08 23:45:25,298 - INFO - training.closure - iteration 837: loss = -7.817177827510692
2023-04-08 23:45:27,930 - INFO - training.closure - iteration 838: loss = -7.817222602615141
2023-04-08 23:45:30,591 - INFO - training.closure - iteration 839: loss = -7.817263224796788
2023-04-08 23:45:33,289 - INFO - training.closure - iteration 840: loss = -7.817281059392331
2023-04-08 23:45:35,970 - INFO - training.closure - iteration 841: loss = -7.817285326967218
2023-04-08 23:45:38,599 - INFO - training.closure - iteration 842: loss = -7.817315313389177
2023-04-08 23:45:41,262 - INFO - training.closure - iteration 843: loss = -7.817325165788576
2023-04-08 23:45:43,893 - INFO - training.closure - iteration 844: loss = -7.8173397719034154
2023-04-08 23:45:46,562 - INFO - training.closure - iteration 845: loss = -7.8173622187546
2023-04-08 23:45:49,298 - INFO - training.closure - iteration 846: loss = -7.817379218294017
2023-04-08 23:45:51,956 - INFO - training.closure - iteration 847: loss = -7.817402856551324
2023-04-08 23:45:54,566 - INFO - training.closure - iteration 848: loss = -7.817410204848385
2023-04-08 23:45:58,868 - INFO - training.closure - iteration 849: loss = -7.817420352283076
2023-04-08 23:46:03,471 - INFO - training.closure - iteration 850: loss = -7.817430879122824
2023-04-08 23:46:07,417 - INFO - training.closure - iteration 851: loss = -7.817440278805761
2023-04-08 23:46:11,261 - INFO - training.closure - iteration 852: loss = -7.81741930358289
2023-04-08 23:46:15,042 - INFO - training.closure - iteration 853: loss = -7.8174436102023
2023-04-08 23:46:18,740 - INFO - training.closure - iteration 854: loss = -7.817458743447819
2023-04-08 23:46:22,409 - INFO - training.closure - iteration 855: loss = -7.817477040987916
2023-04-08 23:46:25,790 - INFO - training.closure - iteration 856: loss = -7.817500389535823
2023-04-08 23:46:28,445 - INFO - training.closure - iteration 857: loss = -7.817535520772165
2023-04-08 23:46:31,190 - INFO - training.closure - iteration 858: loss = -7.817589185494684
2023-04-08 23:46:33,757 - INFO - training.closure - iteration 859: loss = -7.817632600808633
2023-04-08 23:46:36,302 - INFO - training.closure - iteration 860: loss = -7.817713691783216
2023-04-08 23:46:38,840 - INFO - training.closure - iteration 861: loss = -7.817775079327987
2023-04-08 23:46:41,411 - INFO - training.closure - iteration 862: loss = -7.817814158446975
2023-04-08 23:46:43,968 - INFO - training.closure - iteration 863: loss = -7.817828289458839
2023-04-08 23:46:46,529 - INFO - training.closure - iteration 864: loss = -7.817835036289808
2023-04-08 23:46:49,192 - INFO - training.closure - iteration 865: loss = -7.817852094760148
2023-04-08 23:46:51,740 - INFO - training.closure - iteration 866: loss = -7.817858920705081
2023-04-08 23:46:54,288 - INFO - training.closure - iteration 867: loss = -7.817864755075079
2023-04-08 23:46:56,921 - INFO - training.closure - iteration 868: loss = -7.817872124585257
2023-04-08 23:46:59,492 - INFO - training.closure - iteration 869: loss = -7.817881765261121
2023-04-08 23:47:02,073 - INFO - training.closure - iteration 870: loss = -7.817890667212269
2023-04-08 23:47:04,660 - INFO - training.closure - iteration 871: loss = -7.817901285458795
2023-04-08 23:47:07,316 - INFO - training.closure - iteration 872: loss = -7.817891572359404
2023-04-08 23:47:09,961 - INFO - training.closure - iteration 873: loss = -7.817905761367961
2023-04-08 23:47:12,666 - INFO - training.closure - iteration 874: loss = -7.81792333410751
2023-04-08 23:47:15,318 - INFO - training.closure - iteration 875: loss = -7.817948312381176
2023-04-08 23:47:17,902 - INFO - training.closure - iteration 876: loss = -7.817960211906616
2023-04-08 23:47:20,483 - INFO - training.closure - iteration 877: loss = -7.81796564128608
2023-04-08 23:47:23,319 - INFO - training.closure - iteration 878: loss = -7.81797607460232
2023-04-08 23:47:25,953 - INFO - training.closure - iteration 879: loss = -7.817996738661451
2023-04-08 23:47:28,515 - INFO - training.closure - iteration 880: loss = -7.818024311184909
2023-04-08 23:47:31,140 - INFO - training.closure - iteration 881: loss = -7.817853349983141
2023-04-08 23:47:33,706 - INFO - training.closure - iteration 882: loss = -7.818033425139497
2023-04-08 23:47:36,273 - INFO - training.closure - iteration 883: loss = -7.8180525341365446
2023-04-08 23:47:39,034 - INFO - training.closure - iteration 884: loss = -7.818081631603844
2023-04-08 23:47:41,638 - INFO - training.closure - iteration 885: loss = -7.818105027687623
2023-04-08 23:47:44,224 - INFO - training.closure - iteration 886: loss = -7.818121653983903
2023-04-08 23:47:46,778 - INFO - training.closure - iteration 887: loss = -7.817849257100154
2023-04-08 23:47:49,417 - INFO - training.closure - iteration 888: loss = -7.818130426523421
2023-04-08 23:47:52,012 - INFO - training.closure - iteration 889: loss = -7.818158081325343
2023-04-08 23:47:54,604 - INFO - training.closure - iteration 890: loss = -7.818194293548945
2023-04-08 23:47:57,328 - INFO - training.closure - iteration 891: loss = -7.818220832405203
2023-04-08 23:47:59,937 - INFO - training.closure - iteration 892: loss = -7.81825993668415
2023-04-08 23:48:02,527 - INFO - training.closure - iteration 893: loss = -7.818317565562225
2023-04-08 23:48:05,139 - INFO - training.closure - iteration 894: loss = -7.818375316160214
2023-04-08 23:48:07,723 - INFO - training.closure - iteration 895: loss = -7.818205450045044
2023-04-08 23:48:10,298 - INFO - training.closure - iteration 896: loss = -7.8183979200676195
2023-04-08 23:48:13,203 - INFO - training.closure - iteration 897: loss = -7.818406182829092
2023-04-08 23:48:16,168 - INFO - training.closure - iteration 898: loss = -7.818420782223575
2023-04-08 23:48:18,888 - INFO - training.closure - iteration 899: loss = -7.818423472150135
2023-04-08 23:48:21,531 - INFO - training.closure - iteration 900: loss = -7.818425762650796
2023-04-08 23:48:24,109 - INFO - training.closure - iteration 901: loss = -7.81843164933205
2023-04-08 23:48:26,705 - INFO - training.closure - iteration 902: loss = -7.818430192934734
2023-04-08 23:48:29,419 - INFO - training.closure - iteration 903: loss = -7.818436525619145
2023-04-08 23:48:32,227 - INFO - training.closure - iteration 904: loss = -7.818440190639707
2023-04-08 23:48:38,247 - INFO - training.closure - iteration 905: loss = -7.818445106435453
2023-04-08 23:48:41,722 - INFO - training.closure - iteration 906: loss = -7.818447865643517
2023-04-08 23:48:44,602 - INFO - training.closure - iteration 907: loss = -7.818453257490763
2023-04-08 23:48:47,173 - INFO - training.closure - iteration 908: loss = -7.818458157886981
2023-04-08 23:48:49,690 - INFO - training.closure - iteration 909: loss = -7.818463364348354
2023-04-08 23:48:52,324 - INFO - training.closure - iteration 910: loss = -7.8184724664238034
2023-04-08 23:48:55,024 - INFO - training.closure - iteration 911: loss = -7.81848806687213
2023-04-08 23:48:57,583 - INFO - training.closure - iteration 912: loss = -7.818462080694944
2023-04-08 23:49:00,146 - INFO - training.closure - iteration 913: loss = -7.818494908843842
2023-04-08 23:49:02,733 - INFO - training.closure - iteration 914: loss = -7.818510688995863
2023-04-08 23:49:05,293 - INFO - training.closure - iteration 915: loss = -7.818522556526686
2023-04-08 23:49:08,030 - INFO - training.closure - iteration 916: loss = -7.81852572522077
2023-04-08 23:49:10,617 - INFO - training.closure - iteration 917: loss = -7.8185286035917025
2023-04-08 23:49:13,208 - INFO - training.closure - iteration 918: loss = -7.818530241640952
2023-04-08 23:49:15,861 - INFO - training.closure - iteration 919: loss = -7.818532668224919
2023-04-08 23:49:18,454 - INFO - training.closure - iteration 920: loss = -7.818536447051111
2023-04-08 23:49:21,031 - INFO - training.closure - iteration 921: loss = -7.818523194399329
2023-04-08 23:49:23,732 - INFO - training.closure - iteration 922: loss = -7.81854150242034
2023-04-08 23:49:26,343 - INFO - training.closure - iteration 923: loss = -7.8185543499199985
2023-04-08 23:49:28,930 - INFO - training.closure - iteration 924: loss = -7.818566693445193
2023-04-08 23:49:31,539 - INFO - training.closure - iteration 925: loss = -7.818578768127553
2023-04-08 23:49:34,166 - INFO - training.closure - iteration 926: loss = -7.8185837080913405
2023-04-08 23:49:36,756 - INFO - training.closure - iteration 927: loss = -7.818586571135832
2023-04-08 23:49:39,365 - INFO - training.closure - iteration 928: loss = -7.818594095875959
2023-04-08 23:49:42,126 - INFO - training.closure - iteration 929: loss = -7.818600919476395
2023-04-08 23:49:44,699 - INFO - training.closure - iteration 930: loss = -7.818607890994523
2023-04-08 23:49:47,427 - INFO - training.closure - iteration 931: loss = -7.81856455869919
2023-04-08 23:49:50,045 - INFO - training.closure - iteration 932: loss = -7.818614627339377
2023-04-08 23:49:52,611 - INFO - training.closure - iteration 933: loss = -7.818632349197314
2023-04-08 23:49:55,181 - INFO - training.closure - iteration 934: loss = -7.8186674049709834
2023-04-08 23:49:57,900 - INFO - training.closure - iteration 935: loss = -7.81873970960139
2023-04-08 23:50:00,488 - INFO - training.closure - iteration 936: loss = -7.818781683698159
2023-04-08 23:50:03,072 - INFO - training.closure - iteration 937: loss = -7.818816349302034
2023-04-08 23:50:05,660 - INFO - training.closure - iteration 938: loss = -7.818831165908456
2023-04-08 23:50:08,238 - INFO - training.closure - iteration 939: loss = -7.818815865583835
2023-04-08 23:50:10,802 - INFO - training.closure - iteration 940: loss = -7.818832991718751
2023-04-08 23:50:13,511 - INFO - training.closure - iteration 941: loss = -7.818835464169206
2023-04-08 23:50:16,116 - INFO - training.closure - iteration 942: loss = -7.818837445513609
2023-04-08 23:50:18,685 - INFO - training.closure - iteration 943: loss = -7.818839353760607
2023-04-08 23:50:21,267 - INFO - training.closure - iteration 944: loss = -7.818842669523956
2023-04-08 23:50:23,929 - INFO - training.closure - iteration 945: loss = -7.818828998914271
2023-04-08 23:50:26,622 - INFO - training.closure - iteration 946: loss = -7.8188438334262065
2023-04-08 23:50:29,215 - INFO - training.closure - iteration 947: loss = -7.8188479974598195
2023-04-08 23:50:31,921 - INFO - training.closure - iteration 948: loss = -7.818852343878698
2023-04-08 23:50:34,532 - INFO - training.closure - iteration 949: loss = -7.818856219651123
2023-04-08 23:50:37,132 - INFO - training.closure - iteration 950: loss = -7.818862853456352
2023-04-08 23:50:40,150 - INFO - training.closure - iteration 951: loss = -7.818875904301622
2023-04-08 23:50:43,129 - INFO - training.closure - iteration 952: loss = -7.818390019241795
2023-04-08 23:50:45,842 - INFO - training.closure - iteration 953: loss = -7.818880954342791
2023-04-08 23:50:48,529 - INFO - training.closure - iteration 954: loss = -7.818905652861276
2023-04-08 23:50:51,190 - INFO - training.closure - iteration 955: loss = -7.818953304803683
2023-04-08 23:50:53,785 - INFO - training.closure - iteration 956: loss = -7.819003295784999
2023-04-08 23:50:56,378 - INFO - training.closure - iteration 957: loss = -7.819051733330772
2023-04-08 23:50:58,957 - INFO - training.closure - iteration 958: loss = -7.819127772884027
2023-04-08 23:51:01,568 - INFO - training.closure - iteration 959: loss = -7.819221023800764
2023-04-08 23:51:04,278 - INFO - training.closure - iteration 960: loss = -7.819069807085386
2023-04-08 23:51:06,876 - INFO - training.closure - iteration 961: loss = -7.8193093530194595
2023-04-08 23:51:09,528 - INFO - training.closure - iteration 962: loss = -7.819386357539637
2023-04-08 23:51:12,150 - INFO - training.closure - iteration 963: loss = -7.819559376035526
2023-04-08 23:51:14,795 - INFO - training.closure - iteration 964: loss = -7.819612756952215
2023-04-08 23:51:17,409 - INFO - training.closure - iteration 965: loss = -7.819651620636842
2023-04-08 23:51:20,008 - INFO - training.closure - iteration 966: loss = -7.819246547584777
2023-04-08 23:51:22,692 - INFO - training.closure - iteration 967: loss = -7.81966634434947
2023-04-08 23:51:25,277 - INFO - training.closure - iteration 968: loss = -7.81971795121224
2023-04-08 23:51:27,894 - INFO - training.closure - iteration 969: loss = -7.8197949455755635
2023-04-08 23:51:30,501 - INFO - training.closure - iteration 970: loss = -7.819811167721175
2023-04-08 23:51:33,132 - INFO - training.closure - iteration 971: loss = -7.819815331640813
2023-04-08 23:51:35,736 - INFO - training.closure - iteration 972: loss = -7.819819202170342
2023-04-08 23:51:38,418 - INFO - training.closure - iteration 973: loss = -7.819826622612376
2023-04-08 23:51:41,034 - INFO - training.closure - iteration 974: loss = -7.81983168836552
2023-04-08 23:51:43,706 - INFO - training.closure - iteration 975: loss = -7.819849789486225
2023-04-08 23:51:46,315 - INFO - training.closure - iteration 976: loss = -7.819858175018187
2023-04-08 23:51:48,914 - INFO - training.closure - iteration 977: loss = -7.819865246362367
2023-04-08 23:51:51,517 - INFO - training.closure - iteration 978: loss = -7.819868526732659
2023-04-08 23:51:54,208 - INFO - training.closure - iteration 979: loss = -7.819874335846995
2023-04-08 23:51:56,948 - INFO - training.closure - iteration 980: loss = -7.819867934776132
2023-04-08 23:51:59,536 - INFO - training.closure - iteration 981: loss = -7.819881974983433
2023-04-08 23:52:02,146 - INFO - training.closure - iteration 982: loss = -7.81990003177505
2023-04-08 23:52:04,738 - INFO - training.closure - iteration 983: loss = -7.8199509641385205
2023-04-08 23:52:07,311 - INFO - training.closure - iteration 984: loss = -7.820029706296723
2023-04-08 23:52:09,965 - INFO - training.closure - iteration 985: loss = -7.820097810211161
2023-04-08 23:52:12,644 - INFO - training.closure - iteration 986: loss = -7.820133138588918
2023-04-08 23:52:15,474 - INFO - training.closure - iteration 987: loss = -7.82016589600497
2023-04-08 23:52:18,290 - INFO - training.closure - iteration 988: loss = -7.82019491258069
2023-04-08 23:52:21,292 - INFO - training.closure - iteration 989: loss = -7.8202156500095
2023-04-08 23:52:24,042 - INFO - training.closure - iteration 990: loss = -7.820240082804216
2023-04-08 23:52:26,901 - INFO - training.closure - iteration 991: loss = -7.820282134127332
2023-04-08 23:52:29,939 - INFO - training.closure - iteration 992: loss = -7.820143908584557
2023-04-08 23:52:32,705 - INFO - training.closure - iteration 993: loss = -7.820344744411008
2023-04-08 23:52:35,650 - INFO - training.closure - iteration 994: loss = -7.8204187497478275
2023-04-08 23:52:38,496 - INFO - training.closure - iteration 995: loss = -7.820543107403989
2023-04-08 23:52:41,511 - INFO - training.closure - iteration 996: loss = -7.820589612792698
2023-04-08 23:52:44,459 - INFO - training.closure - iteration 997: loss = -7.820359196079542
2023-04-08 23:52:47,528 - INFO - training.closure - iteration 998: loss = -7.820621614841805
2023-04-08 23:52:50,607 - INFO - training.closure - iteration 999: loss = -7.820584666777618
2023-04-08 23:52:53,704 - INFO - training.closure - iteration 1000: loss = -7.82067731570241
2023-04-08 23:52:56,686 - INFO - training.closure - iteration 1001: loss = -7.820693778427717
2023-04-08 23:52:59,265 - INFO - training.closure - iteration 1002: loss = -7.820713025834519
2023-04-08 23:53:01,880 - INFO - training.closure - iteration 1003: loss = -7.8207349789761595
2023-04-08 23:53:04,490 - INFO - training.closure - iteration 1004: loss = -7.820750340751986
2023-04-08 23:53:07,183 - INFO - training.closure - iteration 1005: loss = -7.820765962301778
2023-04-08 23:53:09,757 - INFO - training.closure - iteration 1006: loss = -7.820784123933006
2023-04-08 23:53:12,533 - INFO - training.closure - iteration 1007: loss = -7.820814515005132
2023-04-08 23:53:15,156 - INFO - training.closure - iteration 1008: loss = -7.820882325505811
2023-04-08 23:53:17,762 - INFO - training.closure - iteration 1009: loss = -7.821002037205552
2023-04-08 23:53:20,350 - INFO - training.closure - iteration 1010: loss = -7.8203839700884155
2023-04-08 23:53:23,033 - INFO - training.closure - iteration 1011: loss = -7.821025937737255
2023-04-08 23:53:25,631 - INFO - training.closure - iteration 1012: loss = -7.821105570757442
2023-04-08 23:53:28,245 - INFO - training.closure - iteration 1013: loss = -7.821148311970559
2023-04-08 23:53:30,851 - INFO - training.closure - iteration 1014: loss = -7.821214452936346
2023-04-08 23:53:33,479 - INFO - training.closure - iteration 1015: loss = -7.821230546767193
2023-04-08 23:53:36,084 - INFO - training.closure - iteration 1016: loss = -7.8212620297154425
2023-04-08 23:53:38,753 - INFO - training.closure - iteration 1017: loss = -7.821276506409047
2023-04-08 23:53:41,449 - INFO - training.closure - iteration 1018: loss = -7.821291027419887
2023-04-08 23:53:44,082 - INFO - training.closure - iteration 1019: loss = -7.821298934465686
2023-04-08 23:53:46,664 - INFO - training.closure - iteration 1020: loss = -7.821307565596636
2023-04-08 23:53:49,252 - INFO - training.closure - iteration 1021: loss = -7.821317885573491
2023-04-08 23:53:51,858 - INFO - training.closure - iteration 1022: loss = -7.821329928786556
2023-04-08 23:53:54,462 - INFO - training.closure - iteration 1023: loss = -7.821344729118472
2023-04-08 23:53:57,146 - INFO - training.closure - iteration 1024: loss = -7.821357244921939
2023-04-08 23:53:59,749 - INFO - training.closure - iteration 1025: loss = -7.821369731844137
2023-04-08 23:54:02,355 - INFO - training.closure - iteration 1026: loss = -7.821383877410598
2023-04-08 23:54:04,931 - INFO - training.closure - iteration 1027: loss = -7.821418215297243
2023-04-08 23:54:07,507 - INFO - training.closure - iteration 1028: loss = -7.8214315466628594
2023-04-08 23:54:10,085 - INFO - training.closure - iteration 1029: loss = -7.821445602506595
2023-04-08 23:54:12,765 - INFO - training.closure - iteration 1030: loss = -7.821469834067811
2023-04-08 23:54:15,362 - INFO - training.closure - iteration 1031: loss = -7.821483582355267
2023-04-08 23:54:17,937 - INFO - training.closure - iteration 1032: loss = -7.8214983631842685
2023-04-08 23:54:20,542 - INFO - training.closure - iteration 1033: loss = -7.821514908008955
2023-04-08 23:54:23,151 - INFO - training.closure - iteration 1034: loss = -7.821127344759742
2023-04-08 23:54:25,755 - INFO - training.closure - iteration 1035: loss = -7.821517927279417
2023-04-08 23:54:28,432 - INFO - training.closure - iteration 1036: loss = -7.821530220190381
2023-04-08 23:54:31,046 - INFO - training.closure - iteration 1037: loss = -7.821508240939155
2023-04-08 23:54:33,650 - INFO - training.closure - iteration 1038: loss = -7.821532701085334
2023-04-08 23:54:36,253 - INFO - training.closure - iteration 1039: loss = -7.821538320042279
2023-04-08 23:54:38,898 - INFO - training.closure - iteration 1040: loss = -7.8215473373143904
2023-04-08 23:54:41,507 - INFO - training.closure - iteration 1041: loss = -7.821569490897852
2023-04-08 23:54:44,174 - INFO - training.closure - iteration 1042: loss = -7.821617649451072
2023-04-08 23:54:46,862 - INFO - training.closure - iteration 1043: loss = -7.821699201802389
2023-04-08 23:54:49,464 - INFO - training.closure - iteration 1044: loss = -7.82169398181551
2023-04-08 23:54:52,074 - INFO - training.closure - iteration 1045: loss = -7.821744124788084
2023-04-08 23:54:54,827 - INFO - training.closure - iteration 1046: loss = -7.821830188788798
2023-04-08 23:54:57,415 - INFO - training.closure - iteration 1047: loss = -7.8218593552415525
2023-04-08 23:54:59,992 - INFO - training.closure - iteration 1048: loss = -7.82191434766718
2023-04-08 23:55:02,663 - INFO - training.closure - iteration 1049: loss = -7.821973026239288
2023-04-08 23:55:05,242 - INFO - training.closure - iteration 1050: loss = -7.822003892460792
2023-04-08 23:55:07,847 - INFO - training.closure - iteration 1051: loss = -7.822062893594624
2023-04-08 23:55:10,444 - INFO - training.closure - iteration 1052: loss = -7.822144885845557
2023-04-08 23:55:13,026 - INFO - training.closure - iteration 1053: loss = -7.822192660499752
2023-04-08 23:55:15,625 - INFO - training.closure - iteration 1054: loss = -7.821947622949304
2023-04-08 23:55:18,303 - INFO - training.closure - iteration 1055: loss = -7.82220016608174
2023-04-08 23:55:20,909 - INFO - training.closure - iteration 1056: loss = -7.82221784647391
2023-04-08 23:55:23,511 - INFO - training.closure - iteration 1057: loss = -7.822230374614777
2023-04-08 23:55:26,115 - INFO - training.closure - iteration 1058: loss = -7.8222519763416845
2023-04-08 23:55:28,792 - INFO - training.closure - iteration 1059: loss = -7.822261863086356
2023-04-08 23:55:31,389 - INFO - training.closure - iteration 1060: loss = -7.822266186256613
2023-04-08 23:55:34,011 - INFO - training.closure - iteration 1061: loss = -7.822271946818304
2023-04-08 23:55:36,690 - INFO - training.closure - iteration 1062: loss = -7.822278438208413
2023-04-08 23:55:39,417 - INFO - training.closure - iteration 1063: loss = -7.822288417525785
2023-04-08 23:55:42,185 - INFO - training.closure - iteration 1064: loss = -7.822309755076728
2023-04-08 23:55:44,823 - INFO - training.closure - iteration 1065: loss = -7.822327740376577
2023-04-08 23:55:47,432 - INFO - training.closure - iteration 1066: loss = -7.822367795255305
2023-04-08 23:55:50,011 - INFO - training.closure - iteration 1067: loss = -7.822436772011419
2023-04-08 23:55:52,688 - INFO - training.closure - iteration 1068: loss = -7.822520406342992
2023-04-08 23:55:55,394 - INFO - training.closure - iteration 1069: loss = -7.822601228644977
2023-04-08 23:55:57,981 - INFO - training.closure - iteration 1070: loss = -7.822653795719107
2023-04-08 23:56:00,653 - INFO - training.closure - iteration 1071: loss = -7.822653074185407
2023-04-08 23:56:03,271 - INFO - training.closure - iteration 1072: loss = -7.822674073195489
2023-04-08 23:56:05,869 - INFO - training.closure - iteration 1073: loss = -7.8227021451508545
2023-04-08 23:56:08,555 - INFO - training.closure - iteration 1074: loss = -7.822720799158593
2023-04-08 23:56:11,147 - INFO - training.closure - iteration 1075: loss = -7.822742415082541
2023-04-08 23:56:13,730 - INFO - training.closure - iteration 1076: loss = -7.822730298068285
2023-04-08 23:56:16,357 - INFO - training.closure - iteration 1077: loss = -7.822757667928748
2023-04-08 23:56:18,992 - INFO - training.closure - iteration 1078: loss = -7.822781992625069
2023-04-08 23:56:21,635 - INFO - training.closure - iteration 1079: loss = -7.822815154246182
2023-04-08 23:56:24,347 - INFO - training.closure - iteration 1080: loss = -7.822515278981392
2023-04-08 23:56:27,083 - INFO - training.closure - iteration 1081: loss = -7.822842038814524
2023-04-08 23:56:29,737 - INFO - training.closure - iteration 1082: loss = -7.8228919648401325
2023-04-08 23:56:32,425 - INFO - training.closure - iteration 1083: loss = -7.822995148478086
2023-04-08 23:56:35,057 - INFO - training.closure - iteration 1084: loss = -7.823052318656958
2023-04-08 23:56:37,686 - INFO - training.closure - iteration 1085: loss = -7.8230875157897195
2023-04-08 23:56:40,338 - INFO - training.closure - iteration 1086: loss = -7.823125514943648
2023-04-08 23:56:43,085 - INFO - training.closure - iteration 1087: loss = -7.823163491854315
2023-04-08 23:56:45,740 - INFO - training.closure - iteration 1088: loss = -7.823194577110053
2023-04-08 23:56:48,412 - INFO - training.closure - iteration 1089: loss = -7.823043181583703
2023-04-08 23:56:51,046 - INFO - training.closure - iteration 1090: loss = -7.823199133615185
2023-04-08 23:56:53,728 - INFO - training.closure - iteration 1091: loss = -7.8232128143715265
2023-04-08 23:56:56,369 - INFO - training.closure - iteration 1092: loss = -7.823228623842844
2023-04-08 23:56:59,087 - INFO - training.closure - iteration 1093: loss = -7.823257186057837
2023-04-08 23:57:01,794 - INFO - training.closure - iteration 1094: loss = -7.823304914875451
2023-04-08 23:57:04,442 - INFO - training.closure - iteration 1095: loss = -7.8233765993939866
2023-04-08 23:57:07,076 - INFO - training.closure - iteration 1096: loss = -7.8234719579375085
2023-04-08 23:57:09,783 - INFO - training.closure - iteration 1097: loss = -7.823457883293944
2023-04-08 23:57:12,443 - INFO - training.closure - iteration 1098: loss = -7.823518401372972
2023-04-08 23:57:15,102 - INFO - training.closure - iteration 1099: loss = -7.8236094790341335
2023-04-08 23:57:17,826 - INFO - training.closure - iteration 1100: loss = -7.823749203643748
2023-04-08 23:57:20,476 - INFO - training.closure - iteration 1101: loss = -7.82381261089123
2023-04-08 23:57:23,147 - INFO - training.closure - iteration 1102: loss = -7.823978083059674
2023-04-08 23:57:25,814 - INFO - training.closure - iteration 1103: loss = -7.823820792419136
2023-04-08 23:57:28,469 - INFO - training.closure - iteration 1104: loss = -7.824070425220272
2023-04-08 23:57:31,097 - INFO - training.closure - iteration 1105: loss = -7.824130035170295
2023-04-08 23:57:33,834 - INFO - training.closure - iteration 1106: loss = -7.824191477232008
2023-04-08 23:57:36,473 - INFO - training.closure - iteration 1107: loss = -7.824215741113992
2023-04-08 23:57:39,101 - INFO - training.closure - iteration 1108: loss = -7.824230159808543
2023-04-08 23:57:41,742 - INFO - training.closure - iteration 1109: loss = -7.824234085304049
2023-04-08 23:57:44,381 - INFO - training.closure - iteration 1110: loss = -7.824235459691569
2023-04-08 23:57:47,033 - INFO - training.closure - iteration 1111: loss = -7.824237386677856
2023-04-08 23:57:49,778 - INFO - training.closure - iteration 1112: loss = -7.824245242200325
2023-04-08 23:57:52,439 - INFO - training.closure - iteration 1113: loss = -7.824257353399446
2023-04-08 23:57:55,094 - INFO - training.closure - iteration 1114: loss = -7.824281704765609
2023-04-08 23:57:57,726 - INFO - training.closure - iteration 1115: loss = -7.824321659078631
2023-04-08 23:58:00,368 - INFO - training.closure - iteration 1116: loss = -7.824377635024141
2023-04-08 23:58:03,007 - INFO - training.closure - iteration 1117: loss = -7.824423672183449
2023-04-08 23:58:05,632 - INFO - training.closure - iteration 1118: loss = -7.824451669850532
2023-04-08 23:58:08,403 - INFO - training.closure - iteration 1119: loss = -7.824466028916288
2023-04-08 23:58:11,190 - INFO - training.closure - iteration 1120: loss = -7.824475870940349
2023-04-08 23:58:13,831 - INFO - training.closure - iteration 1121: loss = -7.82451346124306
2023-04-08 23:58:16,514 - INFO - training.closure - iteration 1122: loss = -7.824538995383664
2023-04-08 23:58:19,144 - INFO - training.closure - iteration 1123: loss = -7.8241330461875656
2023-04-08 23:58:21,799 - INFO - training.closure - iteration 1124: loss = -7.824558520410984
2023-04-08 23:58:24,510 - INFO - training.closure - iteration 1125: loss = -7.824576878332983
2023-04-08 23:58:27,180 - INFO - training.closure - iteration 1126: loss = -7.8246133108520795
2023-04-08 23:58:29,837 - INFO - training.closure - iteration 1127: loss = -7.824626924574325
2023-04-08 23:58:32,503 - INFO - training.closure - iteration 1128: loss = -7.824641740424237
2023-04-08 23:58:35,155 - INFO - training.closure - iteration 1129: loss = -7.824659640114521
2023-04-08 23:58:37,809 - INFO - training.closure - iteration 1130: loss = -7.82469761345685
2023-04-08 23:58:40,518 - INFO - training.closure - iteration 1131: loss = -7.824764672736724
2023-04-08 23:58:43,173 - INFO - training.closure - iteration 1132: loss = -7.822872943902806
2023-04-08 23:58:45,829 - INFO - training.closure - iteration 1133: loss = -7.824796628354451
2023-04-08 23:58:48,489 - INFO - training.closure - iteration 1134: loss = -7.8248538648585075
2023-04-08 23:58:51,145 - INFO - training.closure - iteration 1135: loss = -7.824900957372329
2023-04-08 23:58:53,800 - INFO - training.closure - iteration 1136: loss = -7.824926796801144
2023-04-08 23:58:56,452 - INFO - training.closure - iteration 1137: loss = -7.824952954386694
2023-04-08 23:58:59,163 - INFO - training.closure - iteration 1138: loss = -7.824968597320092
2023-04-08 23:59:01,814 - INFO - training.closure - iteration 1139: loss = -7.824980306545479
2023-04-08 23:59:04,451 - INFO - training.closure - iteration 1140: loss = -7.82498880372254
2023-04-08 23:59:07,105 - INFO - training.closure - iteration 1141: loss = -7.824997716833855
2023-04-08 23:59:09,785 - INFO - training.closure - iteration 1142: loss = -7.824985111780787
2023-04-08 23:59:12,458 - INFO - training.closure - iteration 1143: loss = -7.825002436421367
2023-04-08 23:59:15,194 - INFO - training.closure - iteration 1144: loss = -7.825006670452621
2023-04-08 23:59:17,847 - INFO - training.closure - iteration 1145: loss = -7.825009984755898
2023-04-08 23:59:20,552 - INFO - training.closure - iteration 1146: loss = -7.825020187846169
2023-04-09 00:03:24,762 - INFO - main.experiment - train - RMSE_b at iteration 0 = 0.12104007118274396
2023-04-09 00:03:24,762 - INFO - main.experiment - train - RMSE_b at last iteration = 0.05356659436150774
2023-04-09 00:03:24,762 - INFO - main.experiment - train - RMSE_a at iteration 0 = 0.2205036761225336
2023-04-09 00:03:24,762 - INFO - main.experiment - train - RMSE_a at last iteration = 0.051322024625666235
2023-04-09 00:03:24,762 - INFO - main.experiment - train - LOGPDF_b at iteration 0 = -0.9221991182956469
2023-04-09 00:03:24,762 - INFO - main.experiment - train - LOGPDF_b at last iteration = -3.0773778191365686
2023-04-09 00:03:24,762 - INFO - main.experiment - train - LOGPDF_a at iteration 0 = 5.3650789398859775
2023-04-09 00:03:24,762 - INFO - main.experiment - train - LOGPDF_a at last iteration = -3.1781520944617703
2023-04-09 00:03:24,762 - INFO - main.experiment - train - LOSS at iteration 0 = 4.4428798215903305
2023-04-09 00:03:24,762 - INFO - main.experiment - train - LOSS at last iteration = -6.255529913598339
2023-04-09 00:04:02,958 - INFO - main.experiment - test - RMSE_b at iteration 0 = 0.13269171314264344
2023-04-09 00:04:02,958 - INFO - main.experiment - test - RMSE_b at last iteration = 0.07064383754772263
2023-04-09 00:04:02,958 - INFO - main.experiment - test - RMSE_a at iteration 0 = 0.08872632789562511
2023-04-09 00:04:02,958 - INFO - main.experiment - test - RMSE_a at last iteration = 0.04719234716992639
2023-04-09 00:04:02,958 - INFO - main.experiment - test - LOGPDF_b at iteration 0 = -0.5860462007262384
2023-04-09 00:04:02,958 - INFO - main.experiment - test - LOGPDF_b at last iteration = 1.6342050411282216
2023-04-09 00:04:02,958 - INFO - main.experiment - test - LOGPDF_a at iteration 0 = -1.6552033848168115
2023-04-09 00:04:02,958 - INFO - main.experiment - test - LOGPDF_a at last iteration = 2.4535238175898506
2023-04-09 00:04:02,958 - INFO - main.experiment - test - LOSS at iteration 0 = -2.24124958554305
2023-04-09 00:04:02,958 - INFO - main.experiment - test - LOSS at last iteration = 4.087728858718072
2023-04-09 00:04:02,958 - INFO - main.experiment - deep = 10 - plot = False - sigma0 = 0.01
2023-04-09 00:04:02,958 - INFO - training.pre_train_full - empirical mean of x0: 3.0000319984329904
2023-04-09 00:04:02,974 - INFO - training.pre_train_full - initial loss: 12.007031065098376
2023-04-09 00:04:03,005 - INFO - training.closure0 - iteration 0: loss = 12.007031065098376
2023-04-09 00:04:03,021 - INFO - training.closure0 - iteration 1: loss = 6.638197328803305
2023-04-09 00:04:03,036 - INFO - training.closure0 - iteration 2: loss = 5.718157581877124
2023-04-09 00:04:03,052 - INFO - training.closure0 - iteration 3: loss = 4.957163340234566
2023-04-09 00:04:03,068 - INFO - training.closure0 - iteration 4: loss = 4.6183457129588135
2023-04-09 00:04:03,068 - INFO - training.closure0 - iteration 5: loss = 4.263543723636038
2023-04-09 00:04:03,096 - INFO - training.closure0 - iteration 6: loss = 3.5321694818936233
2023-04-09 00:04:03,099 - INFO - training.closure0 - iteration 7: loss = 4.067561926641515
2023-04-09 00:04:03,115 - INFO - training.closure0 - iteration 8: loss = 2.585630121282783
2023-04-09 00:04:03,131 - INFO - training.closure0 - iteration 9: loss = 6072.262449970343
2023-04-09 00:04:03,146 - INFO - training.closure0 - iteration 10: loss = 91.49869206389062
2023-04-09 00:04:03,162 - INFO - training.closure0 - iteration 11: loss = 4.4269103051293754
2023-04-09 00:04:03,178 - INFO - training.closure0 - iteration 12: loss = 2.379229329495842
2023-04-09 00:04:03,196 - INFO - training.closure0 - iteration 13: loss = 1.528515313885664
2023-04-09 00:04:03,209 - INFO - training.closure0 - iteration 14: loss = 1.4205882666767131
2023-04-09 00:04:03,225 - INFO - training.closure0 - iteration 15: loss = 100213.91336902733
2023-04-09 00:04:03,240 - INFO - training.closure0 - iteration 16: loss = 498.42251657123126
2023-04-09 00:04:03,256 - INFO - training.closure0 - iteration 17: loss = 5.133804167414947
2023-04-09 00:04:03,256 - INFO - training.closure0 - iteration 18: loss = 0.5170414849465588
2023-04-09 00:04:03,272 - INFO - training.closure0 - iteration 19: loss = 304011089.10971224
2023-04-09 00:04:03,297 - INFO - training.closure0 - iteration 20: loss = 0.5170413648560364
2023-04-09 00:04:03,303 - INFO - training.closure0 - iteration 21: loss = 7.52007627694141
2023-04-09 00:04:03,319 - INFO - training.closure0 - iteration 22: loss = 0.4940125418408813
2023-04-09 00:04:03,334 - INFO - training.closure0 - iteration 23: loss = 0.37604169515544716
2023-04-09 00:04:03,350 - INFO - training.closure0 - iteration 24: loss = 1821.1905617526827
2023-04-09 00:04:03,366 - INFO - training.closure0 - iteration 25: loss = 22.4978494387507
2023-04-09 00:04:03,381 - INFO - training.closure0 - iteration 26: loss = -0.01635604855111447
2023-04-09 00:04:03,397 - INFO - training.closure0 - iteration 27: loss = -0.19019153569432898
2023-04-09 00:04:03,413 - INFO - training.closure0 - iteration 28: loss = 8.791624108782251
2023-04-09 00:04:03,429 - INFO - training.closure0 - iteration 29: loss = 0.035474214606081834
2023-04-09 00:04:03,444 - INFO - training.closure0 - iteration 30: loss = -0.2255656735284118
2023-04-09 00:04:03,460 - INFO - training.closure0 - iteration 31: loss = -0.24890097208413958
2023-04-09 00:04:03,460 - INFO - training.closure0 - iteration 32: loss = -0.2727022132657524
2023-04-09 00:04:03,476 - INFO - training.closure0 - iteration 33: loss = -0.28089807625907737
2023-04-09 00:04:03,497 - INFO - training.closure0 - iteration 34: loss = -0.335822511353115
2023-04-09 00:04:03,507 - INFO - training.closure0 - iteration 35: loss = -0.2830595216636535
2023-04-09 00:04:03,523 - INFO - training.closure0 - iteration 36: loss = -0.3775297221045447
2023-04-09 00:04:03,538 - INFO - training.closure0 - iteration 37: loss = -0.44906942648828296
2023-04-09 00:04:03,554 - INFO - training.closure0 - iteration 38: loss = -0.5692676551482231
2023-04-09 00:04:03,570 - INFO - training.closure0 - iteration 39: loss = 42.94725761560402
2023-04-09 00:04:03,585 - INFO - training.closure0 - iteration 40: loss = 0.4516054562300197
2023-04-09 00:04:03,601 - INFO - training.closure0 - iteration 41: loss = -0.588320134362826
2023-04-09 00:04:03,617 - INFO - training.closure0 - iteration 42: loss = -0.652173296823263
2023-04-09 00:04:03,633 - INFO - training.closure0 - iteration 43: loss = -0.6319376737155799
2023-04-09 00:04:03,648 - INFO - training.closure0 - iteration 44: loss = -0.6997535104008167
2023-04-09 00:04:03,664 - INFO - training.closure0 - iteration 45: loss = -0.7921059524018872
2023-04-09 00:04:03,679 - INFO - training.closure0 - iteration 46: loss = -0.19575482948322787
2023-04-09 00:04:03,697 - INFO - training.closure0 - iteration 47: loss = -0.8087066524138252
2023-04-09 00:04:03,711 - INFO - training.closure0 - iteration 48: loss = -0.8678870241921216
2023-04-09 00:04:03,727 - INFO - training.closure0 - iteration 49: loss = -0.9046590394466614
2023-04-09 00:04:03,742 - INFO - training.closure0 - iteration 50: loss = -0.9813049971538572
2023-04-09 00:04:03,758 - INFO - training.closure0 - iteration 51: loss = 0.4706787084106445
2023-04-09 00:04:03,774 - INFO - training.closure0 - iteration 52: loss = -0.9855448811750649
2023-04-09 00:04:03,789 - INFO - training.closure0 - iteration 53: loss = -0.9979956502179077
2023-04-09 00:04:03,805 - INFO - training.closure0 - iteration 54: loss = -1.0128545170888381
2023-04-09 00:04:03,805 - INFO - training.closure0 - iteration 55: loss = -1.0380225956759683
2023-04-09 00:04:03,821 - INFO - training.closure0 - iteration 56: loss = -1.0427751200829807
2023-04-09 00:04:03,836 - INFO - training.closure0 - iteration 57: loss = -1.0451494733666555
2023-04-09 00:04:03,852 - INFO - training.closure0 - iteration 58: loss = -1.0556257763759016
2023-04-09 00:04:03,868 - INFO - training.closure0 - iteration 59: loss = -1.0588120739522044
2023-04-09 00:04:03,897 - INFO - training.closure0 - iteration 60: loss = -1.0598712232130652
2023-04-09 00:04:03,899 - INFO - training.closure0 - iteration 61: loss = -1.0599938120848846
2023-04-09 00:04:03,931 - INFO - training.closure0 - iteration 62: loss = -1.0600937604806633
2023-04-09 00:04:03,947 - INFO - training.closure0 - iteration 63: loss = -1.0602136712745396
2023-04-09 00:04:03,962 - INFO - training.closure0 - iteration 64: loss = -1.0602444055438132
2023-04-09 00:04:03,978 - INFO - training.closure0 - iteration 65: loss = -1.0606255828536066
2023-04-09 00:04:04,009 - INFO - training.closure0 - iteration 66: loss = -1.0613437266920436
2023-04-09 00:04:04,009 - INFO - training.closure0 - iteration 67: loss = -1.0636223134723544
2023-04-09 00:04:04,025 - INFO - training.closure0 - iteration 68: loss = -1.0695581097161275
2023-04-09 00:04:04,041 - INFO - training.closure0 - iteration 69: loss = -1.0876848690728478
2023-04-09 00:04:04,056 - INFO - training.closure0 - iteration 70: loss = -1.2078753179658284
2023-04-09 00:04:04,072 - INFO - training.closure0 - iteration 71: loss = 5.71502244241559
2023-04-09 00:04:04,088 - INFO - training.closure0 - iteration 72: loss = -1.4205727403290385
2023-04-09 00:04:04,104 - INFO - training.closure0 - iteration 73: loss = -1.45431599933233
2023-04-09 00:04:04,119 - INFO - training.closure0 - iteration 74: loss = 233.3206208274132
2023-04-09 00:04:04,135 - INFO - training.closure0 - iteration 75: loss = 5.803737007300301
2023-04-09 00:04:04,150 - INFO - training.closure0 - iteration 76: loss = -1.3893918072954603
2023-04-09 00:04:04,166 - INFO - training.closure0 - iteration 77: loss = -1.464827374630404
2023-04-09 00:04:04,182 - INFO - training.closure0 - iteration 78: loss = -1.7557127138034583
2023-04-09 00:04:04,198 - INFO - training.closure0 - iteration 79: loss = 4.7236505670026805
2023-04-09 00:04:04,213 - INFO - training.closure0 - iteration 80: loss = -1.7281801135862127
2023-04-09 00:04:04,229 - INFO - training.closure0 - iteration 81: loss = -1.8353641501573479
2023-04-09 00:04:04,245 - INFO - training.closure0 - iteration 82: loss = 43965.85885801062
2023-04-09 00:04:04,260 - INFO - training.closure0 - iteration 83: loss = 280.0129277621391
2023-04-09 00:04:04,276 - INFO - training.closure0 - iteration 84: loss = -1.000246890576806
2023-04-09 00:04:04,292 - INFO - training.closure0 - iteration 85: loss = -2.828269408945752
2023-04-09 00:04:04,308 - INFO - training.closure0 - iteration 86: loss = -2.9188599143652083
2023-04-09 00:04:04,323 - INFO - training.closure0 - iteration 87: loss = -3.255472459482089
2023-04-09 00:04:04,339 - INFO - training.closure0 - iteration 88: loss = 87923.81043951848
2023-04-09 00:04:04,354 - INFO - training.closure0 - iteration 89: loss = 1025.878472191208
2023-04-09 00:04:04,370 - INFO - training.closure0 - iteration 90: loss = 24.504303991651412
2023-04-09 00:04:04,386 - INFO - training.closure0 - iteration 91: loss = -2.410387486831401
2023-04-09 00:04:04,403 - INFO - training.closure0 - iteration 92: loss = -3.287649880232321
2023-04-09 00:04:04,418 - INFO - training.closure0 - iteration 93: loss = -3.614580402885463
2023-04-09 00:04:04,418 - INFO - training.closure0 - iteration 94: loss = 392056.4248936783
2023-04-09 00:04:04,433 - INFO - training.closure0 - iteration 95: loss = 2585.0617976629437
2023-04-09 00:04:04,449 - INFO - training.closure0 - iteration 96: loss = 42.04844033415769
2023-04-09 00:04:04,464 - INFO - training.closure0 - iteration 97: loss = -2.6794273943888185
2023-04-09 00:04:04,480 - INFO - training.closure0 - iteration 98: loss = -3.8016706515857197
2023-04-09 00:04:04,498 - INFO - training.closure0 - iteration 99: loss = -3.870722682172701
2023-04-09 00:04:04,512 - INFO - training.closure0 - iteration 100: loss = -4.257665376713971
2023-04-09 00:04:04,527 - INFO - training.closure0 - iteration 101: loss = 449.8818532955394
2023-04-09 00:04:04,543 - INFO - training.closure0 - iteration 102: loss = -3.3766414109396203
2023-04-09 00:04:04,559 - INFO - training.closure0 - iteration 103: loss = -4.350379786969842
2023-04-09 00:04:04,574 - INFO - training.closure0 - iteration 104: loss = 17509.31571775199
2023-04-09 00:04:04,590 - INFO - training.closure0 - iteration 105: loss = 184.65667419866247
2023-04-09 00:04:04,606 - INFO - training.closure0 - iteration 106: loss = -1.8566337802352484
2023-04-09 00:04:04,621 - INFO - training.closure0 - iteration 107: loss = -4.758778486587115
2023-04-09 00:04:04,637 - INFO - training.closure0 - iteration 108: loss = -4.964899659995512
2023-04-09 00:04:04,653 - INFO - training.closure0 - iteration 109: loss = -2.7506331675141897
2023-04-09 00:04:04,668 - INFO - training.closure0 - iteration 110: loss = -5.040910054986649
2023-04-09 00:04:04,684 - INFO - training.closure0 - iteration 111: loss = -5.1063034739329485
2023-04-09 00:04:04,699 - INFO - training.closure0 - iteration 112: loss = -5.092117879883231
2023-04-09 00:04:04,700 - INFO - training.closure0 - iteration 113: loss = -5.153371639745912
2023-04-09 00:04:04,716 - INFO - training.closure0 - iteration 114: loss = -5.156566134461398
2023-04-09 00:04:04,731 - INFO - training.closure0 - iteration 115: loss = -5.158692317632179
2023-04-09 00:04:04,747 - INFO - training.closure0 - iteration 116: loss = -5.1638741428202835
2023-04-09 00:04:04,763 - INFO - training.closure0 - iteration 117: loss = -5.188250386861705
2023-04-09 00:04:04,778 - INFO - training.closure0 - iteration 118: loss = -5.248378891430946
2023-04-09 00:04:04,799 - INFO - training.closure0 - iteration 119: loss = -5.395304163169609
2023-04-09 00:04:04,810 - INFO - training.closure0 - iteration 120: loss = 331.6562416061434
2023-04-09 00:04:04,826 - INFO - training.closure0 - iteration 121: loss = -2.412206281309785
2023-04-09 00:04:04,841 - INFO - training.closure0 - iteration 122: loss = -5.856611993892109
2023-04-09 00:04:04,857 - INFO - training.closure0 - iteration 123: loss = -4.926376042941428
2023-04-09 00:04:04,857 - INFO - training.closure0 - iteration 124: loss = -5.872732166935609
2023-04-09 00:04:04,888 - INFO - training.closure0 - iteration 125: loss = -5.575498717640249
2023-04-09 00:04:04,904 - INFO - training.closure0 - iteration 126: loss = -5.889680627391797
2023-04-09 00:04:04,920 - INFO - training.closure0 - iteration 127: loss = -5.925701581353011
2023-04-09 00:04:04,935 - INFO - training.closure0 - iteration 128: loss = -6.198673272188036
2023-04-09 00:04:04,951 - INFO - training.closure0 - iteration 129: loss = -6.295212767893112
2023-04-09 00:04:04,967 - INFO - training.closure0 - iteration 130: loss = -6.442697261519253
2023-04-09 00:04:04,983 - INFO - training.closure0 - iteration 131: loss = -6.4526513648167425
2023-04-09 00:04:04,999 - INFO - training.closure0 - iteration 132: loss = -6.45605246305292
2023-04-09 00:04:05,014 - INFO - training.closure0 - iteration 133: loss = -6.4567028485971525
2023-04-09 00:04:05,030 - INFO - training.closure0 - iteration 134: loss = -6.456825864862626
2023-04-09 00:04:05,045 - INFO - training.closure0 - iteration 135: loss = -6.456909894973334
2023-04-09 00:04:05,061 - INFO - training.closure0 - iteration 136: loss = -6.456910698990779
2023-04-09 00:04:05,077 - INFO - training.closure0 - iteration 137: loss = -6.456911675851854
2023-04-09 00:04:05,092 - INFO - training.closure0 - iteration 138: loss = -6.456911675911181
2023-04-09 00:04:05,108 - INFO - training.closure0 - iteration 139: loss = -6.456911675911378
2023-04-09 00:04:05,124 - INFO - training.pre_train_full - a0 mean: [2.99944253 3.00062146]
2023-04-09 00:04:05,124 - INFO - training.pre_train_full - a0 var: [1.00409417e-04 8.44174480e-05]
2023-04-09 00:04:05,124 - INFO - training.pre_train_full - a0 covar: [[0.00010040941709807073, -5.5086862855826346e-06], [-5.5086862855826346e-06, 8.441744802792219e-05]]
2023-04-09 00:04:08,466 - INFO - training.closure - iteration 0: loss = 346425.59336108825
2023-04-09 00:04:12,116 - INFO - training.closure - iteration 1: loss = 26288.673757862627
2023-04-09 00:04:16,110 - INFO - training.closure - iteration 2: loss = 17192.308982619066
2023-04-09 00:04:23,011 - INFO - training.closure - iteration 3: loss = 11355.30047309038
2023-04-09 00:04:29,416 - INFO - training.closure - iteration 4: loss = 5739.383715704935
2023-04-09 00:04:34,590 - INFO - training.closure - iteration 5: loss = 2677.2831947969885
2023-04-09 00:04:39,878 - INFO - training.closure - iteration 6: loss = 1005.9925978376239
2023-04-09 00:04:43,958 - INFO - training.closure - iteration 7: loss = 325.4981413014355
2023-04-09 00:04:47,680 - INFO - training.closure - iteration 8: loss = 119.99084604564206
2023-04-09 00:04:51,634 - INFO - training.closure - iteration 9: loss = 53.21268169818045
2023-04-09 00:04:55,361 - INFO - training.closure - iteration 10: loss = 28.973807038363383
2023-04-09 00:04:59,138 - INFO - training.closure - iteration 11: loss = 19.468790099259024
2023-04-09 00:05:02,911 - INFO - training.closure - iteration 12: loss = 12.645731951318291
2023-04-09 00:05:06,712 - INFO - training.closure - iteration 13: loss = 7.841729614626517
2023-04-09 00:05:10,509 - INFO - training.closure - iteration 14: loss = 5.673217832533567
2023-04-09 00:05:14,293 - INFO - training.closure - iteration 15: loss = 4.766195049830166
2023-04-09 00:05:17,978 - INFO - training.closure - iteration 16: loss = 3.5263364539649116
2023-04-09 00:05:21,744 - INFO - training.closure - iteration 17: loss = 2.2859162598757656
2023-04-09 00:05:25,449 - INFO - training.closure - iteration 18: loss = 1.2502861686060616
2023-04-09 00:05:29,237 - INFO - training.closure - iteration 19: loss = 0.4378937298142631
2023-04-09 00:05:32,938 - INFO - training.closure - iteration 20: loss = 0.021869502404579855
2023-04-09 00:05:36,801 - INFO - training.closure - iteration 21: loss = -0.3719759614327929
2023-04-09 00:05:40,582 - INFO - training.closure - iteration 22: loss = -0.8685414641485485
2023-04-09 00:05:44,332 - INFO - training.closure - iteration 23: loss = -1.039604017705063
2023-04-09 00:05:48,196 - INFO - training.closure - iteration 24: loss = -1.1964387703651784
2023-04-09 00:05:51,917 - INFO - training.closure - iteration 25: loss = -1.5483228862789513
2023-04-09 00:05:55,686 - INFO - training.closure - iteration 26: loss = -1.8655504626069943
2023-04-09 00:05:59,673 - INFO - training.closure - iteration 27: loss = -2.2127628758171403
2023-04-09 00:06:03,376 - INFO - training.closure - iteration 28: loss = -2.3729512211282433
2023-04-09 00:06:07,353 - INFO - training.closure - iteration 29: loss = -2.420717094051596
2023-04-09 00:06:11,042 - INFO - training.closure - iteration 30: loss = -2.4339193153904226
2023-04-09 00:06:14,918 - INFO - training.closure - iteration 31: loss = -2.4418467203017986
2023-04-09 00:06:18,780 - INFO - training.closure - iteration 32: loss = -2.455991085596438
2023-04-09 00:06:22,452 - INFO - training.closure - iteration 33: loss = -2.484946598546454
2023-04-09 00:06:26,173 - INFO - training.closure - iteration 34: loss = -2.543545457756507
2023-04-09 00:06:29,890 - INFO - training.closure - iteration 35: loss = -2.616992542431489
2023-04-09 00:06:33,694 - INFO - training.closure - iteration 36: loss = -2.710242154398354
2023-04-09 00:06:37,520 - INFO - training.closure - iteration 37: loss = -2.7669143047842626
2023-04-09 00:06:41,247 - INFO - training.closure - iteration 38: loss = -2.7830501011923796
2023-04-09 00:06:44,925 - INFO - training.closure - iteration 39: loss = -2.799951593114917
2023-04-09 00:06:48,655 - INFO - training.closure - iteration 40: loss = -2.818233641816418
2023-04-09 00:06:52,449 - INFO - training.closure - iteration 41: loss = -2.832430279122781
2023-04-09 00:06:56,482 - INFO - training.closure - iteration 42: loss = -2.8604143958683856
2023-04-09 00:07:00,279 - INFO - training.closure - iteration 43: loss = -2.935350157245116
2023-04-09 00:07:04,145 - INFO - training.closure - iteration 44: loss = -3.028123487236635
2023-04-09 00:07:08,020 - INFO - training.closure - iteration 45: loss = -3.124336799023313
2023-04-09 00:07:11,839 - INFO - training.closure - iteration 46: loss = -3.2492584835975933
2023-04-09 00:07:15,565 - INFO - training.closure - iteration 47: loss = -3.2841320136285903
2023-04-09 00:07:19,310 - INFO - training.closure - iteration 48: loss = -3.3074274706962066
2023-04-09 00:07:23,115 - INFO - training.closure - iteration 49: loss = -3.3253102174235547
2023-04-09 00:07:26,848 - INFO - training.closure - iteration 50: loss = -3.3529727417272523
2023-04-09 00:07:30,583 - INFO - training.closure - iteration 51: loss = -3.3668934567812348
2023-04-09 00:07:34,318 - INFO - training.closure - iteration 52: loss = -3.378985588902793
2023-04-09 00:07:38,124 - INFO - training.closure - iteration 53: loss = -3.387646488893033
2023-04-09 00:07:42,148 - INFO - training.closure - iteration 54: loss = -3.401588771396445
2023-04-09 00:07:46,072 - INFO - training.closure - iteration 55: loss = -3.4179103050313113
2023-04-09 00:07:49,882 - INFO - training.closure - iteration 56: loss = -3.446237955193845
2023-04-09 00:07:53,611 - INFO - training.closure - iteration 57: loss = -3.4557181024418355
2023-04-09 00:07:57,421 - INFO - training.closure - iteration 58: loss = -3.4665622360989654
2023-04-09 00:08:01,158 - INFO - training.closure - iteration 59: loss = -3.471073716372285
2023-04-09 00:08:04,984 - INFO - training.closure - iteration 60: loss = -3.474509738770547
2023-04-09 00:08:08,844 - INFO - training.closure - iteration 61: loss = -3.4788177002049863
2023-04-09 00:08:12,653 - INFO - training.closure - iteration 62: loss = -3.4869505986355884
2023-04-09 00:08:16,370 - INFO - training.closure - iteration 63: loss = -3.509149993967354
2023-04-09 00:08:20,465 - INFO - training.closure - iteration 64: loss = -3.5593390084368277
2023-04-09 00:08:24,633 - INFO - training.closure - iteration 65: loss = -3.611483798640232
2023-04-09 00:08:28,613 - INFO - training.closure - iteration 66: loss = -3.6730601193725323
2023-04-09 00:08:32,345 - INFO - training.closure - iteration 67: loss = -3.7308055489506584
2023-04-09 00:08:36,074 - INFO - training.closure - iteration 68: loss = -3.8378107474102583
2023-04-09 00:08:40,085 - INFO - training.closure - iteration 69: loss = 186.82659360815
2023-04-09 00:08:43,963 - INFO - training.closure - iteration 70: loss = -0.8611969370475157
2023-04-09 00:08:47,930 - INFO - training.closure - iteration 71: loss = -3.9783913570661626
2023-04-09 00:08:51,666 - INFO - training.closure - iteration 72: loss = -3.997995704497874
2023-04-09 00:08:55,665 - INFO - training.closure - iteration 73: loss = 527.1384620482659
2023-04-09 00:08:59,837 - INFO - training.closure - iteration 74: loss = 7.823464265640904
2023-04-09 00:09:04,011 - INFO - training.closure - iteration 75: loss = -3.771304355428142
2023-04-09 00:09:07,740 - INFO - training.closure - iteration 76: loss = -4.073822326199068
2023-04-09 00:09:11,436 - INFO - training.closure - iteration 77: loss = -1.5559643521278095
2023-04-09 00:09:15,320 - INFO - training.closure - iteration 78: loss = -4.222617265577394
2023-04-09 00:09:19,174 - INFO - training.closure - iteration 79: loss = -4.539029070220855
2023-04-09 00:09:22,897 - INFO - training.closure - iteration 80: loss = -4.67843192997167
2023-04-09 00:09:26,729 - INFO - training.closure - iteration 81: loss = -4.956241654237268
2023-04-09 00:09:30,506 - INFO - training.closure - iteration 82: loss = -4.447060155242858
2023-04-09 00:09:34,290 - INFO - training.closure - iteration 83: loss = -5.033693567526654
2023-04-09 00:09:38,540 - INFO - training.closure - iteration 84: loss = -0.7961956545611857
2023-04-09 00:09:42,655 - INFO - training.closure - iteration 85: loss = -5.086686890294147
2023-04-09 00:09:46,722 - INFO - training.closure - iteration 86: loss = -5.174860921435179
2023-04-09 00:09:50,730 - INFO - training.closure - iteration 87: loss = -5.227994012998574
2023-04-09 00:09:54,849 - INFO - training.closure - iteration 88: loss = -5.3082360471185694
2023-04-09 00:09:58,903 - INFO - training.closure - iteration 89: loss = -5.48049466297356
2023-04-09 00:10:02,871 - INFO - training.closure - iteration 90: loss = -5.63552053195563
2023-04-09 00:10:07,122 - INFO - training.closure - iteration 91: loss = -5.902185105506314
2023-04-09 00:10:11,467 - INFO - training.closure - iteration 92: loss = 26.179444085696733
2023-04-09 00:10:15,807 - INFO - training.closure - iteration 93: loss = -5.387760345003899
2023-04-09 00:10:19,766 - INFO - training.closure - iteration 94: loss = -6.067224151982229
2023-04-09 00:10:23,536 - INFO - training.closure - iteration 95: loss = -4.231556945755802
2023-04-09 00:10:27,308 - INFO - training.closure - iteration 96: loss = -6.363154905765096
2023-04-09 00:10:31,065 - INFO - training.closure - iteration 97: loss = -5.265303832551967
2023-04-09 00:10:34,805 - INFO - training.closure - iteration 98: loss = -6.506265420503736
2023-04-09 00:10:38,580 - INFO - training.closure - iteration 99: loss = -6.580533741477265
2023-04-09 00:10:42,278 - INFO - training.closure - iteration 100: loss = -6.609587427322117
2023-04-09 00:10:46,169 - INFO - training.closure - iteration 101: loss = -6.640673308279854
2023-04-09 00:10:50,015 - INFO - training.closure - iteration 102: loss = -6.650949191162964
2023-04-09 00:10:53,824 - INFO - training.closure - iteration 103: loss = -6.665124400971213
2023-04-09 00:10:57,781 - INFO - training.closure - iteration 104: loss = -6.75866338517111
2023-04-09 00:11:01,675 - INFO - training.closure - iteration 105: loss = -6.921195932164138
2023-04-09 00:11:05,482 - INFO - training.closure - iteration 106: loss = -7.016400401935297
2023-04-09 00:11:09,292 - INFO - training.closure - iteration 107: loss = -7.02995789491025
2023-04-09 00:11:13,098 - INFO - training.closure - iteration 108: loss = -7.129445003869587
2023-04-09 00:11:17,068 - INFO - training.closure - iteration 109: loss = -7.187953660427098
2023-04-09 00:11:20,819 - INFO - training.closure - iteration 110: loss = -7.241143912383469
2023-04-09 00:11:24,586 - INFO - training.closure - iteration 111: loss = -7.256210311795099
2023-04-09 00:11:28,397 - INFO - training.closure - iteration 112: loss = -7.287555785272085
2023-04-09 00:11:32,220 - INFO - training.closure - iteration 113: loss = -7.350073821805274
2023-04-09 00:11:36,108 - INFO - training.closure - iteration 114: loss = -7.4600046803095985
2023-04-09 00:11:40,008 - INFO - training.closure - iteration 115: loss = -7.643408112502899
2023-04-09 00:11:43,812 - INFO - training.closure - iteration 116: loss = -7.755346429382262
2023-04-09 00:11:47,628 - INFO - training.closure - iteration 117: loss = -7.802573913137148
2023-04-09 00:11:51,524 - INFO - training.closure - iteration 118: loss = -7.7782097434262765
2023-04-09 00:11:55,339 - INFO - training.closure - iteration 119: loss = -7.862474164246829
2023-04-09 00:11:59,160 - INFO - training.closure - iteration 120: loss = -7.876318911779751
2023-04-09 00:12:02,984 - INFO - training.closure - iteration 121: loss = -7.895203331466417
2023-04-09 00:12:06,877 - INFO - training.closure - iteration 122: loss = -7.901380794428627
2023-04-09 00:12:10,693 - INFO - training.closure - iteration 123: loss = -7.9068303099838495
2023-04-09 00:12:14,510 - INFO - training.closure - iteration 124: loss = -7.913090186415213
2023-04-09 00:12:18,412 - INFO - training.closure - iteration 125: loss = -7.9214707253943
2023-04-09 00:12:22,420 - INFO - training.closure - iteration 126: loss = -7.937381974095777
2023-04-09 00:12:26,308 - INFO - training.closure - iteration 127: loss = -7.541341948562596
2023-04-09 00:12:30,154 - INFO - training.closure - iteration 128: loss = -7.948060483184546
2023-04-09 00:12:34,151 - INFO - training.closure - iteration 129: loss = -7.994281097919501
2023-04-09 00:12:37,966 - INFO - training.closure - iteration 130: loss = -8.051560798155025
2023-04-09 00:12:41,859 - INFO - training.closure - iteration 131: loss = 146.49026691796533
2023-04-09 00:12:45,668 - INFO - training.closure - iteration 132: loss = -5.315322227757099
2023-04-09 00:12:49,581 - INFO - training.closure - iteration 133: loss = -8.066904957171143
2023-04-09 00:12:53,391 - INFO - training.closure - iteration 134: loss = -8.140293998132515
2023-04-09 00:12:57,288 - INFO - training.closure - iteration 135: loss = -8.168544537056762
2023-04-09 00:13:01,104 - INFO - training.closure - iteration 136: loss = -8.185055548328478
2023-04-09 00:13:04,916 - INFO - training.closure - iteration 137: loss = -8.195484049838058
2023-04-09 00:13:08,728 - INFO - training.closure - iteration 138: loss = -8.203343680138328
2023-04-09 00:13:12,616 - INFO - training.closure - iteration 139: loss = -8.218654085956864
2023-04-09 00:13:16,462 - INFO - training.closure - iteration 140: loss = -8.24418092631187
2023-04-09 00:13:20,322 - INFO - training.closure - iteration 141: loss = -8.278083938319378
2023-04-09 00:13:24,134 - INFO - training.closure - iteration 142: loss = -8.34366009957751
2023-04-09 00:13:27,942 - INFO - training.closure - iteration 143: loss = -8.459273338341582
2023-04-09 00:13:31,833 - INFO - training.closure - iteration 144: loss = -8.078906711188896
2023-04-09 00:13:35,647 - INFO - training.closure - iteration 145: loss = -8.50169623846925
2023-04-09 00:13:39,487 - INFO - training.closure - iteration 146: loss = -8.56231436487562
2023-04-09 00:13:43,203 - INFO - training.closure - iteration 147: loss = -8.491061320600355
2023-04-09 00:13:47,093 - INFO - training.closure - iteration 148: loss = -8.606077217309078
2023-04-09 00:13:50,908 - INFO - training.closure - iteration 149: loss = -8.662329363522574
2023-04-09 00:13:54,720 - INFO - training.closure - iteration 150: loss = -8.731163810228422
2023-04-09 00:13:58,534 - INFO - training.closure - iteration 151: loss = -8.759127512506563
2023-04-09 00:14:02,433 - INFO - training.closure - iteration 152: loss = -8.779165555069397
2023-04-09 00:14:06,248 - INFO - training.closure - iteration 153: loss = -8.838914957205763
2023-04-09 00:14:10,055 - INFO - training.closure - iteration 154: loss = -8.874432339329156
2023-04-09 00:14:13,869 - INFO - training.closure - iteration 155: loss = -8.921397211348866
2023-04-09 00:14:17,682 - INFO - training.closure - iteration 156: loss = -9.037091412177478
2023-04-09 00:14:21,566 - INFO - training.closure - iteration 157: loss = -9.111377519672168
2023-04-09 00:14:25,379 - INFO - training.closure - iteration 158: loss = -9.233901297158923
2023-04-09 00:14:29,184 - INFO - training.closure - iteration 159: loss = -9.105908922679532
2023-04-09 00:14:33,029 - INFO - training.closure - iteration 160: loss = -9.287796771203135
2023-04-09 00:14:36,920 - INFO - training.closure - iteration 161: loss = -9.323632167154791
2023-04-09 00:14:40,752 - INFO - training.closure - iteration 162: loss = -9.366840998961717
2023-04-09 00:14:44,612 - INFO - training.closure - iteration 163: loss = -9.363263858920092
2023-04-09 00:14:48,633 - INFO - training.closure - iteration 164: loss = -9.391979733168707
2023-04-09 00:14:52,628 - INFO - training.closure - iteration 165: loss = -9.396896826545905
2023-04-09 00:14:56,450 - INFO - training.closure - iteration 166: loss = -9.406493276717622
2023-04-09 00:15:00,269 - INFO - training.closure - iteration 167: loss = -9.423229996967923
2023-04-09 00:15:04,028 - INFO - training.closure - iteration 168: loss = -9.429838939936946
2023-04-09 00:15:07,951 - INFO - training.closure - iteration 169: loss = -9.438845381074003
2023-04-09 00:15:11,827 - INFO - training.closure - iteration 170: loss = -9.491003380743278
2023-04-09 00:15:15,667 - INFO - training.closure - iteration 171: loss = -9.568537015213732
2023-04-09 00:15:19,484 - INFO - training.closure - iteration 172: loss = -9.421777640020725
2023-04-09 00:15:23,300 - INFO - training.closure - iteration 173: loss = -9.595966978965135
2023-04-09 00:15:27,194 - INFO - training.closure - iteration 174: loss = -9.614431160916837
2023-04-09 00:15:31,018 - INFO - training.closure - iteration 175: loss = -9.642434651936934
2023-04-09 00:15:34,833 - INFO - training.closure - iteration 176: loss = -9.6700142240086
2023-04-09 00:15:38,717 - INFO - training.closure - iteration 177: loss = -9.697023313383031
2023-04-09 00:15:42,611 - INFO - training.closure - iteration 178: loss = -9.70885606523028
2023-04-09 00:15:46,453 - INFO - training.closure - iteration 179: loss = -9.724059039765933
2023-04-09 00:15:50,270 - INFO - training.closure - iteration 180: loss = -9.736861217724798
2023-04-09 00:15:54,034 - INFO - training.closure - iteration 181: loss = -9.750849839528327
2023-04-09 00:15:58,027 - INFO - training.closure - iteration 182: loss = -9.786427507646998
2023-04-09 00:16:01,835 - INFO - training.closure - iteration 183: loss = -9.814004288748684
2023-04-09 00:16:05,863 - INFO - training.closure - iteration 184: loss = -9.842928532664997
2023-04-09 00:16:09,734 - INFO - training.closure - iteration 185: loss = -9.87847403235092
2023-04-09 00:16:13,721 - INFO - training.closure - iteration 186: loss = -9.897314256323684
2023-04-09 00:16:17,554 - INFO - training.closure - iteration 187: loss = -9.927941836205136
2023-04-09 00:16:21,305 - INFO - training.closure - iteration 188: loss = -9.94155750491271
2023-04-09 00:16:25,134 - INFO - training.closure - iteration 189: loss = -9.985685816451047
2023-04-09 00:16:28,950 - INFO - training.closure - iteration 190: loss = -9.803688466020184
2023-04-09 00:16:32,846 - INFO - training.closure - iteration 191: loss = -9.997538066197627
2023-04-09 00:16:36,663 - INFO - training.closure - iteration 192: loss = -10.033409822356312
2023-04-09 00:16:40,478 - INFO - training.closure - iteration 193: loss = -10.054284032963238
2023-04-09 00:16:44,293 - INFO - training.closure - iteration 194: loss = -10.066213476458106
2023-04-09 00:16:48,143 - INFO - training.closure - iteration 195: loss = -10.09142730882797
2023-04-09 00:16:51,955 - INFO - training.closure - iteration 196: loss = -10.110371369800948
2023-04-09 00:16:55,757 - INFO - training.closure - iteration 197: loss = -10.128394155339983
2023-04-09 00:16:59,616 - INFO - training.closure - iteration 198: loss = -10.140975261090157
2023-04-09 00:17:03,513 - INFO - training.closure - iteration 199: loss = -10.155050933635742
2023-04-09 00:17:07,314 - INFO - training.closure - iteration 200: loss = -10.165631625975191
2023-04-09 00:17:11,168 - INFO - training.closure - iteration 201: loss = -10.181017248805574
2023-04-09 00:17:14,986 - INFO - training.closure - iteration 202: loss = -10.175500233871567
2023-04-09 00:17:18,797 - INFO - training.closure - iteration 203: loss = -10.190090106818397
2023-04-09 00:17:22,698 - INFO - training.closure - iteration 204: loss = -10.207200664991086
2023-04-09 00:17:26,508 - INFO - training.closure - iteration 205: loss = -10.22210662379536
2023-04-09 00:17:30,326 - INFO - training.closure - iteration 206: loss = -10.236653805301504
2023-04-09 00:17:34,136 - INFO - training.closure - iteration 207: loss = -10.238655698395608
2023-04-09 00:17:38,034 - INFO - training.closure - iteration 208: loss = -10.23247179437276
2023-04-09 00:17:41,988 - INFO - training.closure - iteration 209: loss = -10.243687847166921
2023-04-09 00:17:45,833 - INFO - training.closure - iteration 210: loss = -10.248173092286548
2023-04-09 00:17:49,642 - INFO - training.closure - iteration 211: loss = -10.2523545725211
2023-04-09 00:17:53,538 - INFO - training.closure - iteration 212: loss = -10.259483562594312
2023-04-09 00:17:57,352 - INFO - training.closure - iteration 213: loss = -10.267210321894929
2023-04-09 00:18:01,173 - INFO - training.closure - iteration 214: loss = -10.277177338038355
2023-04-09 00:18:04,988 - INFO - training.closure - iteration 215: loss = -10.283576566691476
2023-04-09 00:18:08,806 - INFO - training.closure - iteration 216: loss = -10.288598171863423
2023-04-09 00:18:12,699 - INFO - training.closure - iteration 217: loss = -10.301703032944346
2023-04-09 00:18:16,521 - INFO - training.closure - iteration 218: loss = -10.331559112845497
2023-04-09 00:18:20,333 - INFO - training.closure - iteration 219: loss = -10.373245115392358
2023-04-09 00:18:24,248 - INFO - training.closure - iteration 220: loss = -10.360373799211695
2023-04-09 00:18:28,063 - INFO - training.closure - iteration 221: loss = -10.430053555185609
2023-04-09 00:18:31,880 - INFO - training.closure - iteration 222: loss = -10.517003612461105
2023-04-09 00:18:35,697 - INFO - training.closure - iteration 223: loss = -10.575466067081063
2023-04-09 00:18:39,514 - INFO - training.closure - iteration 224: loss = -10.625338292866434
2023-04-09 00:18:43,421 - INFO - training.closure - iteration 225: loss = -10.596989362367427
2023-04-09 00:18:47,298 - INFO - training.closure - iteration 226: loss = -10.658088577572936
2023-04-09 00:18:51,118 - INFO - training.closure - iteration 227: loss = -10.686691460609444
2023-04-09 00:18:54,935 - INFO - training.closure - iteration 228: loss = -10.742374927506873
2023-04-09 00:18:58,917 - INFO - training.closure - iteration 229: loss = -10.758253040935003
2023-04-09 00:19:02,918 - INFO - training.closure - iteration 230: loss = -10.795843039963145
2023-04-09 00:19:06,779 - INFO - training.closure - iteration 231: loss = -10.802471724007606
2023-04-09 00:19:10,727 - INFO - training.closure - iteration 232: loss = -10.824713214142875
2023-04-09 00:19:15,458 - INFO - training.closure - iteration 233: loss = -10.830053485598853
2023-04-09 00:19:19,831 - INFO - training.closure - iteration 234: loss = -10.835874967467113
2023-04-09 00:19:23,984 - INFO - training.closure - iteration 235: loss = -10.840123310997239
2023-04-09 00:19:28,248 - INFO - training.closure - iteration 236: loss = -10.842917621266103
2023-04-09 00:19:32,396 - INFO - training.closure - iteration 237: loss = -10.845137687710306
2023-04-09 00:19:36,298 - INFO - training.closure - iteration 238: loss = -10.849458966890921
2023-04-09 00:19:40,123 - INFO - training.closure - iteration 239: loss = -10.853840254817342
2023-04-09 00:19:43,926 - INFO - training.closure - iteration 240: loss = -10.857836517384593
2023-04-09 00:19:47,658 - INFO - training.closure - iteration 241: loss = -10.860012161830177
2023-04-09 00:19:51,562 - INFO - training.closure - iteration 242: loss = -10.8612810560488
2023-04-09 00:19:55,413 - INFO - training.closure - iteration 243: loss = -10.863452534295375
2023-04-09 00:19:59,226 - INFO - training.closure - iteration 244: loss = -10.869620082027524
2023-04-09 00:20:03,086 - INFO - training.closure - iteration 245: loss = -10.880026899376727
2023-04-09 00:20:06,905 - INFO - training.closure - iteration 246: loss = -10.879293990957247
2023-04-09 00:20:10,802 - INFO - training.closure - iteration 247: loss = -10.890317293301575
2023-04-09 00:20:14,667 - INFO - training.closure - iteration 248: loss = -10.901023501972686
2023-04-09 00:20:18,490 - INFO - training.closure - iteration 249: loss = -10.914445044188751
2023-04-09 00:20:22,324 - INFO - training.closure - iteration 250: loss = -10.915270821388884
2023-04-09 00:20:26,142 - INFO - training.closure - iteration 251: loss = -10.920943059214508
2023-04-09 00:20:29,971 - INFO - training.closure - iteration 252: loss = -10.92770692701889
2023-04-09 00:20:33,802 - INFO - training.closure - iteration 253: loss = -10.941975357936135
2023-04-09 00:20:37,689 - INFO - training.closure - iteration 254: loss = -10.950873042655031
2023-04-09 00:20:41,594 - INFO - training.closure - iteration 255: loss = -10.9780988977714
2023-04-09 00:20:45,504 - INFO - training.closure - iteration 256: loss = -11.00473867208138
2023-04-09 00:20:49,317 - INFO - training.closure - iteration 257: loss = -10.873267089195881
2023-04-09 00:20:53,126 - INFO - training.closure - iteration 258: loss = -11.020593441392288
2023-04-09 00:20:57,100 - INFO - training.closure - iteration 259: loss = -11.01031502334428
2023-04-09 00:21:00,989 - INFO - training.closure - iteration 260: loss = -11.049594363023509
2023-04-09 00:21:04,805 - INFO - training.closure - iteration 261: loss = -11.078930325376412
2023-04-09 00:21:08,617 - INFO - training.closure - iteration 262: loss = -11.125856323907307
2023-04-09 00:21:12,429 - INFO - training.closure - iteration 263: loss = -10.878405365860477
2023-04-09 00:21:16,359 - INFO - training.closure - iteration 264: loss = -11.151660189547083
2023-04-09 00:21:20,173 - INFO - training.closure - iteration 265: loss = -11.204677081390763
2023-04-09 00:21:23,986 - INFO - training.closure - iteration 266: loss = -11.255468697159085
2023-04-09 00:21:27,861 - INFO - training.closure - iteration 267: loss = -11.107376821189453
2023-04-09 00:21:31,707 - INFO - training.closure - iteration 268: loss = -11.282484803696747
2023-04-09 00:21:35,547 - INFO - training.closure - iteration 269: loss = -11.294265492717575
2023-04-09 00:21:39,364 - INFO - training.closure - iteration 270: loss = -11.304827636883763
2023-04-09 00:21:43,180 - INFO - training.closure - iteration 271: loss = -11.314830642079404
2023-04-09 00:21:47,084 - INFO - training.closure - iteration 272: loss = -11.332691186020298
2023-04-09 00:21:50,925 - INFO - training.closure - iteration 273: loss = -11.356076160507158
2023-04-09 00:21:54,727 - INFO - training.closure - iteration 274: loss = -11.38073264524854
2023-04-09 00:21:58,544 - INFO - training.closure - iteration 275: loss = -11.40613456305232
2023-04-09 00:22:02,368 - INFO - training.closure - iteration 276: loss = -11.441137004733882
2023-04-09 00:22:06,267 - INFO - training.closure - iteration 277: loss = -11.464111518667146
2023-04-09 00:22:10,088 - INFO - training.closure - iteration 278: loss = -11.477277723148198
2023-04-09 00:22:13,905 - INFO - training.closure - iteration 279: loss = -11.494243646884488
2023-04-09 00:22:17,700 - INFO - training.closure - iteration 280: loss = -11.501609822003612
2023-04-09 00:22:21,568 - INFO - training.closure - iteration 281: loss = -11.510022953228457
2023-04-09 00:22:25,377 - INFO - training.closure - iteration 282: loss = -11.521454091546925
2023-04-09 00:22:29,191 - INFO - training.closure - iteration 283: loss = -11.534740411684126
2023-04-09 00:22:33,015 - INFO - training.closure - iteration 284: loss = -11.54785336671473
2023-04-09 00:22:36,872 - INFO - training.closure - iteration 285: loss = -11.53508342066295
2023-04-09 00:22:40,642 - INFO - training.closure - iteration 286: loss = -11.55194725159376
2023-04-09 00:22:44,379 - INFO - training.closure - iteration 287: loss = -11.556446090127306
2023-04-09 00:22:48,191 - INFO - training.closure - iteration 288: loss = -11.558921688187054
2023-04-09 00:22:52,004 - INFO - training.closure - iteration 289: loss = -11.565896067746579
2023-04-09 00:22:55,897 - INFO - training.closure - iteration 290: loss = -11.572790752621362
2023-04-09 00:22:59,717 - INFO - training.closure - iteration 291: loss = -11.579541912039861
2023-04-09 00:23:03,545 - INFO - training.closure - iteration 292: loss = -11.579966499867473
2023-04-09 00:23:07,364 - INFO - training.closure - iteration 293: loss = -11.58505775182595
2023-04-09 00:23:11,256 - INFO - training.closure - iteration 294: loss = -11.588328108329167
2023-04-09 00:23:15,092 - INFO - training.closure - iteration 295: loss = -11.59101952596661
2023-04-09 00:23:19,023 - INFO - training.closure - iteration 296: loss = -11.593420463307691
2023-04-09 00:23:22,838 - INFO - training.closure - iteration 297: loss = -11.595087244625114
2023-04-09 00:23:26,736 - INFO - training.closure - iteration 298: loss = -11.595999879286307
2023-04-09 00:23:30,554 - INFO - training.closure - iteration 299: loss = -11.596913072286254
2023-04-09 00:23:34,374 - INFO - training.closure - iteration 300: loss = -11.597805251751925
2023-04-09 00:23:38,193 - INFO - training.closure - iteration 301: loss = -11.598105974663271
2023-04-09 00:23:42,092 - INFO - training.closure - iteration 302: loss = -11.598154426797258
2023-04-09 00:23:46,114 - INFO - training.closure - iteration 303: loss = -11.5981714099285
2023-04-09 00:23:49,933 - INFO - training.closure - iteration 304: loss = -11.598214899036606
2023-04-09 00:23:53,752 - INFO - training.closure - iteration 305: loss = -11.598325230229015
2023-04-09 00:23:57,509 - INFO - training.closure - iteration 306: loss = -11.59858985344939
2023-04-09 00:24:01,398 - INFO - training.closure - iteration 307: loss = -11.59901511299709
2023-04-09 00:24:05,213 - INFO - training.closure - iteration 308: loss = -11.59929667428177
2023-04-09 00:24:09,058 - INFO - training.closure - iteration 309: loss = -11.599697306059308
2023-04-09 00:24:12,882 - INFO - training.closure - iteration 310: loss = -11.599905950105505
2023-04-09 00:24:16,784 - INFO - training.closure - iteration 311: loss = -11.60000286955567
2023-04-09 00:24:20,634 - INFO - training.closure - iteration 312: loss = -11.599805716291575
2023-04-09 00:24:24,466 - INFO - training.closure - iteration 313: loss = -11.600149218918133
2023-04-09 00:24:28,286 - INFO - training.closure - iteration 314: loss = -11.59887002720902
2023-04-09 00:24:32,187 - INFO - training.closure - iteration 315: loss = -11.600263808333459
2023-04-09 00:24:36,012 - INFO - training.closure - iteration 316: loss = -11.600467851841493
2023-04-09 00:24:39,830 - INFO - training.closure - iteration 317: loss = -11.601139792918694
2023-04-09 00:24:43,646 - INFO - training.closure - iteration 318: loss = -11.591825456382647
2023-04-09 00:24:47,787 - INFO - training.closure - iteration 319: loss = -11.601419937040411
2023-04-09 00:24:51,706 - INFO - training.closure - iteration 320: loss = -11.602128854725894
2023-04-09 00:24:55,598 - INFO - training.closure - iteration 321: loss = -11.602948535717616
2023-04-09 00:24:59,416 - INFO - training.closure - iteration 322: loss = -11.603102974229326
2023-04-09 00:25:03,279 - INFO - training.closure - iteration 323: loss = -11.603476286228375
2023-04-09 00:25:07,291 - INFO - training.closure - iteration 324: loss = -11.603890757267305
2023-04-09 00:25:11,110 - INFO - training.closure - iteration 325: loss = -11.604396270738668
2023-04-09 00:25:14,931 - INFO - training.closure - iteration 326: loss = -11.605341975852108
2023-04-09 00:25:18,784 - INFO - training.closure - iteration 327: loss = -11.606117917789073
2023-04-09 00:25:22,683 - INFO - training.closure - iteration 328: loss = -11.60763756773962
2023-04-09 00:25:26,471 - INFO - training.closure - iteration 329: loss = -11.610553117646894
2023-04-09 00:25:30,277 - INFO - training.closure - iteration 330: loss = -11.614375381372549
2023-04-09 00:25:34,097 - INFO - training.closure - iteration 331: loss = -11.617894393051081
2023-04-09 00:25:38,006 - INFO - training.closure - iteration 332: loss = -11.6199791130531
2023-04-09 00:25:41,850 - INFO - training.closure - iteration 333: loss = -11.62104304027796
2023-04-09 00:25:45,664 - INFO - training.closure - iteration 334: loss = -11.621578642467174
2023-04-09 00:25:49,675 - INFO - training.closure - iteration 335: loss = -11.619081073108482
2023-04-09 00:25:53,557 - INFO - training.closure - iteration 336: loss = -11.621744386298474
2023-04-09 00:25:57,446 - INFO - training.closure - iteration 337: loss = -11.622213844315613
2023-04-09 00:26:01,279 - INFO - training.closure - iteration 338: loss = -11.622557430221097
2023-04-09 00:26:05,105 - INFO - training.closure - iteration 339: loss = -11.622865522179342
2023-04-09 00:26:08,902 - INFO - training.closure - iteration 340: loss = -11.6232436770888
2023-04-09 00:26:12,714 - INFO - training.closure - iteration 341: loss = -11.54752764039306
2023-04-09 00:26:16,531 - INFO - training.closure - iteration 342: loss = -11.623347273963791
2023-04-09 00:26:20,350 - INFO - training.closure - iteration 343: loss = -11.623893706268014
2023-04-09 00:26:24,233 - INFO - training.closure - iteration 344: loss = -11.624361701083512
2023-04-09 00:26:28,133 - INFO - training.closure - iteration 345: loss = -11.624942017403956
2023-04-09 00:26:31,971 - INFO - training.closure - iteration 346: loss = -11.625863460638506
2023-04-09 00:26:35,859 - INFO - training.closure - iteration 347: loss = -11.629320029314854
2023-04-09 00:26:39,702 - INFO - training.closure - iteration 348: loss = -11.632431280405546
2023-04-09 00:26:43,524 - INFO - training.closure - iteration 349: loss = -11.635517338463252
2023-04-09 00:26:47,478 - INFO - training.closure - iteration 350: loss = -11.635065548764047
2023-04-09 00:26:51,297 - INFO - training.closure - iteration 351: loss = -11.638012441743122
2023-04-09 00:26:55,116 - INFO - training.closure - iteration 352: loss = -11.640855795124736
2023-04-09 00:26:58,933 - INFO - training.closure - iteration 353: loss = -11.644874410455655
2023-04-09 00:27:02,833 - INFO - training.closure - iteration 354: loss = -11.649719954170378
2023-04-09 00:27:06,658 - INFO - training.closure - iteration 355: loss = -11.657823262588483
2023-04-09 00:27:10,464 - INFO - training.closure - iteration 356: loss = -11.665136588843303
2023-04-09 00:27:14,293 - INFO - training.closure - iteration 357: loss = -11.669045100873584
2023-04-09 00:27:18,204 - INFO - training.closure - iteration 358: loss = -11.67158840551798
2023-04-09 00:27:22,070 - INFO - training.closure - iteration 359: loss = -11.672851647963213
2023-04-09 00:27:25,896 - INFO - training.closure - iteration 360: loss = -11.673487624221668
2023-04-09 00:27:29,717 - INFO - training.closure - iteration 361: loss = -11.67432789954849
2023-04-09 00:27:33,625 - INFO - training.closure - iteration 362: loss = -11.674931916843079
2023-04-09 00:27:37,468 - INFO - training.closure - iteration 363: loss = -11.676393918274844
2023-04-09 00:27:41,289 - INFO - training.closure - iteration 364: loss = -11.680123808826316
2023-04-09 00:27:45,125 - INFO - training.closure - iteration 365: loss = -11.683748661944223
2023-04-09 00:27:48,949 - INFO - training.closure - iteration 366: loss = -11.65192311761207
2023-04-09 00:27:52,847 - INFO - training.closure - iteration 367: loss = -11.684700153025432
2023-04-09 00:27:56,666 - INFO - training.closure - iteration 368: loss = -11.68757526127714
2023-04-09 00:28:00,492 - INFO - training.closure - iteration 369: loss = -11.688396228779403
2023-04-09 00:28:04,312 - INFO - training.closure - iteration 370: loss = -11.689011170089545
2023-04-09 00:28:08,218 - INFO - training.closure - iteration 371: loss = -11.689885634633558
2023-04-09 00:28:12,035 - INFO - training.closure - iteration 372: loss = -11.691519999099377
2023-04-09 00:28:15,855 - INFO - training.closure - iteration 373: loss = -11.687904473609027
2023-04-09 00:28:19,683 - INFO - training.closure - iteration 374: loss = -11.692130027570276
2023-04-09 00:28:23,588 - INFO - training.closure - iteration 375: loss = -11.693648740517283
2023-04-09 00:28:27,553 - INFO - training.closure - iteration 376: loss = -11.694760122114488
2023-04-09 00:28:31,292 - INFO - training.closure - iteration 377: loss = -11.695335051026813
2023-04-09 00:28:35,113 - INFO - training.closure - iteration 378: loss = -11.6962535575232
2023-04-09 00:28:38,939 - INFO - training.closure - iteration 379: loss = -11.697364618451353
2023-04-09 00:28:42,920 - INFO - training.closure - iteration 380: loss = -11.69833359055334
2023-04-09 00:28:46,824 - INFO - training.closure - iteration 381: loss = -11.698515439414393
2023-04-09 00:28:50,657 - INFO - training.closure - iteration 382: loss = -11.698102115968346
2023-04-09 00:28:54,478 - INFO - training.closure - iteration 383: loss = -11.698632264485148
2023-04-09 00:28:58,375 - INFO - training.closure - iteration 384: loss = -11.698939539161387
2023-04-09 00:29:02,200 - INFO - training.closure - iteration 385: loss = -11.69943796029106
2023-04-09 00:29:06,022 - INFO - training.closure - iteration 386: loss = -11.699880471013909
2023-04-09 00:29:09,841 - INFO - training.closure - iteration 387: loss = -11.700173517425046
2023-04-09 00:29:13,740 - INFO - training.closure - iteration 388: loss = -11.700345399004014
2023-04-09 00:29:17,621 - INFO - training.closure - iteration 389: loss = -11.700505276151414
2023-04-09 00:29:21,440 - INFO - training.closure - iteration 390: loss = -11.700720040601047
2023-04-09 00:29:25,313 - INFO - training.closure - iteration 391: loss = -11.700949743777054
2023-04-09 00:29:29,282 - INFO - training.closure - iteration 392: loss = -11.701218516375114
2023-04-09 00:29:33,182 - INFO - training.closure - iteration 393: loss = -11.701427579528515
2023-04-09 00:29:37,003 - INFO - training.closure - iteration 394: loss = -11.701542445883867
2023-04-09 00:29:40,824 - INFO - training.closure - iteration 395: loss = -11.70166798096368
2023-04-09 00:29:44,648 - INFO - training.closure - iteration 396: loss = -11.701483468807105
2023-04-09 00:29:48,548 - INFO - training.closure - iteration 397: loss = -11.701712373416502
2023-04-09 00:29:52,363 - INFO - training.closure - iteration 398: loss = -11.701780102277386
2023-04-09 00:29:56,179 - INFO - training.closure - iteration 399: loss = -11.701875668648446
2023-04-09 00:29:59,997 - INFO - training.closure - iteration 400: loss = -11.702006284602081
2023-04-09 00:30:04,437 - INFO - training.closure - iteration 401: loss = -11.702222921365319
2023-04-09 00:30:08,241 - INFO - training.closure - iteration 402: loss = -11.702319604774988
2023-04-09 00:30:12,066 - INFO - training.closure - iteration 403: loss = -11.702421136995088
2023-04-09 00:30:15,909 - INFO - training.closure - iteration 404: loss = -11.702483212552156
2023-04-09 00:30:19,823 - INFO - training.closure - iteration 405: loss = -11.702496118857837
2023-04-09 00:30:23,642 - INFO - training.closure - iteration 406: loss = -11.702540942246443
2023-04-09 00:30:27,466 - INFO - training.closure - iteration 407: loss = -11.702582122632599
2023-04-09 00:30:31,308 - INFO - training.closure - iteration 408: loss = -11.7026531853156
2023-04-09 00:30:35,137 - INFO - training.closure - iteration 409: loss = -11.702732197023714
2023-04-09 00:30:39,046 - INFO - training.closure - iteration 410: loss = -11.702565164439594
2023-04-09 00:30:42,874 - INFO - training.closure - iteration 411: loss = -11.702779147350421
2023-04-09 00:30:46,829 - INFO - training.closure - iteration 412: loss = -11.702998340277398
2023-04-09 00:30:50,770 - INFO - training.closure - iteration 413: loss = -11.703240536317988
2023-04-09 00:30:54,672 - INFO - training.closure - iteration 414: loss = -11.7036936826762
2023-04-09 00:30:58,493 - INFO - training.closure - iteration 415: loss = -11.704217545045658
2023-04-09 00:31:02,317 - INFO - training.closure - iteration 416: loss = -11.704715226779175
2023-04-09 00:31:06,140 - INFO - training.closure - iteration 417: loss = -11.705729796833488
2023-04-09 00:31:10,119 - INFO - training.closure - iteration 418: loss = -11.70735766043886
2023-04-09 00:31:13,815 - INFO - training.closure - iteration 419: loss = -11.706196634784323
2023-04-09 00:31:17,763 - INFO - training.closure - iteration 420: loss = -11.707854005897357
2023-04-09 00:31:21,584 - INFO - training.closure - iteration 421: loss = -11.70877404417886
2023-04-09 00:31:25,400 - INFO - training.closure - iteration 422: loss = -11.709400241889128
2023-04-09 00:31:29,284 - INFO - training.closure - iteration 423: loss = -11.710538199701972
2023-04-09 00:31:33,239 - INFO - training.closure - iteration 424: loss = -11.714185121920229
2023-04-09 00:31:37,071 - INFO - training.closure - iteration 425: loss = -11.718101114885862
2023-04-09 00:31:40,866 - INFO - training.closure - iteration 426: loss = -11.704878110740236
2023-04-09 00:31:44,810 - INFO - training.closure - iteration 427: loss = -11.718966638558761
2023-04-09 00:31:48,602 - INFO - training.closure - iteration 428: loss = -11.721003932562699
2023-04-09 00:31:52,427 - INFO - training.closure - iteration 429: loss = -11.72100282324525
2023-04-09 00:31:56,259 - INFO - training.closure - iteration 430: loss = -11.721324602482088
2023-04-09 00:32:00,171 - INFO - training.closure - iteration 431: loss = -11.721598629065191
2023-04-09 00:32:04,041 - INFO - training.closure - iteration 432: loss = -11.721721388070936
2023-04-09 00:32:07,788 - INFO - training.closure - iteration 433: loss = -11.721931613625507
2023-04-09 00:32:11,600 - INFO - training.closure - iteration 434: loss = -11.722158765789754
2023-04-09 00:32:15,537 - INFO - training.closure - iteration 435: loss = -11.722472603514273
2023-04-09 00:32:19,424 - INFO - training.closure - iteration 436: loss = -11.721297854562863
2023-04-09 00:32:23,201 - INFO - training.closure - iteration 437: loss = -11.722885245867188
2023-04-09 00:32:27,013 - INFO - training.closure - iteration 438: loss = -11.71185105499345
2023-04-09 00:32:30,751 - INFO - training.closure - iteration 439: loss = -11.724482761666026
2023-04-09 00:32:34,657 - INFO - training.closure - iteration 440: loss = -11.725818635079616
2023-04-09 00:32:38,531 - INFO - training.closure - iteration 441: loss = -11.728057622699907
2023-04-09 00:32:42,902 - INFO - training.closure - iteration 442: loss = -11.728731474800473
2023-04-09 00:32:47,036 - INFO - training.closure - iteration 443: loss = -11.729832194587706
2023-04-09 00:32:50,994 - INFO - training.closure - iteration 444: loss = -11.730609306459948
2023-04-09 00:32:54,832 - INFO - training.closure - iteration 445: loss = -11.731204223076404
2023-04-09 00:32:58,673 - INFO - training.closure - iteration 446: loss = -11.731346909995032
2023-04-09 00:33:02,519 - INFO - training.closure - iteration 447: loss = -11.73203971197406
2023-04-09 00:33:06,442 - INFO - training.closure - iteration 448: loss = -11.732415156931168
2023-04-09 00:33:10,288 - INFO - training.closure - iteration 449: loss = -11.732909096289514
2023-04-09 00:33:14,247 - INFO - training.closure - iteration 450: loss = -11.733652956936652
2023-04-09 00:33:18,096 - INFO - training.closure - iteration 451: loss = -11.734708559459584
2023-04-09 00:33:21,926 - INFO - training.closure - iteration 452: loss = -11.73736846787251
2023-04-09 00:33:25,841 - INFO - training.closure - iteration 453: loss = -11.741563690556397
2023-04-09 00:33:29,677 - INFO - training.closure - iteration 454: loss = -11.743069953151767
2023-04-09 00:33:33,510 - INFO - training.closure - iteration 455: loss = -11.748361388552311
2023-04-09 00:33:37,348 - INFO - training.closure - iteration 456: loss = -11.751382161680809
2023-04-09 00:33:41,268 - INFO - training.closure - iteration 457: loss = -11.75273863847056
2023-04-09 00:33:45,102 - INFO - training.closure - iteration 458: loss = -11.755776365095933
2023-04-09 00:33:49,287 - INFO - training.closure - iteration 459: loss = -11.759498855368756
2023-04-09 00:33:53,183 - INFO - training.closure - iteration 460: loss = -11.762668725843863
2023-04-09 00:33:57,099 - INFO - training.closure - iteration 461: loss = -11.764555089387114
2023-04-09 00:34:00,977 - INFO - training.closure - iteration 462: loss = -11.765631820473791
2023-04-09 00:34:04,818 - INFO - training.closure - iteration 463: loss = -11.766241822590207
2023-04-09 00:34:08,655 - INFO - training.closure - iteration 464: loss = -11.766630270304486
2023-04-09 00:34:12,552 - INFO - training.closure - iteration 465: loss = -11.76731523728415
2023-04-09 00:34:16,354 - INFO - training.closure - iteration 466: loss = -11.768814242740211
2023-04-09 00:34:20,188 - INFO - training.closure - iteration 467: loss = -11.770185422001902
2023-04-09 00:34:24,011 - INFO - training.closure - iteration 468: loss = -11.764177282447342
2023-04-09 00:34:27,842 - INFO - training.closure - iteration 469: loss = -11.770781580457484
2023-04-09 00:34:31,759 - INFO - training.closure - iteration 470: loss = -11.770926562667295
2023-04-09 00:34:35,605 - INFO - training.closure - iteration 471: loss = -11.771589312697653
2023-04-09 00:34:39,454 - INFO - training.closure - iteration 472: loss = -11.771900606881426
2023-04-09 00:34:43,297 - INFO - training.closure - iteration 473: loss = -11.772047892684233
2023-04-09 00:34:47,167 - INFO - training.closure - iteration 474: loss = -11.772481261559477
2023-04-09 00:34:51,128 - INFO - training.closure - iteration 475: loss = -11.772749529102654
2023-04-09 00:34:55,216 - INFO - training.closure - iteration 476: loss = -11.769059924954519
2023-04-09 00:34:59,240 - INFO - training.closure - iteration 477: loss = -11.772813534749261
2023-04-09 00:35:03,292 - INFO - training.closure - iteration 478: loss = -11.77300193550516
2023-04-09 00:35:07,144 - INFO - training.closure - iteration 479: loss = -11.77311717643759
2023-04-09 00:35:11,121 - INFO - training.closure - iteration 480: loss = -11.773060822201744
2023-04-09 00:35:15,014 - INFO - training.closure - iteration 481: loss = -11.773219082886946
2023-04-09 00:35:18,857 - INFO - training.closure - iteration 482: loss = -11.773416191329964
2023-04-09 00:35:22,770 - INFO - training.closure - iteration 483: loss = -11.773771810144435
2023-04-09 00:35:26,609 - INFO - training.closure - iteration 484: loss = -11.77399631990394
2023-04-09 00:35:30,479 - INFO - training.closure - iteration 485: loss = -11.774318043418836
2023-04-09 00:35:34,323 - INFO - training.closure - iteration 486: loss = -11.774408922895049
2023-04-09 00:35:38,253 - INFO - training.closure - iteration 487: loss = -11.77446519070087
2023-04-09 00:35:42,188 - INFO - training.closure - iteration 488: loss = -11.77445673680809
2023-04-09 00:35:46,009 - INFO - training.closure - iteration 489: loss = -11.774484459444082
2023-04-09 00:35:49,957 - INFO - training.closure - iteration 490: loss = -11.774510749351878
2023-04-09 00:35:53,904 - INFO - training.closure - iteration 491: loss = -11.774540739960013
2023-04-09 00:35:57,801 - INFO - training.closure - iteration 492: loss = -11.774576804360835
2023-04-09 00:36:01,913 - INFO - training.closure - iteration 493: loss = -11.774608635273708
2023-04-09 00:36:05,761 - INFO - training.closure - iteration 494: loss = -11.774669415236731
2023-04-09 00:36:09,691 - INFO - training.closure - iteration 495: loss = -11.77473263731985
2023-04-09 00:36:13,530 - INFO - training.closure - iteration 496: loss = -11.774824026751816
2023-04-09 00:36:17,343 - INFO - training.closure - iteration 497: loss = -11.774927283851301
2023-04-09 00:36:21,187 - INFO - training.closure - iteration 498: loss = -11.775058030914863
2023-04-09 00:36:25,056 - INFO - training.closure - iteration 499: loss = -11.77522513206884
2023-04-09 00:36:29,144 - INFO - training.closure - iteration 500: loss = -11.775388605194127
2023-04-09 00:36:33,005 - INFO - training.closure - iteration 501: loss = -11.775094248931614
2023-04-09 00:36:36,844 - INFO - training.closure - iteration 502: loss = -11.775487165476754
2023-04-09 00:36:40,685 - INFO - training.closure - iteration 503: loss = -11.775530958152405
2023-04-09 00:36:44,697 - INFO - training.closure - iteration 504: loss = -11.775696223334105
2023-04-09 00:36:48,851 - INFO - training.closure - iteration 505: loss = -11.77597801390027
2023-04-09 00:36:53,028 - INFO - training.closure - iteration 506: loss = -11.77520672273884
2023-04-09 00:36:57,113 - INFO - training.closure - iteration 507: loss = -11.776137860007342
2023-04-09 00:37:01,038 - INFO - training.closure - iteration 508: loss = -11.776433632871186
2023-04-09 00:37:05,048 - INFO - training.closure - iteration 509: loss = -11.776823545337235
2023-04-09 00:37:08,927 - INFO - training.closure - iteration 510: loss = -11.777024574127385
2023-04-09 00:37:12,771 - INFO - training.closure - iteration 511: loss = -11.777396795664322
2023-04-09 00:37:16,602 - INFO - training.closure - iteration 512: loss = -11.777545005320498
2023-04-09 00:37:20,518 - INFO - training.closure - iteration 513: loss = -11.777747243646772
2023-04-09 00:37:24,349 - INFO - training.closure - iteration 514: loss = -11.777857269167747
2023-04-09 00:37:28,181 - INFO - training.closure - iteration 515: loss = -11.777957919470847
2023-04-09 00:37:32,093 - INFO - training.closure - iteration 516: loss = -11.778063265711594
2023-04-09 00:37:36,103 - INFO - training.closure - iteration 517: loss = -11.776309461221471
2023-04-09 00:37:39,994 - INFO - training.closure - iteration 518: loss = -11.778071430221052
2023-04-09 00:37:43,841 - INFO - training.closure - iteration 519: loss = -11.778087476051919
2023-04-09 00:37:47,713 - INFO - training.closure - iteration 520: loss = -11.77809317423776
2023-04-09 00:37:51,722 - INFO - training.closure - iteration 521: loss = -11.778101838728473
2023-04-09 00:37:55,541 - INFO - training.closure - iteration 522: loss = -11.778119992949502
2023-04-09 00:37:59,424 - INFO - training.closure - iteration 523: loss = -11.77814970266082
2023-04-09 00:38:03,220 - INFO - training.closure - iteration 524: loss = -11.778174660807426
2023-04-09 00:38:07,035 - INFO - training.closure - iteration 525: loss = -11.778190840981486
2023-04-09 00:38:10,935 - INFO - training.closure - iteration 526: loss = -11.77818942028231
2023-04-09 00:38:14,766 - INFO - training.closure - iteration 527: loss = -11.778195826521898
2023-04-09 00:38:18,741 - INFO - training.closure - iteration 528: loss = -11.778201121274702
2023-04-09 00:38:22,674 - INFO - training.closure - iteration 529: loss = -11.778204482826862
2023-04-09 00:38:26,573 - INFO - training.closure - iteration 530: loss = -11.778208802031203
2023-04-09 00:38:30,391 - INFO - training.closure - iteration 531: loss = -11.778214360004547
2023-04-09 00:38:34,211 - INFO - training.closure - iteration 532: loss = -11.778222085530327
2023-04-09 00:38:38,036 - INFO - training.closure - iteration 533: loss = -11.778236944741908
2023-04-09 00:38:42,019 - INFO - training.closure - iteration 534: loss = -11.778235298117998
2023-04-09 00:38:45,839 - INFO - training.closure - iteration 535: loss = -11.778251347180309
2023-04-09 00:38:49,613 - INFO - training.closure - iteration 536: loss = -11.77826383665836
2023-04-09 00:38:53,466 - INFO - training.closure - iteration 537: loss = -11.777893031035074
2023-04-09 00:38:57,374 - INFO - training.closure - iteration 538: loss = -11.7782713871295
2023-04-09 00:39:01,176 - INFO - training.closure - iteration 539: loss = -11.778291559807302
2023-04-09 00:39:05,041 - INFO - training.closure - iteration 540: loss = -11.77829932599831
2023-04-09 00:39:08,917 - INFO - training.closure - iteration 541: loss = -11.778310746279802
2023-04-09 00:39:12,767 - INFO - training.closure - iteration 542: loss = -11.778317581608775
2023-04-09 00:39:16,703 - INFO - training.closure - iteration 543: loss = -11.778328794159485
2023-04-09 00:39:20,533 - INFO - training.closure - iteration 544: loss = -11.778355539000334
2023-04-09 00:39:24,350 - INFO - training.closure - iteration 545: loss = -11.77845637343098
2023-04-09 00:39:28,151 - INFO - training.closure - iteration 546: loss = -11.778653867736477
2023-04-09 00:39:32,055 - INFO - training.closure - iteration 547: loss = -11.77896387062119
2023-04-09 00:39:35,941 - INFO - training.closure - iteration 548: loss = -11.779316017812764
2023-04-09 00:39:39,765 - INFO - training.closure - iteration 549: loss = -11.778442956972649
2023-04-09 00:39:43,587 - INFO - training.closure - iteration 550: loss = -11.779377789163831
2023-04-09 00:39:47,492 - INFO - training.closure - iteration 551: loss = -11.779486633634185
2023-04-09 00:39:51,314 - INFO - training.closure - iteration 552: loss = -11.779474374999609
2023-04-09 00:39:55,136 - INFO - training.closure - iteration 553: loss = -11.779534042438916
2023-04-09 00:39:59,004 - INFO - training.closure - iteration 554: loss = -11.779554537048604
2023-04-09 00:40:02,821 - INFO - training.closure - iteration 555: loss = -11.779410333970567
2023-04-09 00:40:06,788 - INFO - training.closure - iteration 556: loss = -11.779569731149806
2023-04-09 00:40:10,606 - INFO - training.closure - iteration 557: loss = -11.779626629781394
2023-04-09 00:40:14,425 - INFO - training.closure - iteration 558: loss = -11.779688025416487
2023-04-09 00:40:18,361 - INFO - training.closure - iteration 559: loss = -11.779781556475564
2023-04-09 00:40:22,256 - INFO - training.closure - iteration 560: loss = -11.779878652357823
2023-04-09 00:40:26,074 - INFO - training.closure - iteration 561: loss = -11.779988661941772
2023-04-09 00:40:29,888 - INFO - training.closure - iteration 562: loss = -11.780102515725428
2023-04-09 00:40:33,714 - INFO - training.closure - iteration 563: loss = -11.780342080302383
2023-04-09 00:40:37,613 - INFO - training.closure - iteration 564: loss = -11.780518569492987
2023-04-09 00:40:41,429 - INFO - training.closure - iteration 565: loss = -11.778481446481472
2023-04-09 00:40:45,270 - INFO - training.closure - iteration 566: loss = -11.780585602704482
2023-04-09 00:40:49,043 - INFO - training.closure - iteration 567: loss = -11.780828547079647
2023-04-09 00:40:53,003 - INFO - training.closure - iteration 568: loss = -11.781007919754462
2023-04-09 00:40:56,867 - INFO - training.closure - iteration 569: loss = -11.781326342592696
2023-04-09 00:41:00,687 - INFO - training.closure - iteration 570: loss = -11.781649355182974
2023-04-09 00:41:04,511 - INFO - training.closure - iteration 571: loss = -11.781835787604598
2023-04-09 00:41:08,331 - INFO - training.closure - iteration 572: loss = -11.781959257374403
2023-04-09 00:41:12,233 - INFO - training.closure - iteration 573: loss = -11.78212542983394
2023-04-09 00:41:16,056 - INFO - training.closure - iteration 574: loss = -11.782492284777025
2023-04-09 00:41:19,883 - INFO - training.closure - iteration 575: loss = -11.78262317921936
2023-04-09 00:41:23,698 - INFO - training.closure - iteration 576: loss = -11.783109961235109
2023-04-09 00:41:27,598 - INFO - training.closure - iteration 577: loss = -11.783511395506885
2023-04-09 00:41:31,360 - INFO - training.closure - iteration 578: loss = -11.784055486209647
2023-04-09 00:41:35,190 - INFO - training.closure - iteration 579: loss = -11.782930502354013
2023-04-09 00:41:39,094 - INFO - training.closure - iteration 580: loss = -11.784277023628864
2023-04-09 00:41:42,997 - INFO - training.closure - iteration 581: loss = -11.784434678637878
2023-04-09 00:41:46,836 - INFO - training.closure - iteration 582: loss = -11.784669728684156
2023-04-09 00:41:50,660 - INFO - training.closure - iteration 583: loss = -11.784764929968699
2023-04-09 00:41:54,485 - INFO - training.closure - iteration 584: loss = -11.784870694036616
2023-04-09 00:41:58,306 - INFO - training.closure - iteration 585: loss = -11.78426262065198
2023-04-09 00:42:02,206 - INFO - training.closure - iteration 586: loss = -11.784973523426492
2023-04-09 00:42:06,042 - INFO - training.closure - iteration 587: loss = -11.78513523091258
2023-04-09 00:42:09,862 - INFO - training.closure - iteration 588: loss = -11.785563781806243
2023-04-09 00:42:13,681 - INFO - training.closure - iteration 589: loss = -11.785727623522815
2023-04-09 00:42:17,579 - INFO - training.closure - iteration 590: loss = -11.7857417412336
2023-04-09 00:42:21,408 - INFO - training.closure - iteration 591: loss = -11.785843809936942
2023-04-09 00:42:25,224 - INFO - training.closure - iteration 592: loss = -11.785962888731468
2023-04-09 00:42:29,060 - INFO - training.closure - iteration 593: loss = -11.786191501613423
2023-04-09 00:42:32,958 - INFO - training.closure - iteration 594: loss = -11.786362885756805
2023-04-09 00:42:36,789 - INFO - training.closure - iteration 595: loss = -11.78663161313617
2023-04-09 00:42:40,610 - INFO - training.closure - iteration 596: loss = -11.78725899033412
2023-04-09 00:42:44,431 - INFO - training.closure - iteration 597: loss = -11.788164180739562
2023-04-09 00:42:48,488 - INFO - training.closure - iteration 598: loss = -11.787959970507693
2023-04-09 00:42:52,350 - INFO - training.closure - iteration 599: loss = -11.788552484822981
2023-04-09 00:42:56,165 - INFO - training.closure - iteration 600: loss = -11.789020177713484
2023-04-09 00:42:59,949 - INFO - training.closure - iteration 601: loss = -11.789342491452881
2023-04-09 00:43:03,757 - INFO - training.closure - iteration 602: loss = -11.789501781897991
2023-04-09 00:43:07,652 - INFO - training.closure - iteration 603: loss = -11.789573543470745
2023-04-09 00:43:11,469 - INFO - training.closure - iteration 604: loss = -11.78994425894915
2023-04-09 00:43:15,286 - INFO - training.closure - iteration 605: loss = -11.789039574882164
2023-04-09 00:43:19,211 - INFO - training.closure - iteration 606: loss = -11.790012960589891
2023-04-09 00:43:23,156 - INFO - training.closure - iteration 607: loss = -11.790181243785305
2023-04-09 00:43:27,065 - INFO - training.closure - iteration 608: loss = -11.79053749633905
2023-04-09 00:43:30,843 - INFO - training.closure - iteration 609: loss = -11.790913790162397
2023-04-09 00:43:34,639 - INFO - training.closure - iteration 610: loss = -11.79112436716532
2023-04-09 00:43:38,546 - INFO - training.closure - iteration 611: loss = -11.791365050407624
2023-04-09 00:43:42,370 - INFO - training.closure - iteration 612: loss = -11.791558160935544
2023-04-09 00:43:46,192 - INFO - training.closure - iteration 613: loss = -11.79176396305732
2023-04-09 00:43:50,002 - INFO - training.closure - iteration 614: loss = -11.791569631994282
2023-04-09 00:43:53,867 - INFO - training.closure - iteration 615: loss = -11.79182979717379
2023-04-09 00:43:57,840 - INFO - training.closure - iteration 616: loss = -11.791915345854008
2023-04-09 00:44:01,661 - INFO - training.closure - iteration 617: loss = -11.79201748267979
2023-04-09 00:44:05,488 - INFO - training.closure - iteration 618: loss = -11.79206211083587
2023-04-09 00:44:09,309 - INFO - training.closure - iteration 619: loss = -11.792150582833308
2023-04-09 00:44:13,213 - INFO - training.closure - iteration 620: loss = -11.792217871488194
2023-04-09 00:44:17,031 - INFO - training.closure - iteration 621: loss = -11.792345857158066
2023-04-09 00:44:20,851 - INFO - training.closure - iteration 622: loss = -11.792429946592925
2023-04-09 00:44:24,672 - INFO - training.closure - iteration 623: loss = -11.791265724588063
2023-04-09 00:44:28,578 - INFO - training.closure - iteration 624: loss = -11.79246703355799
2023-04-09 00:44:32,400 - INFO - training.closure - iteration 625: loss = -11.792538549786899
2023-04-09 00:44:36,227 - INFO - training.closure - iteration 626: loss = -11.79256743625098
2023-04-09 00:44:40,151 - INFO - training.closure - iteration 627: loss = -11.79261621569334
2023-04-09 00:44:44,061 - INFO - training.closure - iteration 628: loss = -11.792631085983597
2023-04-09 00:44:48,099 - INFO - training.closure - iteration 629: loss = -11.792635178671679
2023-04-09 00:44:51,902 - INFO - training.closure - iteration 630: loss = -11.792668067143019
2023-04-09 00:44:55,720 - INFO - training.closure - iteration 631: loss = -11.792732310126198
2023-04-09 00:44:59,542 - INFO - training.closure - iteration 632: loss = -11.792815757834472
2023-04-09 00:45:03,447 - INFO - training.closure - iteration 633: loss = -11.792911713561693
2023-04-09 00:45:07,256 - INFO - training.closure - iteration 634: loss = -11.793001273068874
2023-04-09 00:45:11,315 - INFO - training.closure - iteration 635: loss = -11.792649676302453
2023-04-09 00:45:15,201 - INFO - training.closure - iteration 636: loss = -11.79302862467554
2023-04-09 00:45:19,441 - INFO - training.closure - iteration 637: loss = -11.793108330585122
2023-04-09 00:45:23,263 - INFO - training.closure - iteration 638: loss = -11.793150383498599
2023-04-09 00:45:27,118 - INFO - training.closure - iteration 639: loss = -11.793196195848418
2023-04-09 00:45:30,946 - INFO - training.closure - iteration 640: loss = -11.793192335031016
2023-04-09 00:45:34,854 - INFO - training.closure - iteration 641: loss = -11.793225949876277
2023-04-09 00:45:38,696 - INFO - training.closure - iteration 642: loss = -11.793270426209858
2023-04-09 00:45:42,521 - INFO - training.closure - iteration 643: loss = -11.793475769006692
2023-04-09 00:45:46,370 - INFO - training.closure - iteration 644: loss = -11.79354143859978
2023-04-09 00:45:50,196 - INFO - training.closure - iteration 645: loss = -11.793601905510439
2023-04-09 00:45:54,099 - INFO - training.closure - iteration 646: loss = -11.79368799694356
2023-04-09 00:45:57,921 - INFO - training.closure - iteration 647: loss = -11.793750538715091
2023-04-09 00:46:01,853 - INFO - training.closure - iteration 648: loss = -11.793789995070094
2023-04-09 00:46:05,639 - INFO - training.closure - iteration 649: loss = -11.793879753277716
2023-04-09 00:46:09,770 - INFO - training.closure - iteration 650: loss = -11.793893566383108
2023-04-09 00:46:13,622 - INFO - training.closure - iteration 651: loss = -11.793906158386658
2023-04-09 00:46:17,440 - INFO - training.closure - iteration 652: loss = -11.793915160985879
2023-04-09 00:46:21,265 - INFO - training.closure - iteration 653: loss = -11.793924585555814
2023-04-09 00:46:25,166 - INFO - training.closure - iteration 654: loss = -11.79392207709888
2023-04-09 00:46:28,982 - INFO - training.closure - iteration 655: loss = -11.793949908206843
2023-04-09 00:46:32,799 - INFO - training.closure - iteration 656: loss = -11.79397835823691
2023-04-09 00:46:36,622 - INFO - training.closure - iteration 657: loss = -11.793991484590599
2023-04-09 00:46:40,442 - INFO - training.closure - iteration 658: loss = -11.79401558065909
2023-04-09 00:46:44,367 - INFO - training.closure - iteration 659: loss = -11.794048870468293
2023-04-09 00:46:48,198 - INFO - training.closure - iteration 660: loss = -11.79409944249926
2023-04-09 00:46:52,022 - INFO - training.closure - iteration 661: loss = -11.79417115647593
2023-04-09 00:46:55,844 - INFO - training.closure - iteration 662: loss = -11.794322029193921
2023-04-09 00:46:59,865 - INFO - training.closure - iteration 663: loss = -11.794331057973665
2023-04-09 00:47:03,695 - INFO - training.closure - iteration 664: loss = -11.79438676700219
2023-04-09 00:47:07,519 - INFO - training.closure - iteration 665: loss = -11.794505140096522
2023-04-09 00:47:11,336 - INFO - training.closure - iteration 666: loss = -11.794553157087067
2023-04-09 00:47:15,306 - INFO - training.closure - iteration 667: loss = -11.794624392493919
2023-04-09 00:47:19,154 - INFO - training.closure - iteration 668: loss = -11.794652370483146
2023-04-09 00:47:22,973 - INFO - training.closure - iteration 669: loss = -11.794707460012388
2023-04-09 00:47:26,791 - INFO - training.closure - iteration 670: loss = -11.794728327796854
2023-04-09 00:47:30,698 - INFO - training.closure - iteration 671: loss = -11.794828875120963
2023-04-09 00:47:34,523 - INFO - training.closure - iteration 672: loss = -11.79491219812054
2023-04-09 00:47:38,318 - INFO - training.closure - iteration 673: loss = -11.794867879917714
2023-04-09 00:47:42,155 - INFO - training.closure - iteration 674: loss = -11.7949553327955
2023-04-09 00:47:45,969 - INFO - training.closure - iteration 675: loss = -11.795024357758956
2023-04-09 00:47:50,034 - INFO - training.closure - iteration 676: loss = -11.795055515921186
2023-04-09 00:47:53,854 - INFO - training.closure - iteration 677: loss = -11.795081128587926
2023-04-09 00:47:57,671 - INFO - training.closure - iteration 678: loss = -11.795099526684211
2023-04-09 00:48:01,491 - INFO - training.closure - iteration 679: loss = -11.79517849425595
2023-04-09 00:48:05,399 - INFO - training.closure - iteration 680: loss = -11.7947518735814
2023-04-09 00:48:09,238 - INFO - training.closure - iteration 681: loss = -11.795206342543043
2023-04-09 00:48:13,070 - INFO - training.closure - iteration 682: loss = -11.795326282632532
2023-04-09 00:48:16,907 - INFO - training.closure - iteration 683: loss = -11.795475779401551
2023-04-09 00:48:20,813 - INFO - training.closure - iteration 684: loss = -11.794925750999361
2023-04-09 00:48:24,632 - INFO - training.closure - iteration 685: loss = -11.79555034310371
2023-04-09 00:48:28,450 - INFO - training.closure - iteration 686: loss = -11.795640537733862
2023-04-09 00:48:32,268 - INFO - training.closure - iteration 687: loss = -11.795805472110292
2023-04-09 00:48:36,091 - INFO - training.closure - iteration 688: loss = -11.79590592111052
2023-04-09 00:48:40,010 - INFO - training.closure - iteration 689: loss = -11.795957929480515
2023-04-09 00:48:43,748 - INFO - training.closure - iteration 690: loss = -11.796021853930336
2023-04-09 00:48:47,779 - INFO - training.closure - iteration 691: loss = -11.79611140916506
2023-04-09 00:48:51,571 - INFO - training.closure - iteration 692: loss = -11.796226127188836
2023-04-09 00:48:55,488 - INFO - training.closure - iteration 693: loss = -11.7963730394064
2023-04-09 00:48:59,276 - INFO - training.closure - iteration 694: loss = -11.796461151522786
2023-04-09 00:49:03,056 - INFO - training.closure - iteration 695: loss = -11.796550146853592
2023-04-09 00:49:06,865 - INFO - training.closure - iteration 696: loss = -11.796666168884382
2023-04-09 00:49:10,756 - INFO - training.closure - iteration 697: loss = -11.79685189503038
2023-04-09 00:49:14,626 - INFO - training.closure - iteration 698: loss = -11.796852155567315
2023-04-09 00:49:18,452 - INFO - training.closure - iteration 699: loss = -11.796956217926468
2023-04-09 00:49:22,272 - INFO - training.closure - iteration 700: loss = -11.797231484608167
2023-04-09 00:49:26,175 - INFO - training.closure - iteration 701: loss = -11.797489435227698
2023-04-09 00:49:30,002 - INFO - training.closure - iteration 702: loss = -11.797652905969478
2023-04-09 00:49:33,819 - INFO - training.closure - iteration 703: loss = -11.797799653991978
2023-04-09 00:49:37,632 - INFO - training.closure - iteration 704: loss = -11.798044551198121
2023-04-09 00:49:41,534 - INFO - training.closure - iteration 705: loss = -11.798314580512631
2023-04-09 00:49:45,446 - INFO - training.closure - iteration 706: loss = -11.798498860456139
2023-04-09 00:49:49,265 - INFO - training.closure - iteration 707: loss = -11.798698289499491
2023-04-09 00:49:53,137 - INFO - training.closure - iteration 708: loss = -11.798797792817245
2023-04-09 00:49:56,878 - INFO - training.closure - iteration 709: loss = -11.798876341379556
2023-04-09 00:50:00,796 - INFO - training.closure - iteration 710: loss = -11.798999030153864
2023-04-09 00:50:04,613 - INFO - training.closure - iteration 711: loss = -11.799151244311023
2023-04-09 00:50:08,434 - INFO - training.closure - iteration 712: loss = -11.799105448086054
2023-04-09 00:50:12,250 - INFO - training.closure - iteration 713: loss = -11.799267768236547
2023-04-09 00:50:16,207 - INFO - training.closure - iteration 714: loss = -11.79956159762612
2023-04-09 00:50:19,985 - INFO - training.closure - iteration 715: loss = -11.799807012545934
2023-04-09 00:50:23,858 - INFO - training.closure - iteration 716: loss = -11.800219382748216
2023-04-09 00:50:27,679 - INFO - training.closure - iteration 717: loss = -11.800674412588851
2023-04-09 00:50:31,502 - INFO - training.closure - iteration 718: loss = -11.801037534699805
2023-04-09 00:50:35,427 - INFO - training.closure - iteration 719: loss = -11.801250769561381
2023-04-09 00:50:39,251 - INFO - training.closure - iteration 720: loss = -11.80138563139271
2023-04-09 00:50:43,087 - INFO - training.closure - iteration 721: loss = -11.801480281239282
2023-04-09 00:50:46,899 - INFO - training.closure - iteration 722: loss = -11.801593621966953
2023-04-09 00:50:50,757 - INFO - training.closure - iteration 723: loss = -11.801758186570412
2023-04-09 00:50:54,578 - INFO - training.closure - iteration 724: loss = -11.801959228069006
2023-04-09 00:50:58,397 - INFO - training.closure - iteration 725: loss = -11.802112703689277
2023-04-09 00:51:02,218 - INFO - training.closure - iteration 726: loss = -11.802202518335218
2023-04-09 00:51:06,118 - INFO - training.closure - iteration 727: loss = -11.80227733592506
2023-04-09 00:51:09,934 - INFO - training.closure - iteration 728: loss = -11.80233315330252
2023-04-09 00:51:13,798 - INFO - training.closure - iteration 729: loss = -11.802389106596674
2023-04-09 00:51:17,798 - INFO - training.closure - iteration 730: loss = -11.8024293670947
2023-04-09 00:51:21,643 - INFO - training.closure - iteration 731: loss = -11.802563379562752
2023-04-09 00:51:25,488 - INFO - training.closure - iteration 732: loss = -11.802684556743305
2023-04-09 00:51:29,364 - INFO - training.closure - iteration 733: loss = -11.802860876074394
2023-04-09 00:51:33,194 - INFO - training.closure - iteration 734: loss = -11.803088371942597
2023-04-09 00:51:37,025 - INFO - training.closure - iteration 735: loss = -11.803189082631707
2023-04-09 00:51:40,928 - INFO - training.closure - iteration 736: loss = -11.803317465975951
2023-04-09 00:51:44,758 - INFO - training.closure - iteration 737: loss = -11.803369793925274
2023-04-09 00:51:48,584 - INFO - training.closure - iteration 738: loss = -11.803478360306492
2023-04-09 00:51:52,410 - INFO - training.closure - iteration 739: loss = -11.803744780899377
2023-04-09 00:51:56,329 - INFO - training.closure - iteration 740: loss = -11.804039535764915
2023-04-09 00:52:00,159 - INFO - training.closure - iteration 741: loss = -11.803927125327101
2023-04-09 00:52:03,986 - INFO - training.closure - iteration 742: loss = -11.804258566324279
2023-04-09 00:52:07,815 - INFO - training.closure - iteration 743: loss = -11.804480102033565
2023-04-09 00:52:11,725 - INFO - training.closure - iteration 744: loss = -11.80461446743012
2023-04-09 00:52:15,554 - INFO - training.closure - iteration 745: loss = -11.80126468633509
2023-04-09 00:52:19,374 - INFO - training.closure - iteration 746: loss = -11.804635034186484
2023-04-09 00:52:23,202 - INFO - training.closure - iteration 747: loss = -11.804671525973879
2023-04-09 00:52:26,967 - INFO - training.closure - iteration 748: loss = -11.804784139751568
2023-04-09 00:52:30,859 - INFO - training.closure - iteration 749: loss = -11.804868330460675
2023-04-09 00:52:34,684 - INFO - training.closure - iteration 750: loss = -11.80474767150286
2023-04-09 00:52:38,723 - INFO - training.closure - iteration 751: loss = -11.804959371925616
2023-04-09 00:52:42,810 - INFO - training.closure - iteration 752: loss = -11.805159057940287
2023-04-09 00:52:47,117 - INFO - training.closure - iteration 753: loss = -11.805672854261319
2023-04-09 00:52:51,049 - INFO - training.closure - iteration 754: loss = -11.805926705904497
2023-04-09 00:52:54,905 - INFO - training.closure - iteration 755: loss = -11.806561619547782
2023-04-09 00:52:58,786 - INFO - training.closure - iteration 756: loss = -11.806991932791838
2023-04-09 00:53:02,762 - INFO - training.closure - iteration 757: loss = -11.807195651060852
2023-04-09 00:53:06,603 - INFO - training.closure - iteration 758: loss = -11.807471932371318
2023-04-09 00:53:10,489 - INFO - training.closure - iteration 759: loss = -11.807767481084586
2023-04-09 00:53:14,324 - INFO - training.closure - iteration 760: loss = -11.807892507481316
2023-04-09 00:53:18,251 - INFO - training.closure - iteration 761: loss = -11.808030208745706
2023-04-09 00:53:22,156 - INFO - training.closure - iteration 762: loss = -11.808145300208876
2023-04-09 00:53:25,989 - INFO - training.closure - iteration 763: loss = -11.808307909583096
2023-04-09 00:53:29,796 - INFO - training.closure - iteration 764: loss = -11.808635661416492
2023-04-09 00:53:33,550 - INFO - training.closure - iteration 765: loss = -11.809106868922882
2023-04-09 00:53:37,478 - INFO - training.closure - iteration 766: loss = -11.8094304438373
2023-04-09 00:53:41,359 - INFO - training.closure - iteration 767: loss = -11.809997718146004
2023-04-09 00:53:45,189 - INFO - training.closure - iteration 768: loss = -11.81017398820687
2023-04-09 00:53:49,025 - INFO - training.closure - iteration 769: loss = -11.810277676410422
2023-04-09 00:53:53,044 - INFO - training.closure - iteration 770: loss = -11.810770032737558
2023-04-09 00:53:57,312 - INFO - training.closure - iteration 771: loss = -11.811240841471047
2023-04-09 00:54:01,346 - INFO - training.closure - iteration 772: loss = -11.812434979254354
2023-04-09 00:54:05,304 - INFO - training.closure - iteration 773: loss = -11.800423187158152
2023-04-09 00:54:09,342 - INFO - training.closure - iteration 774: loss = -11.812548102735683
2023-04-09 00:54:13,270 - INFO - training.closure - iteration 775: loss = -11.812938318894542
2023-04-09 00:54:17,168 - INFO - training.closure - iteration 776: loss = -11.813320157063437
2023-04-09 00:54:21,261 - INFO - training.closure - iteration 777: loss = -11.813545565839718
2023-04-09 00:54:25,140 - INFO - training.closure - iteration 778: loss = -11.814046547710046
2023-04-09 00:54:29,121 - INFO - training.closure - iteration 779: loss = -11.81455042365845
2023-04-09 00:54:32,990 - INFO - training.closure - iteration 780: loss = -11.815069950712603
2023-04-09 00:54:36,834 - INFO - training.closure - iteration 781: loss = -11.815543798501356
2023-04-09 00:54:40,674 - INFO - training.closure - iteration 782: loss = -11.816392898910692
2023-04-09 00:54:44,617 - INFO - training.closure - iteration 783: loss = -11.817723453866153
2023-04-09 00:54:48,455 - INFO - training.closure - iteration 784: loss = -11.814867153087972
2023-04-09 00:54:52,293 - INFO - training.closure - iteration 785: loss = -11.818071163817521
2023-04-09 00:54:56,128 - INFO - training.closure - iteration 786: loss = -11.819382084030353
2023-04-09 00:55:00,043 - INFO - training.closure - iteration 787: loss = -11.820458963943029
2023-04-09 00:55:03,767 - INFO - training.closure - iteration 788: loss = -11.821021031162157
2023-04-09 00:55:07,498 - INFO - training.closure - iteration 789: loss = -11.821543485114265
2023-04-09 00:55:11,362 - INFO - training.closure - iteration 790: loss = -11.822037185527044
2023-04-09 00:55:15,198 - INFO - training.closure - iteration 791: loss = -11.823171976862108
2023-04-09 00:55:19,212 - INFO - training.closure - iteration 792: loss = -11.82377237245274
2023-04-09 00:55:23,158 - INFO - training.closure - iteration 793: loss = -11.824538866711547
2023-04-09 00:55:27,279 - INFO - training.closure - iteration 794: loss = -11.825416076718984
2023-04-09 00:55:31,342 - INFO - training.closure - iteration 795: loss = -11.827000318473148
2023-04-09 00:55:35,149 - INFO - training.closure - iteration 796: loss = -11.827512761238818
2023-04-09 00:55:38,963 - INFO - training.closure - iteration 797: loss = -11.827831200807848
2023-04-09 00:55:42,788 - INFO - training.closure - iteration 798: loss = -11.827994337605041
2023-04-09 00:55:46,726 - INFO - training.closure - iteration 799: loss = -11.828137495314731
2023-04-09 00:55:50,759 - INFO - training.closure - iteration 800: loss = -11.828305686104773
2023-04-09 00:55:54,597 - INFO - training.closure - iteration 801: loss = -11.828607534590155
2023-04-09 00:55:58,441 - INFO - training.closure - iteration 802: loss = -11.828936670091998
2023-04-09 00:56:02,422 - INFO - training.closure - iteration 803: loss = -11.829476385507258
2023-04-09 00:56:06,539 - INFO - training.closure - iteration 804: loss = -11.829853976877839
2023-04-09 00:56:10,356 - INFO - training.closure - iteration 805: loss = -11.831186499584168
2023-04-09 00:56:14,194 - INFO - training.closure - iteration 806: loss = -11.833204222970647
2023-04-09 00:56:18,291 - INFO - training.closure - iteration 807: loss = -11.834917246589782
2023-04-09 00:56:22,131 - INFO - training.closure - iteration 808: loss = -11.836266244445497
2023-04-09 00:56:26,049 - INFO - training.closure - iteration 809: loss = -11.837181054045399
2023-04-09 00:56:29,884 - INFO - training.closure - iteration 810: loss = -11.837369906258875
2023-04-09 00:56:33,800 - INFO - training.closure - iteration 811: loss = -11.837632255145305
2023-04-09 00:56:37,660 - INFO - training.closure - iteration 812: loss = -11.837944877976437
2023-04-09 00:56:41,663 - INFO - training.closure - iteration 813: loss = -11.838249117285027
2023-04-09 00:56:45,737 - INFO - training.closure - iteration 814: loss = -11.838655609784162
2023-04-09 00:56:50,012 - INFO - training.closure - iteration 815: loss = -11.838999015535732
2023-04-09 00:56:53,884 - INFO - training.closure - iteration 816: loss = -11.839592695822093
2023-04-09 00:56:57,975 - INFO - training.closure - iteration 817: loss = -11.839896953245983
2023-04-09 00:57:01,916 - INFO - training.closure - iteration 818: loss = -11.840255835543925
2023-04-09 00:57:06,040 - INFO - training.closure - iteration 819: loss = -11.840727168351936
2023-04-09 00:57:10,013 - INFO - training.closure - iteration 820: loss = -11.840986398262014
2023-04-09 00:57:13,884 - INFO - training.closure - iteration 821: loss = -11.841150887434448
2023-04-09 00:57:17,942 - INFO - training.closure - iteration 822: loss = -11.840829636834297
2023-04-09 00:57:21,786 - INFO - training.closure - iteration 823: loss = -11.84129010647464
2023-04-09 00:57:25,759 - INFO - training.closure - iteration 824: loss = -11.841753372266977
2023-04-09 00:57:29,645 - INFO - training.closure - iteration 825: loss = -11.842045887640229
2023-04-09 00:57:33,578 - INFO - training.closure - iteration 826: loss = -11.842333415199823
2023-04-09 00:57:37,528 - INFO - training.closure - iteration 827: loss = -11.842603258843102
2023-04-09 00:57:41,726 - INFO - training.closure - iteration 828: loss = -11.84377872947984
2023-04-09 00:57:45,821 - INFO - training.closure - iteration 829: loss = -11.84401733704435
2023-04-09 00:57:49,965 - INFO - training.closure - iteration 830: loss = -11.83426218018515
2023-04-09 00:57:53,864 - INFO - training.closure - iteration 831: loss = -11.844224417980389
2023-04-09 00:57:58,330 - INFO - training.closure - iteration 832: loss = -11.844408189551352
2023-04-09 00:58:02,406 - INFO - training.closure - iteration 833: loss = -11.844549592137973
2023-04-09 00:58:06,561 - INFO - training.closure - iteration 834: loss = -11.844641652590209
2023-04-09 00:58:10,847 - INFO - training.closure - iteration 835: loss = -11.84474190558517
2023-04-09 00:58:15,032 - INFO - training.closure - iteration 836: loss = -11.844813815215353
2023-04-09 00:58:19,151 - INFO - training.closure - iteration 837: loss = -11.844890453212932
2023-04-09 00:58:23,165 - INFO - training.closure - iteration 838: loss = -11.844949390911534
2023-04-09 00:58:27,076 - INFO - training.closure - iteration 839: loss = -11.845059008958676
2023-04-09 00:58:30,896 - INFO - training.closure - iteration 840: loss = -11.845199346283046
2023-04-09 00:58:34,724 - INFO - training.closure - iteration 841: loss = -11.8454000998336
2023-04-09 00:58:38,720 - INFO - training.closure - iteration 842: loss = -11.845640831032703
2023-04-09 00:58:42,811 - INFO - training.closure - iteration 843: loss = -11.845788921445543
2023-04-09 00:58:46,634 - INFO - training.closure - iteration 844: loss = -11.84593576229611
2023-04-09 00:58:50,486 - INFO - training.closure - iteration 845: loss = -11.846100500860349
2023-04-09 00:58:54,350 - INFO - training.closure - iteration 846: loss = -11.846245547011813
2023-04-09 00:58:58,271 - INFO - training.closure - iteration 847: loss = -11.846484184467155
2023-04-09 00:59:02,097 - INFO - training.closure - iteration 848: loss = -11.846782421897469
2023-04-09 00:59:05,957 - INFO - training.closure - iteration 849: loss = -11.84706399317001
2023-04-09 00:59:09,841 - INFO - training.closure - iteration 850: loss = -11.847178565676273
2023-04-09 00:59:13,634 - INFO - training.closure - iteration 851: loss = -11.847458821401407
2023-04-09 00:59:17,539 - INFO - training.closure - iteration 852: loss = -11.847677041258962
2023-04-09 00:59:21,366 - INFO - training.closure - iteration 853: loss = -11.843738479075604
2023-04-09 00:59:25,182 - INFO - training.closure - iteration 854: loss = -11.847742403806599
2023-04-09 00:59:29,230 - INFO - training.closure - iteration 855: loss = -11.847906114119446
2023-04-09 00:59:33,134 - INFO - training.closure - iteration 856: loss = -11.848211323601006
2023-04-09 00:59:36,963 - INFO - training.closure - iteration 857: loss = -11.848286273230013
2023-04-09 00:59:40,790 - INFO - training.closure - iteration 858: loss = -11.848475054441735
2023-04-09 00:59:44,611 - INFO - training.closure - iteration 859: loss = -11.848627186652969
2023-04-09 00:59:48,516 - INFO - training.closure - iteration 860: loss = -11.848871126059365
2023-04-09 00:59:52,385 - INFO - training.closure - iteration 861: loss = -11.8491683712986
2023-04-09 00:59:56,213 - INFO - training.closure - iteration 862: loss = -11.848588737933195
2023-04-09 01:00:00,030 - INFO - training.closure - iteration 863: loss = -11.849363647552721
2023-04-09 01:00:04,010 - INFO - training.closure - iteration 864: loss = -11.849475948957634
2023-04-09 01:00:07,898 - INFO - training.closure - iteration 865: loss = -11.849899373945666
2023-04-09 01:00:11,728 - INFO - training.closure - iteration 866: loss = -11.850021451866116
2023-04-09 01:00:15,562 - INFO - training.closure - iteration 867: loss = -11.85025004425371
2023-04-09 01:00:19,434 - INFO - training.closure - iteration 868: loss = -11.850530805670214
2023-04-09 01:00:23,296 - INFO - training.closure - iteration 869: loss = -11.851285147955334
2023-04-09 01:00:27,185 - INFO - training.closure - iteration 870: loss = -11.852702154169194
2023-04-09 01:00:31,063 - INFO - training.closure - iteration 871: loss = -11.854485279194346
2023-04-09 01:00:34,895 - INFO - training.closure - iteration 872: loss = -11.85273502697093
2023-04-09 01:00:38,817 - INFO - training.closure - iteration 873: loss = -11.855491910438436
2023-04-09 01:00:42,620 - INFO - training.closure - iteration 874: loss = -11.85748249273711
2023-04-09 01:00:46,352 - INFO - training.closure - iteration 875: loss = -11.858851529411274
2023-04-09 01:00:50,186 - INFO - training.closure - iteration 876: loss = -11.859541670754442
2023-04-09 01:00:54,108 - INFO - training.closure - iteration 877: loss = -11.859691755200721
2023-04-09 01:00:57,935 - INFO - training.closure - iteration 878: loss = -11.860533887106799
2023-04-09 01:01:01,764 - INFO - training.closure - iteration 879: loss = -11.860896053459086
2023-04-09 01:01:05,594 - INFO - training.closure - iteration 880: loss = -11.862058071217707
2023-04-09 01:01:09,421 - INFO - training.closure - iteration 881: loss = -11.863077051470636
2023-04-09 01:01:13,497 - INFO - training.closure - iteration 882: loss = -11.864125032391144
2023-04-09 01:01:17,319 - INFO - training.closure - iteration 883: loss = -11.864889934763454
2023-04-09 01:01:21,139 - INFO - training.closure - iteration 884: loss = -11.865496660101833
2023-04-09 01:01:24,970 - INFO - training.closure - iteration 885: loss = -11.865757597790534
2023-04-09 01:01:29,100 - INFO - training.closure - iteration 886: loss = -11.86639842971073
2023-04-09 01:01:32,935 - INFO - training.closure - iteration 887: loss = -11.86725545930274
2023-04-09 01:01:36,692 - INFO - training.closure - iteration 888: loss = -11.868471583154859
2023-04-09 01:01:40,572 - INFO - training.closure - iteration 889: loss = -11.869689280493663
2023-04-09 01:01:44,518 - INFO - training.closure - iteration 890: loss = -11.870545891007644
2023-04-09 01:01:48,371 - INFO - training.closure - iteration 891: loss = -11.87220962346985
2023-04-09 01:01:52,201 - INFO - training.closure - iteration 892: loss = -11.874163618515645
2023-04-09 01:01:56,021 - INFO - training.closure - iteration 893: loss = -11.875879021450423
2023-04-09 01:01:59,942 - INFO - training.closure - iteration 894: loss = -11.876721497912385
2023-04-09 01:02:03,775 - INFO - training.closure - iteration 895: loss = -11.877550645589785
2023-04-09 01:02:07,620 - INFO - training.closure - iteration 896: loss = -11.878069174705793
2023-04-09 01:02:11,440 - INFO - training.closure - iteration 897: loss = -11.878604916174641
2023-04-09 01:02:15,266 - INFO - training.closure - iteration 898: loss = -11.879712171387512
2023-04-09 01:02:19,191 - INFO - training.closure - iteration 899: loss = -11.88094224105394
2023-04-09 01:02:23,228 - INFO - training.closure - iteration 900: loss = -11.882161682550983
2023-04-09 01:02:27,129 - INFO - training.closure - iteration 901: loss = -11.883780475753387
2023-04-09 01:02:30,957 - INFO - training.closure - iteration 902: loss = -11.885982492316042
2023-04-09 01:02:34,866 - INFO - training.closure - iteration 903: loss = -11.880107787124913
2023-04-09 01:02:38,697 - INFO - training.closure - iteration 904: loss = -11.888880742818861
2023-04-09 01:02:42,522 - INFO - training.closure - iteration 905: loss = -11.892951722660666
2023-04-09 01:02:46,499 - INFO - training.closure - iteration 906: loss = -11.89509384114933
2023-04-09 01:02:50,286 - INFO - training.closure - iteration 907: loss = -11.896064326990949
2023-04-09 01:02:54,109 - INFO - training.closure - iteration 908: loss = -11.896994247126177
2023-04-09 01:02:58,028 - INFO - training.closure - iteration 909: loss = -11.897753428784412
2023-04-09 01:03:01,783 - INFO - training.closure - iteration 910: loss = -11.898479897566588
2023-04-09 01:03:05,606 - INFO - training.closure - iteration 911: loss = -11.899549813107424
2023-04-09 01:03:09,657 - INFO - training.closure - iteration 912: loss = -11.900202008431227
2023-04-09 01:03:13,482 - INFO - training.closure - iteration 913: loss = -11.900816099137451
2023-04-09 01:03:17,452 - INFO - training.closure - iteration 914: loss = -11.901313717793307
2023-04-09 01:03:21,228 - INFO - training.closure - iteration 915: loss = -11.901724639095097
2023-04-09 01:03:25,137 - INFO - training.closure - iteration 916: loss = -11.90248485971236
2023-04-09 01:03:28,965 - INFO - training.closure - iteration 917: loss = -11.903297450601148
2023-04-09 01:03:32,789 - INFO - training.closure - iteration 918: loss = -11.90203149589463
2023-04-09 01:03:36,616 - INFO - training.closure - iteration 919: loss = -11.903538090460334
2023-04-09 01:03:40,538 - INFO - training.closure - iteration 920: loss = -11.90414479363779
2023-04-09 01:03:44,377 - INFO - training.closure - iteration 921: loss = -11.904583658194444
2023-04-09 01:03:48,212 - INFO - training.closure - iteration 922: loss = -11.905050901668567
2023-04-09 01:03:52,042 - INFO - training.closure - iteration 923: loss = -11.905285272977782
2023-04-09 01:03:55,856 - INFO - training.closure - iteration 924: loss = -11.905574557911237
2023-04-09 01:03:59,830 - INFO - training.closure - iteration 925: loss = -11.90575877180124
2023-04-09 01:04:03,642 - INFO - training.closure - iteration 926: loss = -11.90588227103518
2023-04-09 01:04:07,488 - INFO - training.closure - iteration 927: loss = -11.906094748027765
2023-04-09 01:04:11,278 - INFO - training.closure - iteration 928: loss = -11.906621949861467
2023-04-09 01:04:15,165 - INFO - training.closure - iteration 929: loss = -11.906999241465694
2023-04-09 01:04:19,030 - INFO - training.closure - iteration 930: loss = -11.904087042148841
2023-04-09 01:04:22,868 - INFO - training.closure - iteration 931: loss = -11.907129668714184
2023-04-09 01:04:26,691 - INFO - training.closure - iteration 932: loss = -11.907507636511983
2023-04-09 01:04:30,604 - INFO - training.closure - iteration 933: loss = -11.90764470048858
2023-04-09 01:04:34,425 - INFO - training.closure - iteration 934: loss = -11.907780448236819
2023-04-09 01:04:38,781 - INFO - training.closure - iteration 935: loss = -11.90802480702143
2023-04-09 01:04:42,705 - INFO - training.closure - iteration 936: loss = -11.907285342747265
2023-04-09 01:04:46,684 - INFO - training.closure - iteration 937: loss = -11.908200787539526
2023-04-09 01:04:50,492 - INFO - training.closure - iteration 938: loss = -11.908573577872762
2023-04-09 01:04:54,526 - INFO - training.closure - iteration 939: loss = -11.908897581473752
2023-04-09 01:04:58,342 - INFO - training.closure - iteration 940: loss = -11.909329093863827
2023-04-09 01:05:02,179 - INFO - training.closure - iteration 941: loss = -11.91012705210639
2023-04-09 01:05:06,093 - INFO - training.closure - iteration 942: loss = -11.911062255133764
2023-04-09 01:05:09,928 - INFO - training.closure - iteration 943: loss = -11.909120860450503
2023-04-09 01:05:13,745 - INFO - training.closure - iteration 944: loss = -11.91161308457285
2023-04-09 01:05:17,503 - INFO - training.closure - iteration 945: loss = -11.912154580118724
2023-04-09 01:05:21,428 - INFO - training.closure - iteration 946: loss = -11.912695692205801
2023-04-09 01:05:25,383 - INFO - training.closure - iteration 947: loss = -11.913155424604298
2023-04-09 01:05:29,304 - INFO - training.closure - iteration 948: loss = -11.91397829707595
2023-04-09 01:05:33,185 - INFO - training.closure - iteration 949: loss = -11.912777624045821
2023-04-09 01:05:37,441 - INFO - training.closure - iteration 950: loss = -11.914300797115144
2023-04-09 01:05:41,316 - INFO - training.closure - iteration 951: loss = -11.914785271011242
2023-04-09 01:05:45,145 - INFO - training.closure - iteration 952: loss = -11.904341229516227
2023-04-09 01:05:49,000 - INFO - training.closure - iteration 953: loss = -11.91497572982595
2023-04-09 01:05:52,835 - INFO - training.closure - iteration 954: loss = -11.915346203666289
2023-04-09 01:05:56,739 - INFO - training.closure - iteration 955: loss = -11.915659574719495
2023-04-09 01:06:00,572 - INFO - training.closure - iteration 956: loss = -11.916333129508878
2023-04-09 01:06:04,395 - INFO - training.closure - iteration 957: loss = -11.916915371268553
2023-04-09 01:06:08,261 - INFO - training.closure - iteration 958: loss = -11.917445854648122
2023-04-09 01:06:12,182 - INFO - training.closure - iteration 959: loss = -11.917750163431695
2023-04-09 01:06:16,041 - INFO - training.closure - iteration 960: loss = -11.918703307679227
2023-04-09 01:06:19,959 - INFO - training.closure - iteration 961: loss = -11.919426666042284
2023-04-09 01:06:23,847 - INFO - training.closure - iteration 962: loss = -11.91972233556585
2023-04-09 01:06:27,767 - INFO - training.closure - iteration 963: loss = -11.920327264566676
2023-04-09 01:06:31,593 - INFO - training.closure - iteration 964: loss = -11.920782826337934
2023-04-09 01:06:35,425 - INFO - training.closure - iteration 965: loss = -11.921150540836148
2023-04-09 01:06:39,263 - INFO - training.closure - iteration 966: loss = -11.922419111780574
2023-04-09 01:06:43,107 - INFO - training.closure - iteration 967: loss = -11.924393776418256
2023-04-09 01:06:46,845 - INFO - training.closure - iteration 968: loss = -11.92379594770799
2023-04-09 01:06:50,707 - INFO - training.closure - iteration 969: loss = -11.924846317101256
2023-04-09 01:06:54,565 - INFO - training.closure - iteration 970: loss = -11.925614445287714
2023-04-09 01:06:58,390 - INFO - training.closure - iteration 971: loss = -11.926379660193433
2023-04-09 01:07:02,299 - INFO - training.closure - iteration 972: loss = -11.926926142365685
2023-04-09 01:07:06,126 - INFO - training.closure - iteration 973: loss = -11.927440522777157
2023-04-09 01:07:09,959 - INFO - training.closure - iteration 974: loss = -11.927709485873656
2023-04-09 01:07:13,785 - INFO - training.closure - iteration 975: loss = -11.927884029806602
2023-04-09 01:07:17,694 - INFO - training.closure - iteration 976: loss = -11.928071834260237
2023-04-09 01:07:21,516 - INFO - training.closure - iteration 977: loss = -11.928347667562416
2023-04-09 01:07:25,349 - INFO - training.closure - iteration 978: loss = -11.928760687171884
2023-04-09 01:07:29,175 - INFO - training.closure - iteration 979: loss = -11.92942479050876
2023-04-09 01:07:33,135 - INFO - training.closure - iteration 980: loss = -11.930859695227152
2023-04-09 01:07:37,041 - INFO - training.closure - iteration 981: loss = -11.930459906312766
2023-04-09 01:07:40,874 - INFO - training.closure - iteration 982: loss = -11.931852260760813
2023-04-09 01:07:44,711 - INFO - training.closure - iteration 983: loss = -11.933354048064032
2023-04-09 01:07:48,564 - INFO - training.closure - iteration 984: loss = -11.93462069229724
2023-04-09 01:07:52,472 - INFO - training.closure - iteration 985: loss = -11.934937964608778
2023-04-09 01:07:56,297 - INFO - training.closure - iteration 986: loss = -11.935700314590436
2023-04-09 01:08:00,031 - INFO - training.closure - iteration 987: loss = -11.935816731461042
2023-04-09 01:08:03,925 - INFO - training.closure - iteration 988: loss = -11.936080788429281
2023-04-09 01:08:07,834 - INFO - training.closure - iteration 989: loss = -11.936688260774662
2023-04-09 01:08:11,656 - INFO - training.closure - iteration 990: loss = -11.93580236849743
2023-04-09 01:08:15,473 - INFO - training.closure - iteration 991: loss = -11.937158159224847
2023-04-09 01:08:19,328 - INFO - training.closure - iteration 992: loss = -11.937869823613875
2023-04-09 01:08:23,244 - INFO - training.closure - iteration 993: loss = -11.938847717058179
2023-04-09 01:08:27,068 - INFO - training.closure - iteration 994: loss = -11.93962027726992
2023-04-09 01:08:30,892 - INFO - training.closure - iteration 995: loss = -11.940391459014382
2023-04-09 01:08:34,715 - INFO - training.closure - iteration 996: loss = -11.941053695886264
2023-04-09 01:08:38,636 - INFO - training.closure - iteration 997: loss = -11.94207377498183
2023-04-09 01:08:42,463 - INFO - training.closure - iteration 998: loss = -11.944300483160994
2023-04-09 01:08:46,318 - INFO - training.closure - iteration 999: loss = -11.947451524091262
2023-04-09 01:08:50,106 - INFO - training.closure - iteration 1000: loss = -11.949603429871905
2023-04-09 01:08:53,932 - INFO - training.closure - iteration 1001: loss = -11.953105229005404
2023-04-09 01:08:57,963 - INFO - training.closure - iteration 1002: loss = -11.955465594662579
2023-04-09 01:09:01,881 - INFO - training.closure - iteration 1003: loss = -11.953243574367704
2023-04-09 01:09:05,859 - INFO - training.closure - iteration 1004: loss = -11.956124531412112
2023-04-09 01:09:09,686 - INFO - training.closure - iteration 1005: loss = -11.956803156607258
2023-04-09 01:09:13,617 - INFO - training.closure - iteration 1006: loss = -11.957806132396238
2023-04-09 01:09:17,334 - INFO - training.closure - iteration 1007: loss = -11.95955546182281
2023-04-09 01:09:21,226 - INFO - training.closure - iteration 1008: loss = -11.963137158825923
2023-04-09 01:09:25,048 - INFO - training.closure - iteration 1009: loss = -11.967859913751816
2023-04-09 01:09:29,018 - INFO - training.closure - iteration 1010: loss = -11.97030061660816
2023-04-09 01:09:32,848 - INFO - training.closure - iteration 1011: loss = -11.975727894900437
2023-04-09 01:09:36,674 - INFO - training.closure - iteration 1012: loss = -11.977669131354737
2023-04-09 01:09:40,508 - INFO - training.closure - iteration 1013: loss = -11.979829677258014
2023-04-09 01:09:44,329 - INFO - training.closure - iteration 1014: loss = -11.980651478894913
2023-04-09 01:09:48,271 - INFO - training.closure - iteration 1015: loss = -11.981455738657221
2023-04-09 01:09:52,122 - INFO - training.closure - iteration 1016: loss = -11.98197785270273
2023-04-09 01:09:55,920 - INFO - training.closure - iteration 1017: loss = -11.98285209351714
2023-04-09 01:09:59,670 - INFO - training.closure - iteration 1018: loss = -11.983838234261352
2023-04-09 01:10:03,634 - INFO - training.closure - iteration 1019: loss = -11.990251896336435
2023-04-09 01:10:07,476 - INFO - training.closure - iteration 1020: loss = -11.99254446662244
2023-04-09 01:10:11,352 - INFO - training.closure - iteration 1021: loss = -11.994468123205356
2023-04-09 01:10:15,180 - INFO - training.closure - iteration 1022: loss = -11.996329277266288
2023-04-09 01:10:19,094 - INFO - training.closure - iteration 1023: loss = -11.918677011090955
2023-04-09 01:10:22,924 - INFO - training.closure - iteration 1024: loss = -11.997594465910677
2023-04-09 01:10:26,764 - INFO - training.closure - iteration 1025: loss = -12.000080336415438
2023-04-09 01:10:30,599 - INFO - training.closure - iteration 1026: loss = -12.003327431215444
2023-04-09 01:10:34,515 - INFO - training.closure - iteration 1027: loss = -12.006808509940662
2023-04-09 01:10:38,335 - INFO - training.closure - iteration 1028: loss = -12.01007671379709
2023-04-09 01:10:42,114 - INFO - training.closure - iteration 1029: loss = -12.011393680736287
2023-04-09 01:10:45,986 - INFO - training.closure - iteration 1030: loss = -12.011798329680943
2023-04-09 01:10:49,819 - INFO - training.closure - iteration 1031: loss = -12.013493286910785
2023-04-09 01:10:53,655 - INFO - training.closure - iteration 1032: loss = -12.013887650286247
2023-04-09 01:10:57,385 - INFO - training.closure - iteration 1033: loss = -12.01467721930489
2023-04-09 01:11:01,195 - INFO - training.closure - iteration 1034: loss = -12.01685813069258
2023-04-09 01:11:05,016 - INFO - training.closure - iteration 1035: loss = -12.018265757295785
2023-04-09 01:11:08,930 - INFO - training.closure - iteration 1036: loss = -12.020956838803725
2023-04-09 01:11:12,727 - INFO - training.closure - iteration 1037: loss = -12.022503238577698
2023-04-09 01:11:16,615 - INFO - training.closure - iteration 1038: loss = -12.024417725402802
2023-04-09 01:11:20,435 - INFO - training.closure - iteration 1039: loss = -12.025458015572326
2023-04-09 01:11:24,353 - INFO - training.closure - iteration 1040: loss = -12.026388523774727
2023-04-09 01:11:28,177 - INFO - training.closure - iteration 1041: loss = -12.028195070897905
2023-04-09 01:11:32,008 - INFO - training.closure - iteration 1042: loss = -12.032500051477527
2023-04-09 01:11:35,792 - INFO - training.closure - iteration 1043: loss = -12.036041746996474
2023-04-09 01:11:39,641 - INFO - training.closure - iteration 1044: loss = -12.044051408492637
2023-04-09 01:11:43,560 - INFO - training.closure - iteration 1045: loss = -12.04859639562569
2023-04-09 01:11:47,356 - INFO - training.closure - iteration 1046: loss = -12.050810338291756
2023-04-09 01:11:51,179 - INFO - training.closure - iteration 1047: loss = -12.052669442613016
2023-04-09 01:11:55,008 - INFO - training.closure - iteration 1048: loss = -12.054942215362574
2023-04-09 01:11:58,900 - INFO - training.closure - iteration 1049: loss = -12.0584797903926
2023-04-09 01:12:02,743 - INFO - training.closure - iteration 1050: loss = -12.06688100087117
2023-04-09 01:12:06,568 - INFO - training.closure - iteration 1051: loss = -12.025146357704939
2023-04-09 01:12:10,402 - INFO - training.closure - iteration 1052: loss = -12.06989246174909
2023-04-09 01:12:14,317 - INFO - training.closure - iteration 1053: loss = -12.076663777418865
2023-04-09 01:12:18,146 - INFO - training.closure - iteration 1054: loss = -12.081852657236304
2023-04-09 01:12:21,969 - INFO - training.closure - iteration 1055: loss = -12.076507558930498
2023-04-09 01:12:25,797 - INFO - training.closure - iteration 1056: loss = -12.084118503741816
2023-04-09 01:12:29,633 - INFO - training.closure - iteration 1057: loss = -12.088894625944983
2023-04-09 01:12:33,607 - INFO - training.closure - iteration 1058: loss = -12.09040326964831
2023-04-09 01:12:37,535 - INFO - training.closure - iteration 1059: loss = -12.091769702289643
2023-04-09 01:12:41,405 - INFO - training.closure - iteration 1060: loss = -12.092689473685464
2023-04-09 01:12:45,309 - INFO - training.closure - iteration 1061: loss = -12.0943875875497
2023-04-09 01:12:49,213 - INFO - training.closure - iteration 1062: loss = -12.097242853445945
2023-04-09 01:12:53,042 - INFO - training.closure - iteration 1063: loss = -12.0988567113846
2023-04-09 01:12:56,795 - INFO - training.closure - iteration 1064: loss = -12.104685123193313
2023-04-09 01:13:00,559 - INFO - training.closure - iteration 1065: loss = -12.107381159941582
2023-04-09 01:13:04,476 - INFO - training.closure - iteration 1066: loss = -12.110234836832188
2023-04-09 01:13:08,309 - INFO - training.closure - iteration 1067: loss = -12.113099739061806
2023-04-09 01:13:12,142 - INFO - training.closure - iteration 1068: loss = -12.116141063403816
2023-04-09 01:13:15,978 - INFO - training.closure - iteration 1069: loss = -12.11911153179966
2023-04-09 01:13:19,905 - INFO - training.closure - iteration 1070: loss = -12.124229318677045
2023-04-09 01:13:23,875 - INFO - training.closure - iteration 1071: loss = -12.130841842788215
2023-04-09 01:13:27,654 - INFO - training.closure - iteration 1072: loss = -12.132514692656004
2023-04-09 01:13:31,491 - INFO - training.closure - iteration 1073: loss = -12.136292750744538
2023-04-09 01:13:35,317 - INFO - training.closure - iteration 1074: loss = -12.141165542466338
2023-04-09 01:13:39,241 - INFO - training.closure - iteration 1075: loss = -12.135531936738797
2023-04-09 01:13:43,077 - INFO - training.closure - iteration 1076: loss = -12.142326807596305
2023-04-09 01:13:46,875 - INFO - training.closure - iteration 1077: loss = -12.142910971008686
2023-04-09 01:13:50,876 - INFO - training.closure - iteration 1078: loss = -12.143079198868952
2023-04-09 01:13:54,803 - INFO - training.closure - iteration 1079: loss = -12.143224199800486
2023-04-09 01:13:58,627 - INFO - training.closure - iteration 1080: loss = -12.143544586799226
2023-04-09 01:14:02,453 - INFO - training.closure - iteration 1081: loss = -12.143822646404175
2023-04-09 01:14:06,285 - INFO - training.closure - iteration 1082: loss = -12.144226740102699
2023-04-09 01:14:10,213 - INFO - training.closure - iteration 1083: loss = -12.144727565925713
2023-04-09 01:14:14,064 - INFO - training.closure - iteration 1084: loss = -12.14632471523068
2023-04-09 01:14:17,899 - INFO - training.closure - iteration 1085: loss = -12.147470636852598
2023-04-09 01:14:21,730 - INFO - training.closure - iteration 1086: loss = -12.149513459369718
2023-04-09 01:14:25,569 - INFO - training.closure - iteration 1087: loss = -12.149909290544706
2023-04-09 01:14:29,482 - INFO - training.closure - iteration 1088: loss = -12.150062276354085
2023-04-09 01:14:33,315 - INFO - training.closure - iteration 1089: loss = -12.15039614172073
2023-04-09 01:14:37,146 - INFO - training.closure - iteration 1090: loss = -12.151130937421254
2023-04-09 01:14:41,015 - INFO - training.closure - iteration 1091: loss = -12.151559438617387
2023-04-09 01:14:44,930 - INFO - training.closure - iteration 1092: loss = -12.152104946495584
2023-04-09 01:14:48,792 - INFO - training.closure - iteration 1093: loss = -12.1528672696997
2023-04-09 01:14:52,621 - INFO - training.closure - iteration 1094: loss = -12.154313899679384
2023-04-09 01:14:56,456 - INFO - training.closure - iteration 1095: loss = -12.153705269502465
2023-04-09 01:15:00,371 - INFO - training.closure - iteration 1096: loss = -12.155293497440365
2023-04-09 01:15:04,229 - INFO - training.closure - iteration 1097: loss = -12.156363357244587
2023-04-09 01:15:08,068 - INFO - training.closure - iteration 1098: loss = -12.156777473193465
2023-04-09 01:15:11,901 - INFO - training.closure - iteration 1099: loss = -12.156983378748631
2023-04-09 01:15:15,741 - INFO - training.closure - iteration 1100: loss = -12.157068584232118
2023-04-09 01:15:19,647 - INFO - training.closure - iteration 1101: loss = -12.157180396538404
2023-04-09 01:15:23,482 - INFO - training.closure - iteration 1102: loss = -12.157284821850855
2023-04-09 01:15:27,328 - INFO - training.closure - iteration 1103: loss = -12.15512678491789
2023-04-09 01:15:31,169 - INFO - training.closure - iteration 1104: loss = -12.157365067692506
2023-04-09 01:15:35,082 - INFO - training.closure - iteration 1105: loss = -12.157624292726961
2023-04-09 01:15:38,912 - INFO - training.closure - iteration 1106: loss = -12.15803621899267
2023-04-09 01:15:42,739 - INFO - training.closure - iteration 1107: loss = -12.158211712333257
2023-04-09 01:15:46,611 - INFO - training.closure - iteration 1108: loss = -12.158549049886998
2023-04-09 01:15:50,524 - INFO - training.closure - iteration 1109: loss = -12.158764454999671
2023-04-09 01:15:54,352 - INFO - training.closure - iteration 1110: loss = -12.159040171656958
2023-04-09 01:15:58,180 - INFO - training.closure - iteration 1111: loss = -12.159482116078934
2023-04-09 01:16:02,032 - INFO - training.closure - iteration 1112: loss = -12.159350589564063
2023-04-09 01:16:05,966 - INFO - training.closure - iteration 1113: loss = -12.159807413600888
2023-04-09 01:16:09,789 - INFO - training.closure - iteration 1114: loss = -12.160284506842476
2023-04-09 01:16:13,764 - INFO - training.closure - iteration 1115: loss = -12.160900975167019
2023-04-09 01:16:17,652 - INFO - training.closure - iteration 1116: loss = -12.161775438217528
2023-04-09 01:16:21,493 - INFO - training.closure - iteration 1117: loss = -12.163324856704502
2023-04-09 01:16:24,642 - INFO - main.experiment - train - RMSE_b at iteration 0 = 0.09468121356536707
2023-04-09 01:16:24,642 - INFO - main.experiment - train - RMSE_b at last iteration = 0.05504918908690175
2023-04-09 01:16:24,642 - INFO - main.experiment - train - RMSE_a at iteration 0 = 0.5800064485163396
2023-04-09 01:16:24,642 - INFO - main.experiment - train - RMSE_a at last iteration = 0.051788159106019555
2023-04-09 01:16:24,642 - INFO - main.experiment - train - LOGPDF_b at iteration 0 = 86.0681954246594
2023-04-09 01:16:24,642 - INFO - main.experiment - train - LOGPDF_b at last iteration = -2.849835211455102
2023-04-09 01:16:24,642 - INFO - main.experiment - train - LOGPDF_a at iteration 0 = 6193338.581261585
2023-04-09 01:16:24,642 - INFO - main.experiment - train - LOGPDF_a at last iteration = -2.9710736811275043
2023-04-09 01:16:24,642 - INFO - main.experiment - train - LOSS at iteration 0 = 6193424.6494570095
2023-04-09 01:16:24,642 - INFO - main.experiment - train - LOSS at last iteration = -5.820908892582606
2023-04-09 01:16:25,079 - INFO - main.experiment - test - RMSE_b at iteration 0 = 0.09396401154963699
2023-04-09 01:16:25,079 - INFO - main.experiment - test - RMSE_b at last iteration = 0.07859109547761398
2023-04-09 01:16:25,080 - INFO - main.experiment - test - RMSE_a at iteration 0 = 0.048638461871510684
2023-04-09 01:16:25,080 - INFO - main.experiment - test - RMSE_a at last iteration = 0.05403136786225913
2023-04-09 01:16:25,080 - INFO - main.experiment - test - LOGPDF_b at iteration 0 = 11.03484244434376
2023-04-09 01:16:25,080 - INFO - main.experiment - test - LOGPDF_b at last iteration = 50339.729959410644
2023-04-09 01:16:25,080 - INFO - main.experiment - test - LOGPDF_a at iteration 0 = -3.6956833610913957
2023-04-09 01:16:25,080 - INFO - main.experiment - test - LOGPDF_a at last iteration = 5762.632523222109
2023-04-09 01:16:25,080 - INFO - main.experiment - test - LOSS at iteration 0 = 7.339159083252364
2023-04-09 01:16:25,080 - INFO - main.experiment - test - LOSS at last iteration = 56102.36248263275
2023-04-09 01:16:25,085 - INFO - main.experiment - deep = 20 - plot = False - sigma0 = 0.01
2023-04-09 01:16:25,093 - INFO - training.pre_train_full - empirical mean of x0: 3.0000319984329904
2023-04-09 01:16:25,103 - INFO - training.pre_train_full - initial loss: 22.13070943468352
2023-04-09 01:16:25,118 - INFO - training.closure0 - iteration 0: loss = 22.13070943468352
2023-04-09 01:16:25,134 - INFO - training.closure0 - iteration 1: loss = 9.08849195033083
2023-04-09 01:16:25,149 - INFO - training.closure0 - iteration 2: loss = 7.167901160174074
2023-04-09 01:16:25,165 - INFO - training.closure0 - iteration 3: loss = 5.561390261045507
2023-04-09 01:16:25,181 - INFO - training.closure0 - iteration 4: loss = 4.915266977288359
2023-04-09 01:16:25,196 - INFO - training.closure0 - iteration 5: loss = 4.552044942814926
2023-04-09 01:16:25,212 - INFO - training.closure0 - iteration 6: loss = 4.247916063865903
2023-04-09 01:16:25,227 - INFO - training.closure0 - iteration 7: loss = 3.6461263213351125
2023-04-09 01:16:25,243 - INFO - training.closure0 - iteration 8: loss = 5.024305542625309
2023-04-09 01:16:25,259 - INFO - training.closure0 - iteration 9: loss = 2.746975866467494
2023-04-09 01:16:25,275 - INFO - training.closure0 - iteration 10: loss = 45.15148100352916
2023-04-09 01:16:25,291 - INFO - training.closure0 - iteration 11: loss = 3.5397208037661407
2023-04-09 01:16:25,307 - INFO - training.closure0 - iteration 12: loss = 2.519605354997771
2023-04-09 01:16:25,324 - INFO - training.closure0 - iteration 13: loss = 13.160040509387212
2023-04-09 01:16:25,342 - INFO - training.closure0 - iteration 14: loss = 1.5161541773702871
2023-04-09 01:16:25,363 - INFO - training.closure0 - iteration 15: loss = 6.844487399355927
2023-04-09 01:16:25,384 - INFO - training.closure0 - iteration 16: loss = 0.6365403375702378
2023-04-09 01:16:25,405 - INFO - training.closure0 - iteration 17: loss = 68007394.18548295
2023-04-09 01:16:25,422 - INFO - training.closure0 - iteration 18: loss = 8233.941691343465
2023-04-09 01:16:25,438 - INFO - training.closure0 - iteration 19: loss = 133.92189862503324
2023-04-09 01:16:25,453 - INFO - training.closure0 - iteration 20: loss = 4.885359647928925
2023-04-09 01:16:25,472 - INFO - training.closure0 - iteration 21: loss = 0.6200063864942665
2023-04-09 01:16:25,487 - INFO - training.closure0 - iteration 22: loss = 0.5568721350996637
2023-04-09 01:16:25,504 - INFO - training.closure0 - iteration 23: loss = 10.046970772682586
2023-04-09 01:16:25,521 - INFO - training.closure0 - iteration 24: loss = -0.1309067942288713
2023-04-09 01:16:25,540 - INFO - training.closure0 - iteration 25: loss = -0.15518564015952446
2023-04-09 01:16:25,555 - INFO - training.closure0 - iteration 26: loss = -0.259099454248783
2023-04-09 01:16:25,573 - INFO - training.closure0 - iteration 27: loss = 0.36078466047196034
2023-04-09 01:16:25,588 - INFO - training.closure0 - iteration 28: loss = -0.32676221626166924
2023-04-09 01:16:25,605 - INFO - training.closure0 - iteration 29: loss = -0.33202868416252057
2023-04-09 01:16:25,620 - INFO - training.closure0 - iteration 30: loss = -0.34546975377093814
2023-04-09 01:16:25,636 - INFO - training.closure0 - iteration 31: loss = -0.35709333309138136
2023-04-09 01:16:25,652 - INFO - training.closure0 - iteration 32: loss = -0.42288204606865276
2023-04-09 01:16:25,668 - INFO - training.closure0 - iteration 33: loss = 1.5366840880328416
2023-04-09 01:16:25,683 - INFO - training.closure0 - iteration 34: loss = -0.5465243303943239
2023-04-09 01:16:25,699 - INFO - training.closure0 - iteration 35: loss = -0.6060490758462784
2023-04-09 01:16:25,715 - INFO - training.closure0 - iteration 36: loss = -0.7487901197794862
2023-04-09 01:16:25,731 - INFO - training.closure0 - iteration 37: loss = 947.8015174158302
2023-04-09 01:16:25,747 - INFO - training.closure0 - iteration 38: loss = 15.966398215751425
2023-04-09 01:16:25,764 - INFO - training.closure0 - iteration 39: loss = -0.6548850142149172
2023-04-09 01:16:25,780 - INFO - training.closure0 - iteration 40: loss = -0.9261001750817626
2023-04-09 01:16:25,797 - INFO - training.closure0 - iteration 41: loss = -0.9849599503492648
2023-04-09 01:16:25,814 - INFO - training.closure0 - iteration 42: loss = 0.504279294644499
2023-04-09 01:16:25,830 - INFO - training.closure0 - iteration 43: loss = -1.0173207193046299
2023-04-09 01:16:25,845 - INFO - training.closure0 - iteration 44: loss = -1.0579561957317105
2023-04-09 01:16:25,862 - INFO - training.closure0 - iteration 45: loss = -1.1173613130302875
2023-04-09 01:16:25,877 - INFO - training.closure0 - iteration 46: loss = -1.1371540502887245
2023-04-09 01:16:25,892 - INFO - training.closure0 - iteration 47: loss = -1.1546807855628367
2023-04-09 01:16:25,908 - INFO - training.closure0 - iteration 48: loss = -1.2051060356375733
2023-04-09 01:16:25,924 - INFO - training.closure0 - iteration 49: loss = -1.2416332827682992
2023-04-09 01:16:25,940 - INFO - training.closure0 - iteration 50: loss = -1.2912033820865347
2023-04-09 01:16:25,956 - INFO - training.closure0 - iteration 51: loss = -1.233811229277785
2023-04-09 01:16:25,972 - INFO - training.closure0 - iteration 52: loss = -1.303542350400072
2023-04-09 01:16:25,988 - INFO - training.closure0 - iteration 53: loss = -1.3066341537279211
2023-04-09 01:16:26,005 - INFO - training.closure0 - iteration 54: loss = -1.3108799965861795
2023-04-09 01:16:26,021 - INFO - training.closure0 - iteration 55: loss = -1.3108469040838249
2023-04-09 01:16:26,037 - INFO - training.closure0 - iteration 56: loss = -1.3112378911280063
2023-04-09 01:16:26,054 - INFO - training.closure0 - iteration 57: loss = -1.3115413924622863
2023-04-09 01:16:26,070 - INFO - training.closure0 - iteration 58: loss = -1.3116296642987424
2023-04-09 01:16:26,085 - INFO - training.closure0 - iteration 59: loss = -1.3117965720635192
2023-04-09 01:16:26,101 - INFO - training.closure0 - iteration 60: loss = -1.3121960868651108
2023-04-09 01:16:26,117 - INFO - training.closure0 - iteration 61: loss = -1.3132613080774018
2023-04-09 01:16:26,135 - INFO - training.closure0 - iteration 62: loss = -1.3159957015002415
2023-04-09 01:16:26,155 - INFO - training.closure0 - iteration 63: loss = -1.3229858493613669
2023-04-09 01:16:26,175 - INFO - training.closure0 - iteration 64: loss = -1.3405734607902304
2023-04-09 01:16:26,195 - INFO - training.closure0 - iteration 65: loss = -1.3825822502241971
2023-04-09 01:16:26,214 - INFO - training.closure0 - iteration 66: loss = 1.4943103569155691
2023-04-09 01:16:26,233 - INFO - training.closure0 - iteration 67: loss = -1.4859937460660804
2023-04-09 01:16:26,252 - INFO - training.closure0 - iteration 68: loss = 3355167.9431795934
2023-04-09 01:16:26,267 - INFO - training.closure0 - iteration 69: loss = -1.4860066561682226
2023-04-09 01:16:26,283 - INFO - training.closure0 - iteration 70: loss = 336.3527368413361
2023-04-09 01:16:26,298 - INFO - training.closure0 - iteration 71: loss = -0.9347465397618075
2023-04-09 01:16:26,313 - INFO - training.closure0 - iteration 72: loss = -1.502787249323848
2023-04-09 01:16:26,329 - INFO - training.closure0 - iteration 73: loss = 111.11589097059431
2023-04-09 01:16:26,344 - INFO - training.closure0 - iteration 74: loss = -2.519617834871995
2023-04-09 01:16:26,360 - INFO - training.closure0 - iteration 75: loss = 0.20139395476305122
2023-04-09 01:16:26,376 - INFO - training.closure0 - iteration 76: loss = -2.570900362561395
2023-04-09 01:16:26,392 - INFO - training.closure0 - iteration 77: loss = -2.576495570646026
2023-04-09 01:16:26,408 - INFO - training.closure0 - iteration 78: loss = 904.282500359937
2023-04-09 01:16:26,426 - INFO - training.closure0 - iteration 79: loss = 25.77970346406353
2023-04-09 01:16:26,445 - INFO - training.closure0 - iteration 80: loss = -2.466776491762867
2023-04-09 01:16:26,463 - INFO - training.closure0 - iteration 81: loss = -2.578412170290364
2023-04-09 01:16:26,479 - INFO - training.closure0 - iteration 82: loss = -2.658637109537934
2023-04-09 01:16:26,495 - INFO - training.closure0 - iteration 83: loss = -2.786534157207182
2023-04-09 01:16:26,514 - INFO - training.closure0 - iteration 84: loss = -2.46592968232396
2023-04-09 01:16:26,531 - INFO - training.closure0 - iteration 85: loss = -2.992983371833665
2023-04-09 01:16:26,546 - INFO - training.closure0 - iteration 86: loss = -3.2284685168134457
2023-04-09 01:16:26,571 - INFO - training.closure0 - iteration 87: loss = 11.948976952469028
2023-04-09 01:16:26,596 - INFO - training.closure0 - iteration 88: loss = -3.570830095287211
2023-04-09 01:16:26,622 - INFO - training.closure0 - iteration 89: loss = -3.7347302915522196
2023-04-09 01:16:26,642 - INFO - training.closure0 - iteration 90: loss = 2142846.7436158005
2023-04-09 01:16:26,663 - INFO - training.closure0 - iteration 91: loss = -3.7347411349991377
2023-04-09 01:16:26,678 - INFO - training.closure0 - iteration 92: loss = -3.710608543899813
2023-04-09 01:16:26,694 - INFO - training.closure0 - iteration 93: loss = -4.010586227412284
2023-04-09 01:16:26,709 - INFO - training.closure0 - iteration 94: loss = 13.889327763914434
2023-04-09 01:16:26,724 - INFO - training.closure0 - iteration 95: loss = -4.023178576828468
2023-04-09 01:16:26,739 - INFO - training.closure0 - iteration 96: loss = -4.2955020565246995
2023-04-09 01:16:26,757 - INFO - training.closure0 - iteration 97: loss = -2.2179183513565093
2023-04-09 01:16:26,783 - INFO - training.closure0 - iteration 98: loss = -4.66706130279284
2023-04-09 01:16:26,809 - INFO - training.closure0 - iteration 99: loss = -5.039289643371281
2023-04-09 01:16:26,835 - INFO - training.closure0 - iteration 100: loss = -4.489356478468102
2023-04-09 01:16:26,858 - INFO - training.closure0 - iteration 101: loss = -5.29701666134569
2023-04-09 01:16:26,874 - INFO - training.closure0 - iteration 102: loss = 14.032807632563962
2023-04-09 01:16:26,889 - INFO - training.closure0 - iteration 103: loss = -6.050335716400602
2023-04-09 01:16:26,905 - INFO - training.closure0 - iteration 104: loss = -6.129373595869923
2023-04-09 01:16:26,921 - INFO - training.closure0 - iteration 105: loss = 503995.3992094277
2023-04-09 01:16:26,938 - INFO - training.closure0 - iteration 106: loss = -6.1296198280676455
2023-04-09 01:16:26,953 - INFO - training.closure0 - iteration 107: loss = 1.6802870866004345
2023-04-09 01:16:26,976 - INFO - training.closure0 - iteration 108: loss = -6.063739776658486
2023-04-09 01:16:26,993 - INFO - training.closure0 - iteration 109: loss = -6.205878303386401
2023-04-09 01:16:27,011 - INFO - training.closure0 - iteration 110: loss = -6.1645352796356665
2023-04-09 01:16:27,029 - INFO - training.closure0 - iteration 111: loss = -6.345891431375645
2023-04-09 01:16:27,048 - INFO - training.closure0 - iteration 112: loss = 0.7322196233225555
2023-04-09 01:16:27,068 - INFO - training.closure0 - iteration 113: loss = -6.259528206006495
2023-04-09 01:16:27,084 - INFO - training.closure0 - iteration 114: loss = -6.381504910075254
2023-04-09 01:16:27,101 - INFO - training.closure0 - iteration 115: loss = -6.385591399155915
2023-04-09 01:16:27,117 - INFO - training.closure0 - iteration 116: loss = -6.395524991665771
2023-04-09 01:16:27,133 - INFO - training.closure0 - iteration 117: loss = -6.422719116489286
2023-04-09 01:16:27,149 - INFO - training.closure0 - iteration 118: loss = -6.4339149959889035
2023-04-09 01:16:27,164 - INFO - training.closure0 - iteration 119: loss = -6.440984992415034
2023-04-09 01:16:27,181 - INFO - training.closure0 - iteration 120: loss = -6.455791392763345
2023-04-09 01:16:27,206 - INFO - training.closure0 - iteration 121: loss = -6.456891407718308
2023-04-09 01:16:27,231 - INFO - training.closure0 - iteration 122: loss = -6.45691023999
2023-04-09 01:16:27,257 - INFO - training.closure0 - iteration 123: loss = -6.456911664534027
2023-04-09 01:16:27,282 - INFO - training.closure0 - iteration 124: loss = -6.456911675832708
2023-04-09 01:16:27,298 - INFO - training.closure0 - iteration 125: loss = -6.456911675911272
2023-04-09 01:16:27,315 - INFO - training.closure0 - iteration 126: loss = -6.456911675911375
2023-04-09 01:16:27,323 - INFO - training.pre_train_full - a0 mean: [2.99944253 3.00062146]
2023-04-09 01:16:27,324 - INFO - training.pre_train_full - a0 var: [1.00409421e-04 8.44174479e-05]
2023-04-09 01:16:27,327 - INFO - training.pre_train_full - a0 covar: [[0.00010040942145918117, -5.508685899637074e-06], [-5.508685899637074e-06, 8.441744785847008e-05]]
2023-04-09 01:16:32,822 - INFO - training.closure - iteration 0: loss = 62359824.020628296
2023-04-09 01:16:38,882 - INFO - training.closure - iteration 1: loss = 204312.30775920485
2023-04-09 01:16:45,068 - INFO - training.closure - iteration 2: loss = 197330.7752178658
2023-04-09 01:16:51,327 - INFO - training.closure - iteration 3: loss = 148311.72330491638
2023-04-09 01:16:57,493 - INFO - training.closure - iteration 4: loss = 111611.23008020375
2023-04-09 01:17:03,849 - INFO - training.closure - iteration 5: loss = 102475.32584916311
2023-04-09 01:17:09,785 - INFO - training.closure - iteration 6: loss = 74149.42999920041
2023-04-09 01:17:15,959 - INFO - training.closure - iteration 7: loss = 45824.23685295621
2023-04-09 01:17:22,045 - INFO - training.closure - iteration 8: loss = 33174.667729492015
2023-04-09 01:17:28,132 - INFO - training.closure - iteration 9: loss = 30723.382938119135
2023-04-09 01:17:34,294 - INFO - training.closure - iteration 10: loss = 29630.569349102323
2023-04-09 01:17:40,394 - INFO - training.closure - iteration 11: loss = 27376.415364769204
2023-04-09 01:17:46,631 - INFO - training.closure - iteration 12: loss = 22367.88004923942
2023-04-09 01:17:52,755 - INFO - training.closure - iteration 13: loss = 13278.434757036339
2023-04-09 01:17:58,846 - INFO - training.closure - iteration 14: loss = 5514.441344731296
2023-04-09 01:18:05,025 - INFO - training.closure - iteration 15: loss = 4372.4423459259915
2023-04-09 01:18:11,111 - INFO - training.closure - iteration 16: loss = 1095.2960720102938
2023-04-09 01:18:17,242 - INFO - training.closure - iteration 17: loss = 576.1786026132518
2023-04-09 01:18:23,406 - INFO - training.closure - iteration 18: loss = 216.492344110182
2023-04-09 01:18:29,614 - INFO - training.closure - iteration 19: loss = 109.74991542443372
2023-04-09 01:18:35,784 - INFO - training.closure - iteration 20: loss = 75.55631520104494
2023-04-09 01:18:41,850 - INFO - training.closure - iteration 21: loss = 41.100097485747455
2023-04-09 01:18:48,006 - INFO - training.closure - iteration 22: loss = 29.464053964902906
2023-04-09 01:18:54,181 - INFO - training.closure - iteration 23: loss = 19.792795403387025
2023-04-09 01:19:00,273 - INFO - training.closure - iteration 24: loss = 13.466942335450671
2023-04-09 01:19:06,358 - INFO - training.closure - iteration 25: loss = 11.085346796550983
2023-04-09 01:19:12,465 - INFO - training.closure - iteration 26: loss = 9.047801098895746
2023-04-09 01:19:18,563 - INFO - training.closure - iteration 27: loss = 8.190810473094322
2023-04-09 01:19:24,734 - INFO - training.closure - iteration 28: loss = 7.641192194500656
2023-04-09 01:19:30,819 - INFO - training.closure - iteration 29: loss = 7.438450968020978
2023-04-09 01:19:36,905 - INFO - training.closure - iteration 30: loss = 6.932512454570175
2023-04-09 01:19:43,126 - INFO - training.closure - iteration 31: loss = 6.216748647631065
2023-04-09 01:19:49,330 - INFO - training.closure - iteration 32: loss = 5.360616444266601
2023-04-09 01:19:55,410 - INFO - training.closure - iteration 33: loss = 4.530477360283625
2023-04-09 01:20:01,610 - INFO - training.closure - iteration 34: loss = 3.6443160109644026
2023-04-09 01:20:07,685 - INFO - training.closure - iteration 35: loss = 3.1184772021789566
2023-04-09 01:20:13,923 - INFO - training.closure - iteration 36: loss = 2.8339723338502356
2023-04-09 01:20:19,995 - INFO - training.closure - iteration 37: loss = 2.0990624400274163
2023-04-09 01:20:26,112 - INFO - training.closure - iteration 38: loss = 1.8989651986009068
2023-04-09 01:20:32,407 - INFO - training.closure - iteration 39: loss = 1.5596188477698005
2023-04-09 01:20:38,507 - INFO - training.closure - iteration 40: loss = 1.335873869086476
2023-04-09 01:20:44,523 - INFO - training.closure - iteration 41: loss = 0.9390263952864277
2023-04-09 01:20:50,911 - INFO - training.closure - iteration 42: loss = 0.7124673666345211
2023-04-09 01:20:57,002 - INFO - training.closure - iteration 43: loss = 0.6402982730997007
2023-04-09 01:21:03,174 - INFO - training.closure - iteration 44: loss = 0.6262948356574247
2023-04-09 01:21:09,270 - INFO - training.closure - iteration 45: loss = 0.6102821096682289
2023-04-09 01:21:15,349 - INFO - training.closure - iteration 46: loss = 0.5816654340166663
2023-04-09 01:21:21,514 - INFO - training.closure - iteration 47: loss = 0.48392463216369297
2023-04-09 01:21:27,712 - INFO - training.closure - iteration 48: loss = 0.30380359709068827
2023-04-09 01:21:33,782 - INFO - training.closure - iteration 49: loss = 0.11031826553197543
2023-04-09 01:21:39,956 - INFO - training.closure - iteration 50: loss = -0.0132133552937157
2023-04-09 01:21:46,062 - INFO - training.closure - iteration 51: loss = -0.14930263412598066
2023-04-09 01:21:52,243 - INFO - training.closure - iteration 52: loss = -0.25705582081476575
2023-04-09 01:21:58,402 - INFO - training.closure - iteration 53: loss = -0.36334068842028344
2023-04-09 01:22:04,555 - INFO - training.closure - iteration 54: loss = -0.45963976231969284
2023-04-09 01:22:10,735 - INFO - training.closure - iteration 55: loss = -0.5056840636614215
2023-04-09 01:22:16,804 - INFO - training.closure - iteration 56: loss = -0.5463092795016156
2023-04-09 01:22:22,834 - INFO - training.closure - iteration 57: loss = -0.5775656209654842
2023-04-09 01:22:29,002 - INFO - training.closure - iteration 58: loss = -0.6054306750517737
2023-04-09 01:22:35,093 - INFO - training.closure - iteration 59: loss = -0.6255061168481522
2023-04-09 01:22:41,275 - INFO - training.closure - iteration 60: loss = -0.6558806047118075
2023-04-09 01:22:47,360 - INFO - training.closure - iteration 61: loss = -0.7199259328157792
2023-04-09 01:22:53,491 - INFO - training.closure - iteration 62: loss = -0.8934099656274546
2023-04-09 01:22:59,660 - INFO - training.closure - iteration 63: loss = 131.83631405966833
2023-04-09 01:23:05,780 - INFO - training.closure - iteration 64: loss = -1.1738586638010746
2023-04-09 01:23:11,972 - INFO - training.closure - iteration 65: loss = -1.3701584645245815
2023-04-09 01:23:18,040 - INFO - training.closure - iteration 66: loss = 1.3302683250858557e+77
2023-04-09 01:23:24,118 - INFO - training.closure - iteration 67: loss = 1.5033598668353526e+46
2023-04-09 01:23:30,290 - INFO - training.closure - iteration 68: loss = 9.788042174900493e+19
2023-04-09 01:23:36,437 - INFO - training.closure - iteration 69: loss = 7.147087955451017e+16
2023-04-09 01:23:42,370 - INFO - training.closure - iteration 70: loss = 1187564219.6656325
2023-04-09 01:23:48,491 - INFO - training.closure - iteration 71: loss = 5007.759945416987
2023-04-09 01:23:54,409 - INFO - training.closure - iteration 72: loss = 9.715366780173452
2023-04-09 01:24:00,577 - INFO - training.closure - iteration 73: loss = -1.1620105108863452
2023-04-09 01:24:06,663 - INFO - training.closure - iteration 74: loss = -1.4676848409312537
2023-04-09 01:24:12,737 - INFO - training.closure - iteration 75: loss = -1.4214063380526358
2023-04-09 01:24:18,908 - INFO - training.closure - iteration 76: loss = -1.550762020730227
2023-04-09 01:24:25,001 - INFO - training.closure - iteration 77: loss = 4.856844517508285
2023-04-09 01:24:31,085 - INFO - training.closure - iteration 78: loss = -1.48420029513567
2023-04-09 01:24:37,269 - INFO - training.closure - iteration 79: loss = -1.6768388569828803
2023-04-09 01:24:43,455 - INFO - training.closure - iteration 80: loss = -1.625952032341357
2023-04-09 01:24:49,672 - INFO - training.closure - iteration 81: loss = -1.821771908945082
2023-04-09 01:24:55,760 - INFO - training.closure - iteration 82: loss = -2.034847820012161
2023-04-09 01:25:01,876 - INFO - training.closure - iteration 83: loss = -2.1282033165930154
2023-04-09 01:25:08,143 - INFO - training.closure - iteration 84: loss = -2.1889803388954965
2023-04-09 01:25:14,287 - INFO - training.closure - iteration 85: loss = -2.285776336976956
2023-04-09 01:25:20,361 - INFO - training.closure - iteration 86: loss = -2.436575407282195
2023-04-09 01:25:26,532 - INFO - training.closure - iteration 87: loss = -2.6276515059636485
2023-04-09 01:25:32,623 - INFO - training.closure - iteration 88: loss = -0.6700679571591168
2023-04-09 01:25:38,813 - INFO - training.closure - iteration 89: loss = -2.7998557209748642
2023-04-09 01:25:44,895 - INFO - training.closure - iteration 90: loss = -2.915595118923792
2023-04-09 01:25:50,991 - INFO - training.closure - iteration 91: loss = -3.023113671267763
2023-04-09 01:25:56,981 - INFO - training.closure - iteration 92: loss = -3.1493961048082513
2023-04-09 01:26:03,112 - INFO - training.closure - iteration 93: loss = -3.342002807408371
2023-04-09 01:26:09,186 - INFO - training.closure - iteration 94: loss = -3.563456147123244
2023-04-09 01:26:15,351 - INFO - training.closure - iteration 95: loss = -3.738260502760534
2023-04-09 01:26:21,444 - INFO - training.closure - iteration 96: loss = -3.760303662004438
2023-04-09 01:26:27,622 - INFO - training.closure - iteration 97: loss = -3.838467717969053
2023-04-09 01:26:33,700 - INFO - training.closure - iteration 98: loss = -3.8751263323898346
2023-04-09 01:26:39,790 - INFO - training.closure - iteration 99: loss = -3.8949066149310876
2023-04-09 01:26:45,959 - INFO - training.closure - iteration 100: loss = -3.9230560952595255
2023-04-09 01:26:52,021 - INFO - training.closure - iteration 101: loss = -3.975783154408644
2023-04-09 01:26:58,128 - INFO - training.closure - iteration 102: loss = -4.054170581937726
2023-04-09 01:27:04,297 - INFO - training.closure - iteration 103: loss = -4.170716603739566
2023-04-09 01:27:10,391 - INFO - training.closure - iteration 104: loss = -4.117447597043457
2023-04-09 01:27:16,568 - INFO - training.closure - iteration 105: loss = -4.290718971077105
2023-04-09 01:27:22,628 - INFO - training.closure - iteration 106: loss = -4.338214399773294
2023-04-09 01:27:28,761 - INFO - training.closure - iteration 107: loss = -4.451898630900223
2023-04-09 01:27:34,919 - INFO - training.closure - iteration 108: loss = -4.489690548575459
2023-04-09 01:27:41,000 - INFO - training.closure - iteration 109: loss = -4.528770803322498
2023-04-09 01:27:47,051 - INFO - training.closure - iteration 110: loss = -4.641119408541486
2023-04-09 01:27:53,127 - INFO - training.closure - iteration 111: loss = -4.733462359323029
2023-04-09 01:27:59,087 - INFO - training.closure - iteration 112: loss = -4.85677832385304
2023-04-09 01:28:05,268 - INFO - training.closure - iteration 113: loss = -4.90694593927319
2023-04-09 01:28:11,411 - INFO - training.closure - iteration 114: loss = -4.938670127828817
2023-04-09 01:28:17,472 - INFO - training.closure - iteration 115: loss = -4.984414813573656
2023-04-09 01:28:23,670 - INFO - training.closure - iteration 116: loss = -5.0844823110873945
2023-04-09 01:28:29,871 - INFO - training.closure - iteration 117: loss = 4.909335675814496
2023-04-09 01:28:35,952 - INFO - training.closure - iteration 118: loss = -4.911508449954187
2023-04-09 01:28:42,219 - INFO - training.closure - iteration 119: loss = -5.148894376274077
2023-04-09 01:28:48,304 - INFO - training.closure - iteration 120: loss = -4.866626977146197
2023-04-09 01:28:54,576 - INFO - training.closure - iteration 121: loss = -5.224959611913483
2023-04-09 01:29:00,565 - INFO - training.closure - iteration 122: loss = -4.618583293441965
2023-04-09 01:29:06,649 - INFO - training.closure - iteration 123: loss = -5.280745484876657
2023-04-09 01:29:12,912 - INFO - training.closure - iteration 124: loss = -5.332779216330942
2023-04-09 01:29:19,004 - INFO - training.closure - iteration 125: loss = -5.096557375304407
2023-04-09 01:29:25,150 - INFO - training.closure - iteration 126: loss = -5.358214201752281
2023-04-09 01:29:31,316 - INFO - training.closure - iteration 127: loss = -5.44579864804717
2023-04-09 01:29:37,403 - INFO - training.closure - iteration 128: loss = -5.570585104974334
2023-04-09 01:29:43,582 - INFO - training.closure - iteration 129: loss = -5.508748014281372
2023-04-09 01:29:49,665 - INFO - training.closure - iteration 130: loss = -5.624585837011829
2023-04-09 01:29:55,579 - INFO - training.closure - iteration 131: loss = -5.2893022937601
2023-04-09 01:30:01,924 - INFO - training.closure - iteration 132: loss = -5.661011777859196
2023-04-09 01:30:08,016 - INFO - training.closure - iteration 133: loss = -5.717861460052969
2023-04-09 01:30:14,051 - INFO - training.closure - iteration 134: loss = -5.80518378129579
2023-04-09 01:30:20,341 - INFO - training.closure - iteration 135: loss = -5.851232925783447
2023-04-09 01:30:26,432 - INFO - training.closure - iteration 136: loss = -5.992698703020594
2023-04-09 01:30:32,612 - INFO - training.closure - iteration 137: loss = -5.312508570679687
2023-04-09 01:30:38,720 - INFO - training.closure - iteration 138: loss = -6.029868838961111
2023-04-09 01:30:44,821 - INFO - training.closure - iteration 139: loss = -6.132114055046848
2023-04-09 01:30:51,010 - INFO - training.closure - iteration 140: loss = -6.229114920699476
2023-04-09 01:30:57,099 - INFO - training.closure - iteration 141: loss = -6.301520157267714
2023-04-09 01:31:03,294 - INFO - training.closure - iteration 142: loss = -6.248534861042165
2023-04-09 01:31:09,459 - INFO - training.closure - iteration 143: loss = -6.321628864321382
2023-04-09 01:31:15,623 - INFO - training.closure - iteration 144: loss = -6.335133890892395
2023-04-09 01:31:21,871 - INFO - training.closure - iteration 145: loss = -6.3651961934289405
2023-04-09 01:31:27,954 - INFO - training.closure - iteration 146: loss = -6.442577535641783
2023-04-09 01:31:34,040 - INFO - training.closure - iteration 147: loss = -6.489399135000201
2023-04-09 01:31:40,205 - INFO - training.closure - iteration 148: loss = -6.585092268423305
2023-04-09 01:31:46,342 - INFO - training.closure - iteration 149: loss = -6.639114095843536
2023-04-09 01:31:52,401 - INFO - training.closure - iteration 150: loss = -6.615476705472197
2023-04-09 01:31:58,422 - INFO - training.closure - iteration 151: loss = -6.7154786728485885
2023-04-09 01:32:04,573 - INFO - training.closure - iteration 152: loss = -6.780706763941907
2023-04-09 01:32:10,596 - INFO - training.closure - iteration 153: loss = -6.912737785496154
2023-04-09 01:32:16,677 - INFO - training.closure - iteration 154: loss = -6.956713811276888
2023-04-09 01:32:22,844 - INFO - training.closure - iteration 155: loss = -6.9133827290108965
2023-04-09 01:32:28,990 - INFO - training.closure - iteration 156: loss = -6.980645194408524
2023-04-09 01:32:35,073 - INFO - training.closure - iteration 157: loss = -6.991531496048956
2023-04-09 01:32:41,201 - INFO - training.closure - iteration 158: loss = -6.9970252347894295
2023-04-09 01:32:47,400 - INFO - training.closure - iteration 159: loss = -7.003716395608999
2023-04-09 01:32:53,468 - INFO - training.closure - iteration 160: loss = -7.0163365483557225
2023-04-09 01:32:59,642 - INFO - training.closure - iteration 161: loss = -7.030463865191257
2023-04-09 01:33:05,880 - INFO - training.closure - iteration 162: loss = -7.040671518860265
2023-04-09 01:33:12,127 - INFO - training.closure - iteration 163: loss = -7.057257465387389
2023-04-09 01:33:18,420 - INFO - training.closure - iteration 164: loss = -7.081628643666104
2023-04-09 01:33:24,565 - INFO - training.closure - iteration 165: loss = -7.082908080938916
2023-04-09 01:33:30,650 - INFO - training.closure - iteration 166: loss = -7.100070485450333
2023-04-09 01:33:36,842 - INFO - training.closure - iteration 167: loss = -7.126486145892333
2023-04-09 01:33:42,924 - INFO - training.closure - iteration 168: loss = -7.1542081411731395
2023-04-09 01:33:49,050 - INFO - training.closure - iteration 169: loss = -7.168496883400484
2023-04-09 01:33:55,161 - INFO - training.closure - iteration 170: loss = -7.18506329173682
2023-04-09 01:34:01,250 - INFO - training.closure - iteration 171: loss = -7.189454482784828
2023-04-09 01:34:07,468 - INFO - training.closure - iteration 172: loss = -7.18833323599509
2023-04-09 01:34:13,545 - INFO - training.closure - iteration 173: loss = -7.192463185837764
2023-04-09 01:34:19,619 - INFO - training.closure - iteration 174: loss = -7.194507982569547
2023-04-09 01:34:25,784 - INFO - training.closure - iteration 175: loss = -7.195293563499768
2023-04-09 01:34:31,874 - INFO - training.closure - iteration 176: loss = -7.197764948433492
2023-04-09 01:34:38,043 - INFO - training.closure - iteration 177: loss = -7.202589725267078
2023-04-09 01:34:44,227 - INFO - training.closure - iteration 178: loss = -7.213363223539835
2023-04-09 01:34:50,331 - INFO - training.closure - iteration 179: loss = -7.2306697577457175
2023-04-09 01:34:56,982 - INFO - training.closure - iteration 180: loss = -7.245900680123258
2023-04-09 01:35:03,073 - INFO - training.closure - iteration 181: loss = -7.260831231058624
2023-04-09 01:35:09,161 - INFO - training.closure - iteration 182: loss = -7.286216708953046
2023-04-09 01:35:15,372 - INFO - training.closure - iteration 183: loss = -7.323299843118756
2023-04-09 01:35:21,491 - INFO - training.closure - iteration 184: loss = -7.405062717289594
2023-04-09 01:35:27,713 - INFO - training.closure - iteration 185: loss = -7.370241096476834
2023-04-09 01:35:33,808 - INFO - training.closure - iteration 186: loss = -7.431013037231997
2023-04-09 01:35:39,751 - INFO - training.closure - iteration 187: loss = -7.309212428842173
2023-04-09 01:35:46,029 - INFO - training.closure - iteration 188: loss = -7.435057170174417
2023-04-09 01:35:52,106 - INFO - training.closure - iteration 189: loss = -7.439702201623417
2023-04-09 01:35:58,208 - INFO - training.closure - iteration 190: loss = -7.44145447798707
2023-04-09 01:36:04,370 - INFO - training.closure - iteration 191: loss = -7.4474582257338335
2023-04-09 01:36:10,483 - INFO - training.closure - iteration 192: loss = -7.447945623863687
2023-04-09 01:36:16,631 - INFO - training.closure - iteration 193: loss = -7.449412529761732
2023-04-09 01:36:22,658 - INFO - training.closure - iteration 194: loss = -7.452241052665534
2023-04-09 01:36:28,735 - INFO - training.closure - iteration 195: loss = -7.453631053973907
2023-04-09 01:36:34,939 - INFO - training.closure - iteration 196: loss = -7.4544206397502375
2023-04-09 01:36:41,149 - INFO - training.closure - iteration 197: loss = -7.454809465226979
2023-04-09 01:36:47,224 - INFO - training.closure - iteration 198: loss = -7.457136739703172
2023-04-09 01:36:53,373 - INFO - training.closure - iteration 199: loss = -7.461864056356212
2023-04-09 01:36:59,549 - INFO - training.closure - iteration 200: loss = -7.465871905295028
2023-04-09 01:37:05,714 - INFO - training.closure - iteration 201: loss = -7.471106045487304
2023-04-09 01:37:11,863 - INFO - training.closure - iteration 202: loss = -7.474420079994127
2023-04-09 01:37:17,985 - INFO - training.closure - iteration 203: loss = -7.4858768848762995
2023-04-09 01:37:24,189 - INFO - training.closure - iteration 204: loss = -7.499682763763651
2023-04-09 01:37:30,265 - INFO - training.closure - iteration 205: loss = -7.512865767162553
2023-04-09 01:37:36,434 - INFO - training.closure - iteration 206: loss = -7.515662786839183
2023-04-09 01:37:42,556 - INFO - training.closure - iteration 207: loss = -7.518153785427087
2023-04-09 01:37:48,696 - INFO - training.closure - iteration 208: loss = -7.522972315174308
2023-04-09 01:37:54,913 - INFO - training.closure - iteration 209: loss = -7.525838244588487
2023-04-09 01:38:01,082 - INFO - training.closure - iteration 210: loss = -7.531895672990068
2023-04-09 01:38:07,162 - INFO - training.closure - iteration 211: loss = -7.540281090217374
2023-04-09 01:38:13,328 - INFO - training.closure - iteration 212: loss = -7.548440487257552
2023-04-09 01:38:19,431 - INFO - training.closure - iteration 213: loss = -7.553909344478258
2023-04-09 01:38:25,604 - INFO - training.closure - iteration 214: loss = -7.556289811585851
2023-04-09 01:38:31,686 - INFO - training.closure - iteration 215: loss = -7.5632421336647395
2023-04-09 01:38:37,761 - INFO - training.closure - iteration 216: loss = -7.568405936796504
2023-04-09 01:38:43,932 - INFO - training.closure - iteration 217: loss = -7.574500721644507
2023-04-09 01:38:49,956 - INFO - training.closure - iteration 218: loss = -7.585000783372913
2023-04-09 01:38:56,035 - INFO - training.closure - iteration 219: loss = -7.603792942180473
2023-04-09 01:39:02,187 - INFO - training.closure - iteration 220: loss = -7.621061389728507
2023-04-09 01:39:08,267 - INFO - training.closure - iteration 221: loss = -7.626612108475328
2023-04-09 01:39:14,445 - INFO - training.closure - iteration 222: loss = -7.669267528013779
2023-04-09 01:39:20,482 - INFO - training.closure - iteration 223: loss = -7.681771986275197
2023-04-09 01:39:26,663 - INFO - training.closure - iteration 224: loss = -7.684016261341001
2023-04-09 01:39:33,011 - INFO - training.closure - iteration 225: loss = -7.704486221894333
2023-04-09 01:39:39,088 - INFO - training.closure - iteration 226: loss = -7.710879056929631
2023-04-09 01:39:45,167 - INFO - training.closure - iteration 227: loss = -7.716853722274113
2023-04-09 01:39:51,364 - INFO - training.closure - iteration 228: loss = -7.721825601747106
2023-04-09 01:39:57,454 - INFO - training.closure - iteration 229: loss = -7.7244878154184295
2023-04-09 01:40:03,631 - INFO - training.closure - iteration 230: loss = -7.726242896369431
2023-04-09 01:40:09,706 - INFO - training.closure - iteration 231: loss = -7.728200374236854
2023-04-09 01:40:15,805 - INFO - training.closure - iteration 232: loss = -7.731625747076773
2023-04-09 01:40:22,056 - INFO - training.closure - iteration 233: loss = -7.736332231379466
2023-04-09 01:40:28,128 - INFO - training.closure - iteration 234: loss = -7.739838389411803
2023-04-09 01:40:34,213 - INFO - training.closure - iteration 235: loss = -7.73791552334662
2023-04-09 01:40:40,369 - INFO - training.closure - iteration 236: loss = -7.744898885046632
2023-04-09 01:40:46,475 - INFO - training.closure - iteration 237: loss = -7.746783354691328
2023-04-09 01:40:52,639 - INFO - training.closure - iteration 238: loss = -7.7488480487492994
2023-04-09 01:40:58,759 - INFO - training.closure - iteration 239: loss = -7.749697128828785
2023-04-09 01:41:04,833 - INFO - training.closure - iteration 240: loss = -7.750698033632089
2023-04-09 01:41:10,993 - INFO - training.closure - iteration 241: loss = -7.540270792053486
2023-04-09 01:41:17,070 - INFO - training.closure - iteration 242: loss = -7.751121379818805
2023-04-09 01:41:23,137 - INFO - training.closure - iteration 243: loss = -7.75254968431516
2023-04-09 01:41:29,258 - INFO - training.closure - iteration 244: loss = -7.753189623659505
2023-04-09 01:41:35,286 - INFO - training.closure - iteration 245: loss = -7.754833930318954
2023-04-09 01:41:41,446 - INFO - training.closure - iteration 246: loss = -7.757500773489944
2023-04-09 01:41:47,613 - INFO - training.closure - iteration 247: loss = -7.759489738907522
2023-04-09 01:41:53,777 - INFO - training.closure - iteration 248: loss = -7.753615745733063
2023-04-09 01:41:59,941 - INFO - training.closure - iteration 249: loss = -7.764180603548578
2023-04-09 01:42:06,023 - INFO - training.closure - iteration 250: loss = -7.773019193140205
2023-04-09 01:42:12,093 - INFO - training.closure - iteration 251: loss = -7.78478499901457
2023-04-09 01:42:18,303 - INFO - training.closure - iteration 252: loss = -7.504417138288365
2023-04-09 01:42:24,405 - INFO - training.closure - iteration 253: loss = -7.785840401352257
2023-04-09 01:42:30,560 - INFO - training.closure - iteration 254: loss = -7.789938535765881
2023-04-09 01:42:36,674 - INFO - training.closure - iteration 255: loss = -7.795006192456579
2023-04-09 01:42:42,755 - INFO - training.closure - iteration 256: loss = -7.801680057245187
2023-04-09 01:42:48,915 - INFO - training.closure - iteration 257: loss = -7.8219120310443255
2023-04-09 01:42:54,988 - INFO - training.closure - iteration 258: loss = -7.8343842115133135
2023-04-09 01:43:01,069 - INFO - training.closure - iteration 259: loss = -7.851034651899043
2023-04-09 01:43:07,265 - INFO - training.closure - iteration 260: loss = -7.857618866350986
2023-04-09 01:43:13,343 - INFO - training.closure - iteration 261: loss = -7.865955284697697
2023-04-09 01:43:19,569 - INFO - training.closure - iteration 262: loss = -0.44134122841760703
2023-04-09 01:43:25,644 - INFO - training.closure - iteration 263: loss = -7.866493622385644
2023-04-09 01:43:31,713 - INFO - training.closure - iteration 264: loss = -7.8720385562239485
2023-04-09 01:43:37,869 - INFO - training.closure - iteration 265: loss = -7.876402063940001
2023-04-09 01:43:43,947 - INFO - training.closure - iteration 266: loss = -7.877566379018165
2023-04-09 01:43:50,090 - INFO - training.closure - iteration 267: loss = -7.880225320134703
2023-04-09 01:43:56,311 - INFO - training.closure - iteration 268: loss = -7.8839088565360935
2023-04-09 01:44:02,411 - INFO - training.closure - iteration 269: loss = -7.889499558213125
2023-04-09 01:44:08,575 - INFO - training.closure - iteration 270: loss = -7.899665074904721
2023-04-09 01:44:14,769 - INFO - training.closure - iteration 271: loss = -7.903150927611211
2023-04-09 01:44:20,833 - INFO - training.closure - iteration 272: loss = -7.9103752893014825
2023-04-09 01:44:26,990 - INFO - training.closure - iteration 273: loss = -7.918772139672966
2023-04-09 01:44:33,065 - INFO - training.closure - iteration 274: loss = -7.936498068235784
2023-04-09 01:44:39,140 - INFO - training.closure - iteration 275: loss = -7.944186026140111
2023-04-09 01:44:45,306 - INFO - training.closure - iteration 276: loss = -7.95870973125235
2023-04-09 01:44:51,458 - INFO - training.closure - iteration 277: loss = -7.981690695528173
2023-04-09 01:44:57,622 - INFO - training.closure - iteration 278: loss = -7.4289184520383165
2023-04-09 01:45:03,706 - INFO - training.closure - iteration 279: loss = -7.9788771030908
2023-04-09 01:45:09,779 - INFO - training.closure - iteration 280: loss = -7.989273056172776
2023-04-09 01:45:15,939 - INFO - training.closure - iteration 281: loss = -7.948651962881431
2023-04-09 01:45:21,916 - INFO - training.closure - iteration 282: loss = -8.002794734739279
2023-04-09 01:45:28,063 - INFO - training.closure - iteration 283: loss = -7.936178487716147
2023-04-09 01:45:34,194 - INFO - training.closure - iteration 284: loss = -8.015569151059886
2023-04-09 01:45:40,274 - INFO - training.closure - iteration 285: loss = -8.022493332975998
2023-04-09 01:45:46,471 - INFO - training.closure - iteration 286: loss = -8.037755811518185
2023-04-09 01:45:52,583 - INFO - training.closure - iteration 287: loss = -8.040962040777632
2023-04-09 01:45:58,747 - INFO - training.closure - iteration 288: loss = -8.050539392375445
2023-04-09 01:46:04,927 - INFO - training.closure - iteration 289: loss = -8.054630568595991
2023-04-09 01:46:11,103 - INFO - training.closure - iteration 290: loss = -8.058052974941686
2023-04-09 01:46:17,233 - INFO - training.closure - iteration 291: loss = -8.048032982042272
2023-04-09 01:46:23,474 - INFO - training.closure - iteration 292: loss = -8.06043669239047
2023-04-09 01:46:29,555 - INFO - training.closure - iteration 293: loss = -8.063644861112078
2023-04-09 01:46:35,724 - INFO - training.closure - iteration 294: loss = -8.068221460540784
2023-04-09 01:46:41,792 - INFO - training.closure - iteration 295: loss = -8.072304743638282
2023-04-09 01:46:48,042 - INFO - training.closure - iteration 296: loss = -8.075354532569879
2023-04-09 01:46:54,168 - INFO - training.closure - iteration 297: loss = -8.080466760145363
2023-04-09 01:47:00,289 - INFO - training.closure - iteration 298: loss = -8.086311522816423
2023-04-09 01:47:06,367 - INFO - training.closure - iteration 299: loss = -8.090309338494102
2023-04-09 01:47:12,542 - INFO - training.closure - iteration 300: loss = -8.091880649298735
2023-04-09 01:47:18,660 - INFO - training.closure - iteration 301: loss = -8.09412477527017
2023-04-09 01:47:24,856 - INFO - training.closure - iteration 302: loss = -8.098742256354752
2023-04-09 01:47:30,977 - INFO - training.closure - iteration 303: loss = -8.105777929176636
2023-04-09 01:47:37,007 - INFO - training.closure - iteration 304: loss = -8.106066388310445
2023-04-09 01:47:43,104 - INFO - training.closure - iteration 305: loss = -8.108187546050136
2023-04-09 01:47:49,174 - INFO - training.closure - iteration 306: loss = -8.109340779362128
2023-04-09 01:47:55,243 - INFO - training.closure - iteration 307: loss = -8.110920070625147
2023-04-09 01:48:01,401 - INFO - training.closure - iteration 308: loss = -8.112246833502905
2023-04-09 01:48:07,467 - INFO - training.closure - iteration 309: loss = -8.11364712842548
2023-04-09 01:48:13,621 - INFO - training.closure - iteration 310: loss = -8.116568411138278
2023-04-09 01:48:19,778 - INFO - training.closure - iteration 311: loss = -8.118296581389291
2023-04-09 01:48:25,843 - INFO - training.closure - iteration 312: loss = -8.120539929220971
2023-04-09 01:48:32,002 - INFO - training.closure - iteration 313: loss = -8.12772195282
2023-04-09 01:48:38,087 - INFO - training.closure - iteration 314: loss = -8.129740270886966
2023-04-09 01:48:44,172 - INFO - training.closure - iteration 315: loss = -8.133354421690857
2023-04-09 01:48:50,424 - INFO - training.closure - iteration 316: loss = -8.138027781707432
2023-04-09 01:48:56,499 - INFO - training.closure - iteration 317: loss = -8.145733896787869
2023-04-09 01:49:02,665 - INFO - training.closure - iteration 318: loss = -7.853760864041418
2023-04-09 01:49:08,840 - INFO - training.closure - iteration 319: loss = -8.146573247302202
2023-04-09 01:49:14,970 - INFO - training.closure - iteration 320: loss = -8.143002513507767
2023-04-09 01:49:21,162 - INFO - training.closure - iteration 321: loss = -8.149401976257716
2023-04-09 01:49:27,244 - INFO - training.closure - iteration 322: loss = -8.15278674369579
2023-04-09 01:49:33,314 - INFO - training.closure - iteration 323: loss = -8.12879287114642
2023-04-09 01:49:39,464 - INFO - training.closure - iteration 324: loss = -8.154698770580545
2023-04-09 01:49:45,552 - INFO - training.closure - iteration 325: loss = -8.163349244753416
2023-04-09 01:49:51,713 - INFO - training.closure - iteration 326: loss = -8.182751839509546
2023-04-09 01:49:57,786 - INFO - training.closure - iteration 327: loss = -8.189830214157563
2023-04-09 01:50:03,794 - INFO - training.closure - iteration 328: loss = -8.198769614013868
2023-04-09 01:50:09,940 - INFO - training.closure - iteration 329: loss = -8.20517765441877
2023-04-09 01:50:16,028 - INFO - training.closure - iteration 330: loss = -8.214320020660438
2023-04-09 01:50:22,129 - INFO - training.closure - iteration 331: loss = -8.221742618668888
2023-04-09 01:50:28,270 - INFO - training.closure - iteration 332: loss = -8.225423271729918
2023-04-09 01:50:34,351 - INFO - training.closure - iteration 333: loss = -8.231800147574717
2023-04-09 01:50:40,651 - INFO - training.closure - iteration 334: loss = -8.244097919345625
2023-04-09 01:50:46,943 - INFO - training.closure - iteration 335: loss = -8.255091424885428
2023-04-09 01:50:53,023 - INFO - training.closure - iteration 336: loss = -8.264721996442313
2023-04-09 01:50:59,184 - INFO - training.closure - iteration 337: loss = -8.279591848236628
2023-04-09 01:51:05,269 - INFO - training.closure - iteration 338: loss = -8.286031606847766
2023-04-09 01:51:11,446 - INFO - training.closure - iteration 339: loss = -8.292171986099332
2023-04-09 01:51:17,474 - INFO - training.closure - iteration 340: loss = -8.297131872453509
2023-04-09 01:51:23,547 - INFO - training.closure - iteration 341: loss = -8.2990151442508
2023-04-09 01:51:29,709 - INFO - training.closure - iteration 342: loss = -8.307671670283849
2023-04-09 01:51:35,785 - INFO - training.closure - iteration 343: loss = -8.314422952815873
2023-04-09 01:51:41,855 - INFO - training.closure - iteration 344: loss = -8.292889975837642
2023-04-09 01:51:48,011 - INFO - training.closure - iteration 345: loss = -8.320216849032365
2023-04-09 01:51:54,077 - INFO - training.closure - iteration 346: loss = -8.322884510099458
2023-04-09 01:52:00,249 - INFO - training.closure - iteration 347: loss = -8.323694206795862
2023-04-09 01:52:06,322 - INFO - training.closure - iteration 348: loss = -8.328120430344084
2023-04-09 01:52:12,430 - INFO - training.closure - iteration 349: loss = -8.33268136208366
2023-04-09 01:52:18,715 - INFO - training.closure - iteration 350: loss = -8.336593320702507
2023-04-09 01:52:24,881 - INFO - training.closure - iteration 351: loss = -8.339545131083646
2023-04-09 01:52:30,951 - INFO - training.closure - iteration 352: loss = -8.342004519892729
2023-04-09 01:52:37,104 - INFO - training.closure - iteration 353: loss = -8.302290351293781
2023-04-09 01:52:43,292 - INFO - training.closure - iteration 354: loss = -8.345713999251764
2023-04-09 01:52:49,496 - INFO - training.closure - iteration 355: loss = -8.358914211320013
2023-04-09 01:52:55,565 - INFO - training.closure - iteration 356: loss = -8.400452295032842
2023-04-09 01:53:01,639 - INFO - training.closure - iteration 357: loss = -4.9345000641638395
2023-04-09 01:53:07,831 - INFO - training.closure - iteration 358: loss = -8.292527832508577
2023-04-09 01:53:13,899 - INFO - training.closure - iteration 359: loss = -8.41210410924198
2023-04-09 01:53:20,213 - INFO - training.closure - iteration 360: loss = -8.481068711821909
2023-04-09 01:53:26,355 - INFO - training.closure - iteration 361: loss = -8.513032841779394
2023-04-09 01:53:32,425 - INFO - training.closure - iteration 362: loss = -7.367093998827113
2023-04-09 01:53:38,594 - INFO - training.closure - iteration 363: loss = -8.531564895439425
2023-04-09 01:53:44,669 - INFO - training.closure - iteration 364: loss = -8.581372331846193
2023-04-09 01:53:50,770 - INFO - training.closure - iteration 365: loss = -8.564569043299588
2023-04-09 01:53:56,927 - INFO - training.closure - iteration 366: loss = -8.599256515395433
2023-04-09 01:54:03,044 - INFO - training.closure - iteration 367: loss = -8.622308181074919
2023-04-09 01:54:09,030 - INFO - training.closure - iteration 368: loss = -8.692406787478843
2023-04-09 01:54:15,217 - INFO - training.closure - iteration 369: loss = -8.678541708582458
2023-04-09 01:54:21,351 - INFO - training.closure - iteration 370: loss = -8.759397116840319
2023-04-09 01:54:27,524 - INFO - training.closure - iteration 371: loss = -8.884027703347996
2023-04-09 01:54:33,601 - INFO - training.closure - iteration 372: loss = -8.957435764293653
2023-04-09 01:54:39,773 - INFO - training.closure - iteration 373: loss = -9.079017145049443
2023-04-09 01:54:45,924 - INFO - training.closure - iteration 374: loss = -9.105312196206963
2023-04-09 01:54:52,000 - INFO - training.closure - iteration 375: loss = -8.607578322727253
2023-04-09 01:54:58,073 - INFO - training.closure - iteration 376: loss = -9.161451745200976
2023-04-09 01:55:04,241 - INFO - training.closure - iteration 377: loss = -9.336580137506253
2023-04-09 01:55:10,311 - INFO - training.closure - iteration 378: loss = -9.388131645183568
2023-04-09 01:55:16,499 - INFO - training.closure - iteration 379: loss = -9.402240302563786
2023-04-09 01:55:22,571 - INFO - training.closure - iteration 380: loss = -9.401585304812247
2023-04-09 01:55:28,649 - INFO - training.closure - iteration 381: loss = -9.455315813019219
2023-04-09 01:55:34,830 - INFO - training.closure - iteration 382: loss = -9.491017332944171
2023-04-09 01:55:40,940 - INFO - training.closure - iteration 383: loss = -9.510166188505753
2023-04-09 01:55:47,048 - INFO - training.closure - iteration 384: loss = -9.539712232342637
2023-04-09 01:55:53,203 - INFO - training.closure - iteration 385: loss = -9.582895726441656
2023-04-09 01:55:59,283 - INFO - training.closure - iteration 386: loss = -9.63789729046458
2023-04-09 01:56:05,462 - INFO - training.closure - iteration 387: loss = -9.626324100926599
2023-04-09 01:56:11,538 - INFO - training.closure - iteration 388: loss = -9.672088649762529
2023-04-09 01:56:17,619 - INFO - training.closure - iteration 389: loss = -9.727091132464079
2023-04-09 01:56:23,747 - INFO - training.closure - iteration 390: loss = -9.735947223841194
2023-04-09 01:56:29,903 - INFO - training.closure - iteration 391: loss = -9.790439149483285
2023-04-09 01:56:36,070 - INFO - training.closure - iteration 392: loss = -9.777894442390295
2023-04-09 01:56:42,233 - INFO - training.closure - iteration 393: loss = -9.827007775904594
2023-04-09 01:56:48,313 - INFO - training.closure - iteration 394: loss = -9.869747055133049
2023-04-09 01:56:54,747 - INFO - training.closure - iteration 395: loss = -9.892982418552652
2023-04-09 01:57:00,862 - INFO - training.closure - iteration 396: loss = -9.904893723122434
2023-04-09 01:57:07,007 - INFO - training.closure - iteration 397: loss = -9.888481818268023
2023-04-09 01:57:13,176 - INFO - training.closure - iteration 398: loss = -9.916520342419187
2023-04-09 01:57:19,275 - INFO - training.closure - iteration 399: loss = -8.443142712733971
2023-04-09 01:57:25,359 - INFO - training.closure - iteration 400: loss = -9.919317801250088
2023-04-09 01:57:31,520 - INFO - training.closure - iteration 401: loss = -9.929285899905192
2023-04-09 01:57:37,626 - INFO - training.closure - iteration 402: loss = -9.941664557009775
2023-04-09 01:57:43,808 - INFO - training.closure - iteration 403: loss = -9.962661163451546
2023-04-09 01:57:49,891 - INFO - training.closure - iteration 404: loss = -9.997124798783627
2023-04-09 01:57:55,975 - INFO - training.closure - iteration 405: loss = -9.995522370586318
2023-04-09 01:58:02,144 - INFO - training.closure - iteration 406: loss = -10.01495325754546
2023-04-09 01:58:08,219 - INFO - training.closure - iteration 407: loss = -5.799435333190407
2023-04-09 01:58:14,292 - INFO - training.closure - iteration 408: loss = -9.989305969339796
2023-04-09 01:58:20,450 - INFO - training.closure - iteration 409: loss = -10.073827344892056
2023-04-09 01:58:26,519 - INFO - training.closure - iteration 410: loss = -10.185576952024576
2023-04-09 01:58:32,683 - INFO - training.closure - iteration 411: loss = -10.242032801306697
2023-04-09 01:58:38,752 - INFO - training.closure - iteration 412: loss = -10.303230470461992
2023-04-09 01:58:44,863 - INFO - training.closure - iteration 413: loss = -10.107638963919005
2023-04-09 01:58:51,205 - INFO - training.closure - iteration 414: loss = -10.333730450271482
2023-04-09 01:58:57,321 - INFO - training.closure - iteration 415: loss = -10.361173583826368
2023-04-09 01:59:03,423 - INFO - training.closure - iteration 416: loss = -10.377375821437958
2023-04-09 01:59:09,583 - INFO - training.closure - iteration 417: loss = -10.38274729417548
2023-04-09 01:59:15,673 - INFO - training.closure - iteration 418: loss = -10.365944134214386
2023-04-09 01:59:21,814 - INFO - training.closure - iteration 419: loss = -10.391155781723965
2023-04-09 01:59:27,997 - INFO - training.closure - iteration 420: loss = -10.394809034600105
2023-04-09 01:59:34,081 - INFO - training.closure - iteration 421: loss = -10.413944052555918
2023-04-09 01:59:40,469 - INFO - training.closure - iteration 422: loss = -10.420171711559913
2023-04-09 01:59:46,621 - INFO - training.closure - iteration 423: loss = -8.785762517830227
2023-04-09 01:59:52,693 - INFO - training.closure - iteration 424: loss = -10.421149989025615
2023-04-09 01:59:58,852 - INFO - training.closure - iteration 425: loss = -10.426211836634
2023-04-09 02:00:05,030 - INFO - training.closure - iteration 426: loss = -10.433962184395465
2023-04-09 02:00:11,211 - INFO - training.closure - iteration 427: loss = -10.439259191389418
2023-04-09 02:00:17,311 - INFO - training.closure - iteration 428: loss = -10.464158695689171
2023-04-09 02:00:23,402 - INFO - training.closure - iteration 429: loss = -10.498780944014047
2023-04-09 02:00:29,571 - INFO - training.closure - iteration 430: loss = -10.549141947348513
2023-04-09 02:00:35,801 - INFO - training.closure - iteration 431: loss = 261.3881031164348
2023-04-09 02:00:41,893 - INFO - training.closure - iteration 432: loss = -10.557528740893613
2023-04-09 02:00:48,098 - INFO - training.closure - iteration 433: loss = -10.540020513845562
2023-04-09 02:00:54,186 - INFO - training.closure - iteration 434: loss = -10.593188030977785
2023-04-09 02:01:00,386 - INFO - training.closure - iteration 435: loss = -10.665619091329745
2023-04-09 02:01:06,541 - INFO - training.closure - iteration 436: loss = -10.645678635526519
2023-04-09 02:01:12,724 - INFO - training.closure - iteration 437: loss = -10.70110156363662
2023-04-09 02:01:18,914 - INFO - training.closure - iteration 438: loss = -10.705741039389032
2023-04-09 02:01:25,229 - INFO - training.closure - iteration 439: loss = -10.719871843698892
2023-04-09 02:01:31,297 - INFO - training.closure - iteration 440: loss = -10.731063172917825
2023-04-09 02:01:37,474 - INFO - training.closure - iteration 441: loss = -10.58531538226465
2023-04-09 02:01:43,571 - INFO - training.closure - iteration 442: loss = -10.746626159920186
2023-04-09 02:01:49,801 - INFO - training.closure - iteration 443: loss = -10.753186221990724
2023-04-09 02:01:55,863 - INFO - training.closure - iteration 444: loss = -10.778571902343682
2023-04-09 02:02:01,974 - INFO - training.closure - iteration 445: loss = -10.789167939327308
2023-04-09 02:02:08,189 - INFO - training.closure - iteration 446: loss = -10.797608600558497
2023-04-09 02:02:14,273 - INFO - training.closure - iteration 447: loss = -10.802547941150127
2023-04-09 02:02:20,406 - INFO - training.closure - iteration 448: loss = -10.805906978911766
2023-04-09 02:02:26,532 - INFO - training.closure - iteration 449: loss = -10.808470948211198
2023-04-09 02:02:32,634 - INFO - training.closure - iteration 450: loss = -10.814799276273089
2023-04-09 02:02:38,816 - INFO - training.closure - iteration 451: loss = -10.760952344106466
2023-04-09 02:02:44,959 - INFO - training.closure - iteration 452: loss = -10.81786032538448
2023-04-09 02:02:51,047 - INFO - training.closure - iteration 453: loss = -10.821747892183819
2023-04-09 02:02:57,209 - INFO - training.closure - iteration 454: loss = -10.826993611883989
2023-04-09 02:03:03,365 - INFO - training.closure - iteration 455: loss = -8.2540071016683
2023-04-09 02:03:09,442 - INFO - training.closure - iteration 456: loss = -10.829206704663992
2023-04-09 02:03:15,584 - INFO - training.closure - iteration 457: loss = -10.83360338620318
2023-04-09 02:03:21,630 - INFO - training.closure - iteration 458: loss = -10.83933462598776
2023-04-09 02:03:27,862 - INFO - training.closure - iteration 459: loss = -10.842605640606834
2023-04-09 02:03:33,873 - INFO - training.closure - iteration 460: loss = -10.845918855164843
2023-04-09 02:03:40,091 - INFO - training.closure - iteration 461: loss = -10.85669297945477
2023-04-09 02:03:46,379 - INFO - training.closure - iteration 462: loss = -10.871502284369967
2023-04-09 02:03:52,462 - INFO - training.closure - iteration 463: loss = -10.943219562653844
2023-04-09 02:03:58,547 - INFO - training.closure - iteration 464: loss = -11.015469150848629
2023-04-09 02:04:04,742 - INFO - training.closure - iteration 465: loss = 395.21989721440946
2023-04-09 02:04:10,834 - INFO - training.closure - iteration 466: loss = 2.854197555340374
2023-04-09 02:04:17,079 - INFO - training.closure - iteration 467: loss = -10.7524887730467
2023-04-09 02:04:23,166 - INFO - training.closure - iteration 468: loss = -11.047542526389496
2023-04-09 02:04:29,574 - INFO - training.closure - iteration 469: loss = -10.99540420819978
2023-04-09 02:04:35,874 - INFO - training.closure - iteration 470: loss = -11.086151394630761
2023-04-09 02:04:42,283 - INFO - training.closure - iteration 471: loss = -11.13628401880835
2023-04-09 02:04:48,472 - INFO - training.closure - iteration 472: loss = -11.19630629069903
2023-04-09 02:04:54,555 - INFO - training.closure - iteration 473: loss = -11.223558421189853
2023-04-09 02:05:00,632 - INFO - training.closure - iteration 474: loss = -11.240957526444802
2023-04-09 02:05:06,812 - INFO - training.closure - iteration 475: loss = -11.253758143260143
2023-04-09 02:05:12,898 - INFO - training.closure - iteration 476: loss = -11.260430651418767
2023-04-09 02:05:19,026 - INFO - training.closure - iteration 477: loss = -11.262021615395806
2023-04-09 02:05:25,150 - INFO - training.closure - iteration 478: loss = -11.267468505238153
2023-04-09 02:05:31,470 - INFO - training.closure - iteration 479: loss = -11.279016308428275
2023-04-09 02:05:37,671 - INFO - training.closure - iteration 480: loss = -11.297218690546151
2023-04-09 02:05:43,762 - INFO - training.closure - iteration 481: loss = -11.306392576701997
2023-04-09 02:05:49,889 - INFO - training.closure - iteration 482: loss = -11.310582395461036
2023-04-09 02:05:56,051 - INFO - training.closure - iteration 483: loss = -11.318506153647565
2023-04-09 02:06:02,201 - INFO - training.closure - iteration 484: loss = -11.3226791567383
2023-04-09 02:06:08,241 - INFO - training.closure - iteration 485: loss = -11.323854999854442
2023-04-09 02:06:14,399 - INFO - training.closure - iteration 486: loss = -11.326503905217296
2023-04-09 02:06:20,413 - INFO - training.closure - iteration 487: loss = -11.327926396261573
2023-04-09 02:06:26,584 - INFO - training.closure - iteration 488: loss = -11.330804671932999
2023-04-09 02:06:32,660 - INFO - training.closure - iteration 489: loss = -11.33472644575702
2023-04-09 02:06:38,908 - INFO - training.closure - iteration 490: loss = -11.339104528656243
2023-04-09 02:06:45,172 - INFO - training.closure - iteration 491: loss = -11.340778574481277
2023-04-09 02:06:51,252 - INFO - training.closure - iteration 492: loss = -11.34324785755198
2023-04-09 02:06:57,340 - INFO - training.closure - iteration 493: loss = -11.34482969276372
2023-04-09 02:07:03,616 - INFO - training.closure - iteration 494: loss = -11.347044544209087
2023-04-09 02:07:09,702 - INFO - training.closure - iteration 495: loss = -11.34743978196823
2023-04-09 02:07:16,009 - INFO - training.closure - iteration 496: loss = -11.34754164128211
2023-04-09 02:07:22,087 - INFO - training.closure - iteration 497: loss = -11.34785543959146
2023-04-09 02:07:28,163 - INFO - training.closure - iteration 498: loss = -11.347945177601956
2023-04-09 02:07:34,343 - INFO - training.closure - iteration 499: loss = -11.348345116494905
2023-04-09 02:07:40,426 - INFO - training.closure - iteration 500: loss = -11.3486729798046
2023-04-09 02:07:46,513 - INFO - training.closure - iteration 501: loss = -11.34381601143588
2023-04-09 02:07:52,716 - INFO - training.closure - iteration 502: loss = -11.348909907321627
2023-04-09 02:07:58,802 - INFO - training.closure - iteration 503: loss = -11.350078847215709
2023-04-09 02:08:04,986 - INFO - training.closure - iteration 504: loss = -11.35305606768124
2023-04-09 02:08:11,072 - INFO - training.closure - iteration 505: loss = -11.35757355149817
2023-04-09 02:08:17,149 - INFO - training.closure - iteration 506: loss = -11.363221185552522
2023-04-09 02:08:23,370 - INFO - training.closure - iteration 507: loss = -11.369436966709568
2023-04-09 02:08:29,441 - INFO - training.closure - iteration 508: loss = -11.377973644174007
2023-04-09 02:08:35,524 - INFO - training.closure - iteration 509: loss = 119.07103498909109
2023-04-09 02:08:41,770 - INFO - training.closure - iteration 510: loss = -5.84611966366759
2023-04-09 02:08:47,940 - INFO - training.closure - iteration 511: loss = -11.30861356758707
2023-04-09 02:08:54,105 - INFO - training.closure - iteration 512: loss = -11.381202549412787
2023-04-09 02:09:00,201 - INFO - training.closure - iteration 513: loss = -11.390942365303019
2023-04-09 02:09:06,300 - INFO - training.closure - iteration 514: loss = -11.390577747224338
2023-04-09 02:09:12,515 - INFO - training.closure - iteration 515: loss = -11.395567068548907
2023-04-09 02:09:18,641 - INFO - training.closure - iteration 516: loss = -11.400894539321115
2023-04-09 02:09:24,719 - INFO - training.closure - iteration 517: loss = -11.402992867400114
2023-04-09 02:09:30,874 - INFO - training.closure - iteration 518: loss = -11.40908975581144
2023-04-09 02:09:37,089 - INFO - training.closure - iteration 519: loss = -11.41904200962868
2023-04-09 02:09:43,408 - INFO - training.closure - iteration 520: loss = -11.423217903006435
2023-04-09 02:09:49,600 - INFO - training.closure - iteration 521: loss = -11.427115361414288
2023-04-09 02:09:55,684 - INFO - training.closure - iteration 522: loss = -11.430141507119195
2023-04-09 02:10:01,849 - INFO - training.closure - iteration 523: loss = -11.435313824523123
2023-04-09 02:10:07,933 - INFO - training.closure - iteration 524: loss = -11.440391966137616
2023-04-09 02:10:14,064 - INFO - training.closure - iteration 525: loss = -11.43344945670486
2023-04-09 02:10:20,172 - INFO - training.closure - iteration 526: loss = -11.445855798554664
2023-04-09 02:10:26,303 - INFO - training.closure - iteration 527: loss = -11.44744487916428
2023-04-09 02:10:32,488 - INFO - training.closure - iteration 528: loss = -11.44862228840612
2023-04-09 02:10:38,639 - INFO - training.closure - iteration 529: loss = -11.449144158627261
2023-04-09 02:10:44,738 - INFO - training.closure - iteration 530: loss = -11.45192916870671
2023-04-09 02:10:50,953 - INFO - training.closure - iteration 531: loss = -11.45712010304809
2023-04-09 02:10:57,043 - INFO - training.closure - iteration 532: loss = -11.46828889152457
2023-04-09 02:11:03,144 - INFO - training.closure - iteration 533: loss = -11.476758556984612
2023-04-09 02:11:09,314 - INFO - training.closure - iteration 534: loss = -11.49018572529613
2023-04-09 02:11:15,611 - INFO - training.closure - iteration 535: loss = -11.47558240117326
2023-04-09 02:11:21,758 - INFO - training.closure - iteration 536: loss = -11.500874129883657
2023-04-09 02:11:27,842 - INFO - training.closure - iteration 537: loss = -11.515732551711825
2023-04-09 02:11:33,942 - INFO - training.closure - iteration 538: loss = -11.526994489395562
2023-04-09 02:11:40,119 - INFO - training.closure - iteration 539: loss = -11.536471384936121
2023-04-09 02:11:46,242 - INFO - training.closure - iteration 540: loss = -11.541972673551882
2023-04-09 02:11:52,382 - INFO - training.closure - iteration 541: loss = -11.557814642912948
2023-04-09 02:11:58,513 - INFO - training.closure - iteration 542: loss = -11.554573246439137
2023-04-09 02:12:04,661 - INFO - training.closure - iteration 543: loss = -11.561521980326283
2023-04-09 02:12:10,919 - INFO - training.closure - iteration 544: loss = -11.564963035306821
2023-04-09 02:12:17,115 - INFO - training.closure - iteration 545: loss = -11.567856803355344
2023-04-09 02:12:23,191 - INFO - training.closure - iteration 546: loss = -11.570942999999765
2023-04-09 02:12:29,377 - INFO - training.closure - iteration 547: loss = -11.579140904395668
2023-04-09 02:12:35,483 - INFO - training.closure - iteration 548: loss = -11.587959464199958
2023-04-09 02:12:41,724 - INFO - training.closure - iteration 549: loss = -11.554011799389574
2023-04-09 02:12:47,920 - INFO - training.closure - iteration 550: loss = -11.592699505832853
2023-04-09 02:12:54,021 - INFO - training.closure - iteration 551: loss = -11.598635198983587
2023-04-09 02:13:00,223 - INFO - training.closure - iteration 552: loss = -11.607848177139058
2023-04-09 02:13:06,364 - INFO - training.closure - iteration 553: loss = -11.612682893817817
2023-04-09 02:13:12,444 - INFO - training.closure - iteration 554: loss = -11.61821873770705
2023-04-09 02:13:18,640 - INFO - training.closure - iteration 555: loss = -11.625198794457507
2023-04-09 02:13:24,725 - INFO - training.closure - iteration 556: loss = -11.639783568497513
2023-04-09 02:13:30,822 - INFO - training.closure - iteration 557: loss = -11.65030196157775
2023-04-09 02:13:36,999 - INFO - training.closure - iteration 558: loss = -11.65376063552454
2023-04-09 02:13:43,090 - INFO - training.closure - iteration 559: loss = -11.655511412180143
2023-04-09 02:13:49,340 - INFO - training.closure - iteration 560: loss = -11.659315124086485
2023-04-09 02:13:55,345 - INFO - training.closure - iteration 561: loss = -11.634642897235764
2023-04-09 02:14:01,435 - INFO - training.closure - iteration 562: loss = -11.660201013599576
2023-04-09 02:14:07,611 - INFO - training.closure - iteration 563: loss = -11.662659613070941
2023-04-09 02:14:13,700 - INFO - training.closure - iteration 564: loss = -11.672433718260542
2023-04-09 02:14:19,825 - INFO - training.closure - iteration 565: loss = -11.677953906791362
2023-04-09 02:14:25,991 - INFO - training.closure - iteration 566: loss = -11.163678365422797
2023-04-09 02:14:32,080 - INFO - training.closure - iteration 567: loss = -11.678332798266837
2023-04-09 02:14:38,263 - INFO - training.closure - iteration 568: loss = -11.681530032865545
2023-04-09 02:14:44,415 - INFO - training.closure - iteration 569: loss = -11.682640934044322
2023-04-09 02:14:50,607 - INFO - training.closure - iteration 570: loss = -11.684207125455385
2023-04-09 02:14:56,787 - INFO - training.closure - iteration 571: loss = -11.685981094760137
2023-04-09 02:15:02,888 - INFO - training.closure - iteration 572: loss = -11.687730965793321
2023-04-09 02:15:08,980 - INFO - training.closure - iteration 573: loss = -11.68891563369336
2023-04-09 02:15:15,143 - INFO - training.closure - iteration 574: loss = -11.689866408710799
2023-04-09 02:15:21,171 - INFO - training.closure - iteration 575: loss = -11.688609779460581
2023-04-09 02:15:27,231 - INFO - training.closure - iteration 576: loss = -11.690552060792056
2023-04-09 02:15:33,398 - INFO - training.closure - iteration 577: loss = -11.691621029021679
2023-04-09 02:15:39,488 - INFO - training.closure - iteration 578: loss = -11.692879605501595
2023-04-09 02:15:45,675 - INFO - training.closure - iteration 579: loss = -11.695115317709497
2023-04-09 02:15:51,817 - INFO - training.closure - iteration 580: loss = -11.697269112133295
2023-04-09 02:15:57,923 - INFO - training.closure - iteration 581: loss = -11.700195801075413
2023-04-09 02:16:04,101 - INFO - training.closure - iteration 582: loss = -11.703393109431236
2023-04-09 02:16:10,197 - INFO - training.closure - iteration 583: loss = -11.706062038208774
2023-04-09 02:16:16,341 - INFO - training.closure - iteration 584: loss = -11.708133863315332
2023-04-09 02:16:22,505 - INFO - training.closure - iteration 585: loss = -11.709311253321177
2023-04-09 02:16:28,623 - INFO - training.closure - iteration 586: loss = -11.710245353458006
2023-04-09 02:16:34,828 - INFO - training.closure - iteration 587: loss = -11.710933053014404
2023-04-09 02:16:40,922 - INFO - training.closure - iteration 588: loss = -11.711957368904883
2023-04-09 02:16:46,887 - INFO - training.closure - iteration 589: loss = -11.713526538067757
2023-04-09 02:16:52,907 - INFO - training.closure - iteration 590: loss = -11.714187730613492
2023-04-09 02:16:58,975 - INFO - training.closure - iteration 591: loss = -11.715079501704672
2023-04-09 02:17:05,635 - INFO - training.closure - iteration 592: loss = -11.716721674869792
2023-04-09 02:17:11,712 - INFO - training.closure - iteration 593: loss = -11.678199992163599
2023-04-09 02:17:17,779 - INFO - training.closure - iteration 594: loss = -11.71716151102658
2023-04-09 02:17:23,947 - INFO - training.closure - iteration 595: loss = -11.719940170773478
2023-04-09 02:17:30,016 - INFO - training.closure - iteration 596: loss = -11.70187082227191
2023-04-09 02:17:36,205 - INFO - training.closure - iteration 597: loss = -11.720626806356215
2023-04-09 02:17:42,332 - INFO - training.closure - iteration 598: loss = -11.724644426543406
2023-04-09 02:17:48,546 - INFO - training.closure - iteration 599: loss = -11.729756217698533
2023-04-09 02:17:54,702 - INFO - training.closure - iteration 600: loss = -11.73406800823254
2023-04-09 02:18:00,782 - INFO - training.closure - iteration 601: loss = -11.739900571741119
2023-04-09 02:18:06,981 - INFO - training.closure - iteration 602: loss = -11.74456167856967
2023-04-09 02:18:13,035 - INFO - training.closure - iteration 603: loss = -11.754332012231883
2023-04-09 02:18:19,025 - INFO - training.closure - iteration 604: loss = -11.748893202664718
2023-04-09 02:18:25,247 - INFO - training.closure - iteration 605: loss = -11.758219885078162
2023-04-09 02:18:31,336 - INFO - training.closure - iteration 606: loss = -11.76391039307913
2023-04-09 02:18:37,394 - INFO - training.closure - iteration 607: loss = -11.74195958998983
2023-04-09 02:18:43,641 - INFO - training.closure - iteration 608: loss = -11.765788198560525
2023-04-09 02:18:49,689 - INFO - training.closure - iteration 609: loss = -11.734030888087679
2023-04-09 02:18:55,848 - INFO - training.closure - iteration 610: loss = -11.768052513963163
2023-04-09 02:19:02,080 - INFO - training.closure - iteration 611: loss = -11.771289847602509
2023-04-09 02:19:08,208 - INFO - training.closure - iteration 612: loss = -11.773544779990601
2023-04-09 02:19:14,368 - INFO - training.closure - iteration 613: loss = -11.777909124291007
2023-04-09 02:19:20,516 - INFO - training.closure - iteration 614: loss = -11.788307464445936
2023-04-09 02:19:26,573 - INFO - training.closure - iteration 615: loss = -11.787165087798261
2023-04-09 02:19:32,733 - INFO - training.closure - iteration 616: loss = -11.794863669203183
2023-04-09 02:19:38,813 - INFO - training.closure - iteration 617: loss = -11.800740248672039
2023-04-09 02:19:44,895 - INFO - training.closure - iteration 618: loss = -11.808014648304315
2023-04-09 02:19:51,071 - INFO - training.closure - iteration 619: loss = -11.810635889216673
2023-04-09 02:19:57,196 - INFO - training.closure - iteration 620: loss = -11.820394669702381
2023-04-09 02:20:03,367 - INFO - training.closure - iteration 621: loss = -11.828452248723876
2023-04-09 02:20:09,438 - INFO - training.closure - iteration 622: loss = -11.835341884299119
2023-04-09 02:20:15,509 - INFO - training.closure - iteration 623: loss = -11.84514241891645
2023-04-09 02:20:21,877 - INFO - training.closure - iteration 624: loss = -11.851569659299589
2023-04-09 02:20:27,945 - INFO - training.closure - iteration 625: loss = -11.859896168379262
2023-04-09 02:20:34,041 - INFO - training.closure - iteration 626: loss = -11.857563681903638
2023-04-09 02:20:40,255 - INFO - training.closure - iteration 627: loss = -11.866677389276582
2023-04-09 02:20:46,357 - INFO - training.closure - iteration 628: loss = -11.876532610389994
2023-04-09 02:20:52,555 - INFO - training.closure - iteration 629: loss = -11.833012791127775
2023-04-09 02:20:58,652 - INFO - training.closure - iteration 630: loss = -11.881744861130475
2023-04-09 02:21:04,735 - INFO - training.closure - iteration 631: loss = -11.887362611487342
2023-04-09 02:21:10,898 - INFO - training.closure - iteration 632: loss = -11.890967406868047
2023-04-09 02:21:16,971 - INFO - training.closure - iteration 633: loss = -11.896012245699328
2023-04-09 02:21:23,045 - INFO - training.closure - iteration 634: loss = -11.900032506571423
2023-04-09 02:21:29,192 - INFO - training.closure - iteration 635: loss = -11.88592722192331
2023-04-09 02:21:35,340 - INFO - training.closure - iteration 636: loss = -11.90485515762105
2023-04-09 02:21:41,515 - INFO - training.closure - iteration 637: loss = -11.908952756663089
2023-04-09 02:21:47,722 - INFO - training.closure - iteration 638: loss = -11.91442458797577
2023-04-09 02:21:53,726 - INFO - training.closure - iteration 639: loss = -11.918729590615207
2023-04-09 02:21:59,906 - INFO - training.closure - iteration 640: loss = -11.92347377591319
2023-04-09 02:22:05,924 - INFO - training.closure - iteration 641: loss = -11.931912699718218
2023-04-09 02:22:12,280 - INFO - training.closure - iteration 642: loss = -11.892329497283786
2023-04-09 02:22:18,471 - INFO - training.closure - iteration 643: loss = -11.93562349723139
2023-04-09 02:22:24,602 - INFO - training.closure - iteration 644: loss = -11.95027060977166
2023-04-09 02:22:30,780 - INFO - training.closure - iteration 645: loss = -11.960163514004524
2023-04-09 02:22:36,865 - INFO - training.closure - iteration 646: loss = -11.967833343686777
2023-04-09 02:22:42,904 - INFO - training.closure - iteration 647: loss = -11.976505802636296
2023-04-09 02:22:49,144 - INFO - training.closure - iteration 648: loss = -11.987095013539209
2023-04-09 02:22:55,221 - INFO - training.closure - iteration 649: loss = -11.971428031121809
2023-04-09 02:23:01,305 - INFO - training.closure - iteration 650: loss = -11.995447948858011
2023-04-09 02:23:07,454 - INFO - training.closure - iteration 651: loss = -12.007884581558375
2023-04-09 02:23:13,492 - INFO - training.closure - iteration 652: loss = -12.017556751777487
2023-04-09 02:23:19,584 - INFO - training.closure - iteration 653: loss = -12.023329057625386
2023-04-09 02:23:25,642 - INFO - training.closure - iteration 654: loss = -12.025426253212935
2023-04-09 02:23:31,790 - INFO - training.closure - iteration 655: loss = -12.026950646123206
2023-04-09 02:23:37,979 - INFO - training.closure - iteration 656: loss = -12.029335436535552
2023-04-09 02:23:44,052 - INFO - training.closure - iteration 657: loss = -12.031375196504277
2023-04-09 02:23:50,282 - INFO - training.closure - iteration 658: loss = -12.032716287262563
2023-04-09 02:23:56,555 - INFO - training.closure - iteration 659: loss = -12.034063169134697
2023-04-09 02:24:02,752 - INFO - training.closure - iteration 660: loss = -12.035050762987218
2023-04-09 02:24:08,973 - INFO - training.closure - iteration 661: loss = -12.035982693495455
2023-04-09 02:24:15,015 - INFO - training.closure - iteration 662: loss = -12.036897768175
2023-04-09 02:24:21,111 - INFO - training.closure - iteration 663: loss = -12.035559745171572
2023-04-09 02:24:27,310 - INFO - training.closure - iteration 664: loss = -12.037315352011952
2023-04-09 02:24:33,410 - INFO - training.closure - iteration 665: loss = -12.038252250853388
2023-04-09 02:24:39,569 - INFO - training.closure - iteration 666: loss = -12.038617581174377
2023-04-09 02:24:45,734 - INFO - training.closure - iteration 667: loss = -12.038787717875964
2023-04-09 02:24:51,813 - INFO - training.closure - iteration 668: loss = -12.038932650952152
2023-04-09 02:24:58,076 - INFO - training.closure - iteration 669: loss = -12.039371579292004
2023-04-09 02:25:04,207 - INFO - training.closure - iteration 670: loss = -12.0396490143046
2023-04-09 02:25:10,331 - INFO - training.closure - iteration 671: loss = -12.021160498093412
2023-04-09 02:25:16,554 - INFO - training.closure - iteration 672: loss = -12.039677127127788
2023-04-09 02:25:22,664 - INFO - training.closure - iteration 673: loss = -12.039872534575714
2023-04-09 02:25:28,752 - INFO - training.closure - iteration 674: loss = -12.040023347319845
2023-04-09 02:25:34,929 - INFO - training.closure - iteration 675: loss = -12.04036406108595
2023-04-09 02:25:41,019 - INFO - training.closure - iteration 676: loss = -12.040706403728317
2023-04-09 02:25:47,270 - INFO - training.closure - iteration 677: loss = -12.040939665447901
2023-04-09 02:25:53,258 - INFO - training.closure - iteration 678: loss = -12.041280010145112
2023-04-09 02:25:59,446 - INFO - training.closure - iteration 679: loss = -12.040864506825809
2023-04-09 02:26:05,635 - INFO - training.closure - iteration 680: loss = -12.041622904090447
2023-04-09 02:26:11,768 - INFO - training.closure - iteration 681: loss = -12.042148193893972
2023-04-09 02:26:17,824 - INFO - training.closure - iteration 682: loss = -12.0439221673453
2023-04-09 02:26:23,989 - INFO - training.closure - iteration 683: loss = -12.04559603164131
2023-04-09 02:26:30,081 - INFO - training.closure - iteration 684: loss = -12.04804668067857
2023-04-09 02:26:36,267 - INFO - training.closure - iteration 685: loss = -12.0372283952032
2023-04-09 02:26:42,363 - INFO - training.closure - iteration 686: loss = -12.050238819758414
2023-04-09 02:26:48,459 - INFO - training.closure - iteration 687: loss = -12.05219962525161
2023-04-09 02:26:54,682 - INFO - training.closure - iteration 688: loss = -12.055027429738722
2023-04-09 02:27:00,778 - INFO - training.closure - iteration 689: loss = -12.056462743060267
2023-04-09 02:27:06,886 - INFO - training.closure - iteration 690: loss = -12.055502534356963
2023-04-09 02:27:13,064 - INFO - training.closure - iteration 691: loss = -12.05708431555572
2023-04-09 02:27:19,212 - INFO - training.closure - iteration 692: loss = -12.0576878991792
2023-04-09 02:27:25,397 - INFO - training.closure - iteration 693: loss = -12.058120127218636
2023-04-09 02:27:31,553 - INFO - training.closure - iteration 694: loss = -12.058322691129856
2023-04-09 02:27:37,835 - INFO - training.closure - iteration 695: loss = -12.058584329506353
2023-04-09 02:27:44,028 - INFO - training.closure - iteration 696: loss = -12.057951420638059
2023-04-09 02:27:50,085 - INFO - training.closure - iteration 697: loss = -12.058840918779023
2023-04-09 02:27:56,163 - INFO - training.closure - iteration 698: loss = -12.059034271586118
2023-04-09 02:28:02,359 - INFO - training.closure - iteration 699: loss = -12.059096881653382
2023-04-09 02:28:08,609 - INFO - training.closure - iteration 700: loss = -12.059266683772131
2023-04-09 02:28:14,721 - INFO - training.closure - iteration 701: loss = -12.059351783989118
2023-04-09 02:28:20,810 - INFO - training.closure - iteration 702: loss = -12.059485058964441
2023-04-09 02:28:26,900 - INFO - training.closure - iteration 703: loss = -12.059737860463965
2023-04-09 02:28:33,107 - INFO - training.closure - iteration 704: loss = -12.059930148597052
2023-04-09 02:28:39,205 - INFO - training.closure - iteration 705: loss = -12.060057016879782
2023-04-09 02:28:45,298 - INFO - training.closure - iteration 706: loss = -12.060190784792503
2023-04-09 02:28:51,514 - INFO - training.closure - iteration 707: loss = -12.060257121165952
2023-04-09 02:28:57,643 - INFO - training.closure - iteration 708: loss = -12.06047113267609
2023-04-09 02:29:03,840 - INFO - training.closure - iteration 709: loss = -12.060738617304223
2023-04-09 02:29:09,949 - INFO - training.closure - iteration 710: loss = -12.060417055922759
2023-04-09 02:29:16,049 - INFO - training.closure - iteration 711: loss = -12.060846492721291
2023-04-09 02:29:22,239 - INFO - training.closure - iteration 712: loss = -12.061068515204195
2023-04-09 02:29:28,393 - INFO - training.closure - iteration 713: loss = -12.061241402211742
2023-04-09 02:29:34,507 - INFO - training.closure - iteration 714: loss = -12.061490681964868
2023-04-09 02:29:40,676 - INFO - training.closure - iteration 715: loss = -12.061825203706752
2023-04-09 02:29:46,747 - INFO - training.closure - iteration 716: loss = -12.062364630222014
2023-04-09 02:29:52,911 - INFO - training.closure - iteration 717: loss = -12.062706041763978
2023-04-09 02:29:59,010 - INFO - training.closure - iteration 718: loss = -12.063107033571203
2023-04-09 02:30:05,124 - INFO - training.closure - iteration 719: loss = -12.06329363148014
2023-04-09 02:30:11,359 - INFO - training.closure - iteration 720: loss = -12.06349129908985
2023-04-09 02:30:17,456 - INFO - training.closure - iteration 721: loss = -12.063452046713719
2023-04-09 02:30:23,565 - INFO - training.closure - iteration 722: loss = -12.063762504185103
2023-04-09 02:30:29,757 - INFO - training.closure - iteration 723: loss = -12.064296405234568
2023-04-09 02:30:35,892 - INFO - training.closure - iteration 724: loss = -12.065622117751829
2023-04-09 02:30:42,202 - INFO - training.closure - iteration 725: loss = -12.066393651856274
2023-04-09 02:30:48,410 - INFO - training.closure - iteration 726: loss = -12.067484457119342
2023-04-09 02:30:54,680 - INFO - training.closure - iteration 727: loss = -12.068436221824879
2023-04-09 02:31:00,883 - INFO - training.closure - iteration 728: loss = -12.069532017547113
2023-04-09 02:31:07,130 - INFO - training.closure - iteration 729: loss = -12.07088678656932
2023-04-09 02:31:13,205 - INFO - training.closure - iteration 730: loss = -12.066927114101384
2023-04-09 02:31:19,566 - INFO - training.closure - iteration 731: loss = -12.071844780718283
2023-04-09 02:31:25,748 - INFO - training.closure - iteration 732: loss = -12.07455593682415
2023-04-09 02:31:31,931 - INFO - training.closure - iteration 733: loss = -12.077380568586623
2023-04-09 02:31:38,002 - INFO - training.closure - iteration 734: loss = -12.079041076433533
2023-04-09 02:31:44,085 - INFO - training.closure - iteration 735: loss = -12.080655654887023
2023-04-09 02:31:50,404 - INFO - training.closure - iteration 736: loss = -12.080437041366212
2023-04-09 02:31:56,516 - INFO - training.closure - iteration 737: loss = -12.081872401799028
2023-04-09 02:32:02,818 - INFO - training.closure - iteration 738: loss = -12.083353774991444
2023-04-09 02:32:08,969 - INFO - training.closure - iteration 739: loss = -12.085202297578011
2023-04-09 02:32:14,918 - INFO - training.closure - iteration 740: loss = -12.085803765963036
2023-04-09 02:32:21,054 - INFO - training.closure - iteration 741: loss = -12.086410790669103
2023-04-09 02:32:27,120 - INFO - training.closure - iteration 742: loss = -12.087162254143841
2023-04-09 02:32:33,188 - INFO - training.closure - iteration 743: loss = -12.088128872101196
2023-04-09 02:32:39,346 - INFO - training.closure - iteration 744: loss = -12.071639815614265
2023-04-09 02:32:45,418 - INFO - training.closure - iteration 745: loss = -12.088451950753296
2023-04-09 02:32:51,631 - INFO - training.closure - iteration 746: loss = -12.089839430644503
2023-04-09 02:32:57,651 - INFO - training.closure - iteration 747: loss = -12.091247128424069
2023-04-09 02:33:03,698 - INFO - training.closure - iteration 748: loss = -12.092050811662789
2023-04-09 02:33:09,861 - INFO - training.closure - iteration 749: loss = -12.09269771807075
2023-04-09 02:33:15,921 - INFO - training.closure - iteration 750: loss = -12.08310294359519
2023-04-09 02:33:22,033 - INFO - training.closure - iteration 751: loss = -12.092946132572923
2023-04-09 02:33:28,217 - INFO - training.closure - iteration 752: loss = -12.093616260513592
2023-04-09 02:33:34,279 - INFO - training.closure - iteration 753: loss = -12.09459354310908
2023-04-09 02:33:40,419 - INFO - training.closure - iteration 754: loss = -12.096286566134449
2023-04-09 02:33:46,506 - INFO - training.closure - iteration 755: loss = -12.098478711675838
2023-04-09 02:33:52,577 - INFO - training.closure - iteration 756: loss = -12.101001504756926
2023-04-09 02:33:58,736 - INFO - training.closure - iteration 757: loss = -12.102723515871128
2023-04-09 02:34:04,819 - INFO - training.closure - iteration 758: loss = -12.103640321339654
2023-04-09 02:34:10,891 - INFO - training.closure - iteration 759: loss = -12.104451524354857
2023-04-09 02:34:17,086 - INFO - training.closure - iteration 760: loss = -12.105216265539482
2023-04-09 02:34:23,157 - INFO - training.closure - iteration 761: loss = -12.107301093592055
2023-04-09 02:34:29,316 - INFO - training.closure - iteration 762: loss = -12.109157888130781
2023-04-09 02:34:35,395 - INFO - training.closure - iteration 763: loss = -11.961905618308972
2023-04-09 02:34:41,467 - INFO - training.closure - iteration 764: loss = -12.110497863852054
2023-04-09 02:34:47,654 - INFO - training.closure - iteration 765: loss = -12.112683736572407
2023-04-09 02:34:53,720 - INFO - training.closure - iteration 766: loss = -12.115239915322684
2023-04-09 02:34:59,856 - INFO - training.closure - iteration 767: loss = -12.117384949146668
2023-04-09 02:35:06,098 - INFO - training.closure - iteration 768: loss = -12.123343139831718
2023-04-09 02:35:12,244 - INFO - training.closure - iteration 769: loss = -12.127485791591226
2023-04-09 02:35:18,405 - INFO - training.closure - iteration 770: loss = -12.131212944423083
2023-04-09 02:35:24,483 - INFO - training.closure - iteration 771: loss = -12.129533799761647
2023-04-09 02:35:30,475 - INFO - training.closure - iteration 772: loss = -12.132798806521226
2023-04-09 02:35:36,660 - INFO - training.closure - iteration 773: loss = -12.134562213013448
2023-04-09 02:35:42,723 - INFO - training.closure - iteration 774: loss = -12.136208560821256
2023-04-09 02:35:48,816 - INFO - training.closure - iteration 775: loss = -12.137193049451708
2023-04-09 02:35:54,971 - INFO - training.closure - iteration 776: loss = -12.139400422741755
2023-04-09 02:36:01,046 - INFO - training.closure - iteration 777: loss = -12.14209188389419
2023-04-09 02:36:07,203 - INFO - training.closure - iteration 778: loss = -12.09774286684187
2023-04-09 02:36:13,268 - INFO - training.closure - iteration 779: loss = -12.143690797557108
2023-04-09 02:36:19,310 - INFO - training.closure - iteration 780: loss = -12.146195386390664
2023-04-09 02:36:25,397 - INFO - training.closure - iteration 781: loss = -12.148538546714786
2023-04-09 02:36:31,447 - INFO - training.closure - iteration 782: loss = -12.149681728310181
2023-04-09 02:36:37,531 - INFO - training.closure - iteration 783: loss = -12.150802264432702
2023-04-09 02:36:43,767 - INFO - training.closure - iteration 784: loss = -12.151251633040573
2023-04-09 02:36:49,840 - INFO - training.closure - iteration 785: loss = -12.15176386596449
2023-04-09 02:36:56,007 - INFO - training.closure - iteration 786: loss = -12.152844229392626
2023-04-09 02:37:01,990 - INFO - training.closure - iteration 787: loss = -12.155038392232747
2023-04-09 02:37:08,113 - INFO - training.closure - iteration 788: loss = -12.158432359441523
2023-04-09 02:37:14,277 - INFO - training.closure - iteration 789: loss = -12.159920596052999
2023-04-09 02:37:20,377 - INFO - training.closure - iteration 790: loss = -12.156303895495185
2023-04-09 02:37:26,447 - INFO - training.closure - iteration 791: loss = -12.161211764937612
2023-04-09 02:37:32,598 - INFO - training.closure - iteration 792: loss = -12.162184065991712
2023-04-09 02:37:38,677 - INFO - training.closure - iteration 793: loss = -12.163113936354268
2023-04-09 02:37:44,862 - INFO - training.closure - iteration 794: loss = -12.163969329767712
2023-04-09 02:37:51,004 - INFO - training.closure - iteration 795: loss = -12.164728440476498
2023-04-09 02:37:57,553 - INFO - training.closure - iteration 796: loss = -12.165148633405314
2023-04-09 02:38:04,547 - INFO - training.closure - iteration 797: loss = -12.165356694232807
2023-04-09 02:38:11,431 - INFO - training.closure - iteration 798: loss = -12.165487459015159
2023-04-09 02:38:18,525 - INFO - training.closure - iteration 799: loss = -12.165708691014
2023-04-09 02:38:24,861 - INFO - training.closure - iteration 800: loss = -12.165936013479065
2023-04-09 02:38:30,991 - INFO - training.closure - iteration 801: loss = -12.16600910700011
2023-04-09 02:38:37,158 - INFO - training.closure - iteration 802: loss = -12.166204853434778
2023-04-09 02:38:43,291 - INFO - training.closure - iteration 803: loss = -12.16626689000784
2023-04-09 02:38:49,371 - INFO - training.closure - iteration 804: loss = -12.166323291237703
2023-04-09 02:38:55,570 - INFO - training.closure - iteration 805: loss = -12.16639675885022
2023-04-09 02:39:01,670 - INFO - training.closure - iteration 806: loss = -12.166501878112587
2023-04-09 02:39:07,994 - INFO - training.closure - iteration 807: loss = -12.166704678281498
2023-04-09 02:39:14,180 - INFO - training.closure - iteration 808: loss = -12.166926836162475
2023-04-09 02:39:20,255 - INFO - training.closure - iteration 809: loss = -12.16720989065001
2023-04-09 02:39:26,434 - INFO - training.closure - iteration 810: loss = -12.167723036124194
2023-04-09 02:39:32,513 - INFO - training.closure - iteration 811: loss = -12.16808654888568
2023-04-09 02:39:38,588 - INFO - training.closure - iteration 812: loss = -12.159002310935346
2023-04-09 02:39:44,751 - INFO - training.closure - iteration 813: loss = -12.168137306733598
2023-04-09 02:39:50,922 - INFO - training.closure - iteration 814: loss = -12.168389426937349
2023-04-09 02:39:57,283 - INFO - training.closure - iteration 815: loss = -12.168936888691452
2023-04-09 02:40:03,569 - INFO - training.closure - iteration 816: loss = -12.169704368112555
2023-04-09 02:40:09,646 - INFO - training.closure - iteration 817: loss = -12.170599153885663
2023-04-09 02:40:15,888 - INFO - training.closure - iteration 818: loss = -12.171733517009478
2023-04-09 02:40:22,145 - INFO - training.closure - iteration 819: loss = -12.172717712892135
2023-04-09 02:40:28,291 - INFO - training.closure - iteration 820: loss = -12.173466304596133
2023-04-09 02:40:34,465 - INFO - training.closure - iteration 821: loss = -12.173924767960983
2023-04-09 02:40:40,548 - INFO - training.closure - iteration 822: loss = -12.174542707606015
2023-04-09 02:40:46,614 - INFO - training.closure - iteration 823: loss = -12.175074340878442
2023-04-09 02:40:52,802 - INFO - training.closure - iteration 824: loss = -12.175670578309251
2023-04-09 02:40:58,870 - INFO - training.closure - iteration 825: loss = -12.176044232751389
2023-04-09 02:41:05,091 - INFO - training.closure - iteration 826: loss = -12.176258374955898
2023-04-09 02:41:11,175 - INFO - training.closure - iteration 827: loss = -12.176363128124716
2023-04-09 02:41:17,326 - INFO - training.closure - iteration 828: loss = -12.176526324642893
2023-04-09 02:41:23,486 - INFO - training.closure - iteration 829: loss = -12.176669839949302
2023-04-09 02:41:29,559 - INFO - training.closure - iteration 830: loss = -12.176909184607034
2023-04-09 02:41:35,640 - INFO - training.closure - iteration 831: loss = -12.177210891947436
2023-04-09 02:41:41,837 - INFO - training.closure - iteration 832: loss = -12.177419554686118
2023-04-09 02:41:47,967 - INFO - training.closure - iteration 833: loss = -12.177749580582311
2023-04-09 02:41:54,162 - INFO - training.closure - iteration 834: loss = -12.178211190031247
2023-04-09 02:42:00,204 - INFO - training.closure - iteration 835: loss = -12.178540477003196
2023-04-09 02:42:06,348 - INFO - training.closure - iteration 836: loss = -12.178972835456236
2023-04-09 02:42:12,511 - INFO - training.closure - iteration 837: loss = -12.178786163596325
2023-04-09 02:42:18,588 - INFO - training.closure - iteration 838: loss = -12.179231510060392
2023-04-09 02:42:24,660 - INFO - training.closure - iteration 839: loss = -12.179541835062727
2023-04-09 02:42:30,916 - INFO - training.closure - iteration 840: loss = -12.180083047710276
2023-04-09 02:42:36,988 - INFO - training.closure - iteration 841: loss = -12.180365971394842
2023-04-09 02:42:43,166 - INFO - training.closure - iteration 842: loss = -12.181098452163546
2023-04-09 02:42:49,364 - INFO - training.closure - iteration 843: loss = -12.180551851646218
2023-04-09 02:42:55,441 - INFO - training.closure - iteration 844: loss = -12.181387658011925
2023-04-09 02:43:01,598 - INFO - training.closure - iteration 845: loss = -12.181667213177281
2023-04-09 02:43:07,692 - INFO - training.closure - iteration 846: loss = -12.18177797286188
2023-04-09 02:43:13,759 - INFO - training.closure - iteration 847: loss = -12.181864202661174
2023-04-09 02:43:19,946 - INFO - training.closure - iteration 848: loss = -12.182217769169782
2023-04-09 02:43:25,965 - INFO - training.closure - iteration 849: loss = -12.182364529593833
2023-04-09 02:43:32,022 - INFO - training.closure - iteration 850: loss = -12.182707661949536
2023-04-09 02:43:38,104 - INFO - training.closure - iteration 851: loss = -12.183074012365207
2023-04-09 02:43:44,167 - INFO - training.closure - iteration 852: loss = -12.183399895733665
2023-04-09 02:43:50,358 - INFO - training.closure - iteration 853: loss = -12.183788139039677
2023-04-09 02:43:56,429 - INFO - training.closure - iteration 854: loss = -12.184296525350788
2023-04-09 02:44:02,512 - INFO - training.closure - iteration 855: loss = -12.183933425643282
2023-04-09 02:44:08,729 - INFO - training.closure - iteration 856: loss = -12.18462143983918
2023-04-09 02:44:14,882 - INFO - training.closure - iteration 857: loss = -12.185308152875189
2023-04-09 02:44:21,124 - INFO - training.closure - iteration 858: loss = -12.185545219023112
2023-04-09 02:44:27,189 - INFO - training.closure - iteration 859: loss = -12.18583369271759
2023-04-09 02:44:33,252 - INFO - training.closure - iteration 860: loss = -12.186177588134242
2023-04-09 02:44:39,400 - INFO - training.closure - iteration 861: loss = -12.186661474915274
2023-04-09 02:44:45,464 - INFO - training.closure - iteration 862: loss = -12.187287911411559
2023-04-09 02:44:51,415 - INFO - training.closure - iteration 863: loss = -12.18786522597046
2023-04-09 02:44:57,462 - INFO - training.closure - iteration 864: loss = -12.18810595400122
2023-04-09 02:45:03,411 - INFO - training.closure - iteration 865: loss = -12.18835644369383
2023-04-09 02:45:09,546 - INFO - training.closure - iteration 866: loss = -12.188624228399142
2023-04-09 02:45:15,590 - INFO - training.closure - iteration 867: loss = -12.189117155456925
2023-04-09 02:45:21,748 - INFO - training.closure - iteration 868: loss = -12.189786234064659
2023-04-09 02:45:27,881 - INFO - training.closure - iteration 869: loss = -12.19013142499312
2023-04-09 02:45:33,949 - INFO - training.closure - iteration 870: loss = -12.190194609481525
2023-04-09 02:45:40,121 - INFO - training.closure - iteration 871: loss = -12.190449809290119
2023-04-09 02:45:46,186 - INFO - training.closure - iteration 872: loss = -12.190531347297451
2023-04-09 02:45:52,317 - INFO - training.closure - iteration 873: loss = -12.190633132648355
2023-04-09 02:45:58,472 - INFO - training.closure - iteration 874: loss = -12.18993152029625
2023-04-09 02:46:04,548 - INFO - training.closure - iteration 875: loss = -12.190673462158438
2023-04-09 02:46:10,634 - INFO - training.closure - iteration 876: loss = -12.190779463798137
2023-04-09 02:46:16,824 - INFO - training.closure - iteration 877: loss = -12.191243702420516
2023-04-09 02:46:22,897 - INFO - training.closure - iteration 878: loss = -12.191934310753592
2023-04-09 02:46:29,541 - INFO - training.closure - iteration 879: loss = -12.19258918765764
2023-04-09 02:46:35,655 - INFO - training.closure - iteration 880: loss = -12.171327446189636
2023-04-09 02:46:41,757 - INFO - training.closure - iteration 881: loss = -12.192780554546008
2023-04-09 02:46:47,970 - INFO - training.closure - iteration 882: loss = -12.193526261508527
2023-04-09 02:46:54,042 - INFO - training.closure - iteration 883: loss = -12.194726680695428
2023-04-09 02:47:00,114 - INFO - training.closure - iteration 884: loss = -12.195675377810414
2023-04-09 02:47:06,277 - INFO - training.closure - iteration 885: loss = -12.196409599533496
2023-04-09 02:47:12,423 - INFO - training.closure - iteration 886: loss = -12.196941476290238
2023-04-09 02:47:18,463 - INFO - training.closure - iteration 887: loss = -12.197497134288662
2023-04-09 02:47:24,537 - INFO - training.closure - iteration 888: loss = -12.198029238998947
2023-04-09 02:47:30,604 - INFO - training.closure - iteration 889: loss = -12.198738051935404
2023-04-09 02:47:36,763 - INFO - training.closure - iteration 890: loss = -12.199371294556443
2023-04-09 02:47:42,826 - INFO - training.closure - iteration 891: loss = -12.199738459287502
2023-04-09 02:47:49,011 - INFO - training.closure - iteration 892: loss = -12.200097085142282
2023-04-09 02:47:55,142 - INFO - training.closure - iteration 893: loss = -12.200439987296328
2023-04-09 02:48:01,269 - INFO - training.closure - iteration 894: loss = -12.200625928934274
2023-04-09 02:48:07,455 - INFO - training.closure - iteration 895: loss = -12.20075909505504
2023-04-09 02:48:13,873 - INFO - training.closure - iteration 896: loss = -12.200893449892364
2023-04-09 02:48:19,988 - INFO - training.closure - iteration 897: loss = -12.201156054382192
2023-04-09 02:48:26,154 - INFO - training.closure - iteration 898: loss = -12.201515193958528
2023-04-09 02:48:32,211 - INFO - training.closure - iteration 899: loss = -12.202108413477632
2023-04-09 02:48:38,291 - INFO - training.closure - iteration 900: loss = -12.20278771982872
2023-04-09 02:48:44,497 - INFO - training.closure - iteration 901: loss = -12.203757445057555
2023-04-09 02:48:50,578 - INFO - training.closure - iteration 902: loss = -12.203519366851527
2023-04-09 02:48:56,752 - INFO - training.closure - iteration 903: loss = -12.204062033540218
2023-04-09 02:49:02,825 - INFO - training.closure - iteration 904: loss = -12.204293034555965
2023-04-09 02:49:08,907 - INFO - training.closure - iteration 905: loss = -12.204500782139455
2023-04-09 02:49:15,239 - INFO - training.closure - iteration 906: loss = -12.204558178667025
2023-04-09 02:49:21,295 - INFO - training.closure - iteration 907: loss = -12.204620629925184
2023-04-09 02:49:27,461 - INFO - training.closure - iteration 908: loss = -12.20469935496558
2023-04-09 02:49:33,627 - INFO - training.closure - iteration 909: loss = -12.204840416038564
2023-04-09 02:49:39,699 - INFO - training.closure - iteration 910: loss = -12.20497169871033
2023-04-09 02:49:45,869 - INFO - training.closure - iteration 911: loss = -12.205291884230562
2023-04-09 02:49:51,960 - INFO - training.closure - iteration 912: loss = -12.20552442065345
2023-04-09 02:49:58,037 - INFO - training.closure - iteration 913: loss = -12.205656050314541
2023-04-09 02:50:04,207 - INFO - training.closure - iteration 914: loss = -12.205718092224528
2023-04-09 02:50:10,275 - INFO - training.closure - iteration 915: loss = -12.205849367292874
2023-04-09 02:50:16,345 - INFO - training.closure - iteration 916: loss = -12.205907438315034
2023-04-09 02:50:22,548 - INFO - training.closure - iteration 917: loss = -12.206137250463168
2023-04-09 02:50:28,658 - INFO - training.closure - iteration 918: loss = -12.2063240663826
2023-04-09 02:50:34,841 - INFO - training.closure - iteration 919: loss = -12.206648455106453
2023-04-09 02:50:41,284 - INFO - training.closure - iteration 920: loss = -12.206692502434926
2023-04-09 02:50:47,397 - INFO - training.closure - iteration 921: loss = -12.205910005704588
2023-04-09 02:50:53,569 - INFO - training.closure - iteration 922: loss = -12.206784689698436
2023-04-09 02:50:59,671 - INFO - training.closure - iteration 923: loss = -12.206851449459037
2023-04-09 02:51:05,916 - INFO - training.closure - iteration 924: loss = -12.206970883443784
2023-04-09 02:51:12,147 - INFO - training.closure - iteration 925: loss = -12.20715881859013
2023-04-09 02:51:18,223 - INFO - training.closure - iteration 926: loss = -12.20724359332246
2023-04-09 02:51:24,407 - INFO - training.closure - iteration 927: loss = -12.20714443898178
2023-04-09 02:51:30,587 - INFO - training.closure - iteration 928: loss = -12.207319896129693
2023-04-09 02:51:36,626 - INFO - training.closure - iteration 929: loss = -12.207454514051648
2023-04-09 02:51:42,812 - INFO - training.closure - iteration 930: loss = -12.207594344764754
2023-04-09 02:51:48,906 - INFO - training.closure - iteration 931: loss = -12.207772527733994
2023-04-09 02:51:55,043 - INFO - training.closure - iteration 932: loss = -12.207907557503942
2023-04-09 02:52:01,225 - INFO - training.closure - iteration 933: loss = -12.208025748330078
2023-04-09 02:52:07,287 - INFO - training.closure - iteration 934: loss = -12.208128862237704
2023-04-09 02:52:13,454 - INFO - training.closure - iteration 935: loss = -12.208287829246558
2023-04-09 02:52:19,545 - INFO - training.closure - iteration 936: loss = -12.208549424368137
2023-04-09 02:52:25,763 - INFO - training.closure - iteration 937: loss = -12.208599045272623
2023-04-09 02:52:31,951 - INFO - training.closure - iteration 938: loss = -12.208737742887099
2023-04-09 02:52:38,047 - INFO - training.closure - iteration 939: loss = -12.208804345795365
2023-04-09 02:52:44,080 - INFO - training.closure - iteration 940: loss = -12.208884673105862
2023-04-09 02:52:50,245 - INFO - training.closure - iteration 941: loss = -12.208961133226017
2023-04-09 02:52:56,375 - INFO - training.closure - iteration 942: loss = -12.209127852110559
2023-04-09 02:53:02,577 - INFO - training.closure - iteration 943: loss = -12.208863160711111
2023-04-09 02:53:08,633 - INFO - training.closure - iteration 944: loss = -12.209213276004668
2023-04-09 02:53:14,718 - INFO - training.closure - iteration 945: loss = -12.209342820551942
2023-04-09 02:53:20,890 - INFO - training.closure - iteration 946: loss = -12.209467044578755
2023-04-09 02:53:26,956 - INFO - training.closure - iteration 947: loss = -12.209529187946469
2023-04-09 02:53:33,033 - INFO - training.closure - iteration 948: loss = -12.209600605302132
2023-04-09 02:53:39,205 - INFO - training.closure - iteration 949: loss = -12.209754419718667
2023-04-09 02:53:45,282 - INFO - training.closure - iteration 950: loss = -12.20983693340537
2023-04-09 02:53:51,446 - INFO - training.closure - iteration 951: loss = -12.20993651317285
2023-04-09 02:53:57,591 - INFO - training.closure - iteration 952: loss = -12.209712673690486
2023-04-09 02:54:03,891 - INFO - training.closure - iteration 953: loss = -12.21001331482977
2023-04-09 02:54:10,060 - INFO - training.closure - iteration 954: loss = -12.21010808848387
2023-04-09 02:54:16,137 - INFO - training.closure - iteration 955: loss = -12.21023064791405
2023-04-09 02:54:22,219 - INFO - training.closure - iteration 956: loss = -12.210269168360878
2023-04-09 02:54:28,511 - INFO - training.closure - iteration 957: loss = -12.210383618089931
2023-04-09 02:54:34,611 - INFO - training.closure - iteration 958: loss = -12.210541021558274
2023-04-09 02:54:40,900 - INFO - training.closure - iteration 959: loss = -12.210486714839725
2023-04-09 02:54:47,041 - INFO - training.closure - iteration 960: loss = -12.210773652989257
2023-04-09 02:54:53,115 - INFO - training.closure - iteration 961: loss = -12.210966282507286
2023-04-09 02:54:59,278 - INFO - training.closure - iteration 962: loss = -12.211184151119038
2023-04-09 02:55:05,354 - INFO - training.closure - iteration 963: loss = -12.211402080074905
2023-04-09 02:55:11,432 - INFO - training.closure - iteration 964: loss = -12.20073003014392
2023-04-09 02:55:17,592 - INFO - training.closure - iteration 965: loss = -12.211623645501374
2023-04-09 02:55:23,760 - INFO - training.closure - iteration 966: loss = -12.211923992919733
2023-04-09 02:55:29,933 - INFO - training.closure - iteration 967: loss = -12.212764557164165
2023-04-09 02:55:36,027 - INFO - training.closure - iteration 968: loss = -12.213074432904865
2023-04-09 02:55:42,139 - INFO - training.closure - iteration 969: loss = -12.21335183270513
2023-04-09 02:55:48,427 - INFO - training.closure - iteration 970: loss = -12.211750194411641
2023-04-09 02:55:54,506 - INFO - training.closure - iteration 971: loss = -12.213432439775719
2023-04-09 02:56:00,593 - INFO - training.closure - iteration 972: loss = -12.21367287230115
2023-04-09 02:56:06,689 - INFO - training.closure - iteration 973: loss = -12.213916524897371
2023-04-09 02:56:12,874 - INFO - training.closure - iteration 974: loss = -12.214063027359813
2023-04-09 02:56:19,086 - INFO - training.closure - iteration 975: loss = -12.214180961323349
2023-04-09 02:56:25,239 - INFO - training.closure - iteration 976: loss = -12.214306446607019
2023-04-09 02:56:31,439 - INFO - training.closure - iteration 977: loss = -12.214443339868513
2023-04-09 02:56:37,675 - INFO - training.closure - iteration 978: loss = -12.214690864788249
2023-04-09 02:56:43,790 - INFO - training.closure - iteration 979: loss = -12.214959222691538
2023-04-09 02:56:49,919 - INFO - training.closure - iteration 980: loss = -12.21538304718274
2023-04-09 02:56:56,149 - INFO - training.closure - iteration 981: loss = -12.215994940503158
2023-04-09 02:57:02,223 - INFO - training.closure - iteration 982: loss = -12.216857577256155
2023-04-09 02:57:08,393 - INFO - training.closure - iteration 983: loss = -12.217803619034175
2023-04-09 02:57:14,465 - INFO - training.closure - iteration 984: loss = -12.21862385346305
2023-04-09 02:57:20,536 - INFO - training.closure - iteration 985: loss = -12.219342871396435
2023-04-09 02:57:26,706 - INFO - training.closure - iteration 986: loss = -12.220144972582812
2023-04-09 02:57:32,802 - INFO - training.closure - iteration 987: loss = -12.221324169068161
2023-04-09 02:57:38,734 - INFO - training.closure - iteration 988: loss = -12.22300947726547
2023-04-09 02:57:44,807 - INFO - training.closure - iteration 989: loss = -12.22434266655768
2023-04-09 02:57:50,850 - INFO - training.closure - iteration 990: loss = -12.225518791254826
2023-04-09 02:57:57,023 - INFO - training.closure - iteration 991: loss = -12.225882083443754
2023-04-09 02:58:03,132 - INFO - training.closure - iteration 992: loss = -12.226204286539273
2023-04-09 02:58:09,213 - INFO - training.closure - iteration 993: loss = -12.226349673877062
2023-04-09 02:58:15,377 - INFO - training.closure - iteration 994: loss = -12.226605284806604
2023-04-09 02:58:21,447 - INFO - training.closure - iteration 995: loss = -12.226789583538014
2023-04-09 02:58:27,641 - INFO - training.closure - iteration 996: loss = -12.227095636078825
2023-04-09 02:58:33,629 - INFO - training.closure - iteration 997: loss = -12.227478633450788
2023-04-09 02:58:39,731 - INFO - training.closure - iteration 998: loss = -12.228073317074502
2023-04-09 02:58:45,932 - INFO - training.closure - iteration 999: loss = -12.22648524767795
2023-04-09 02:58:51,873 - INFO - training.closure - iteration 1000: loss = -12.228249061109427
2023-04-09 02:58:57,972 - INFO - training.closure - iteration 1001: loss = -12.228521697714005
2023-04-09 02:59:04,137 - INFO - training.closure - iteration 1002: loss = -12.228835069704592
2023-04-09 02:59:10,217 - INFO - training.closure - iteration 1003: loss = -12.229005532351207
2023-04-09 02:59:16,389 - INFO - training.closure - iteration 1004: loss = -12.229060136740307
2023-04-09 02:59:22,492 - INFO - training.closure - iteration 1005: loss = -12.229077214482281
2023-04-09 02:59:28,514 - INFO - training.closure - iteration 1006: loss = -12.22910409407337
2023-04-09 02:59:34,855 - INFO - training.closure - iteration 1007: loss = -12.229200978373555
2023-04-09 02:59:40,969 - INFO - training.closure - iteration 1008: loss = -12.229179776564493
2023-04-09 02:59:47,048 - INFO - training.closure - iteration 1009: loss = -12.22929594078811
2023-04-09 02:59:53,262 - INFO - training.closure - iteration 1010: loss = -12.229495680487265
2023-04-09 02:59:59,484 - INFO - training.closure - iteration 1011: loss = -12.229713461938262
2023-04-09 03:00:05,623 - INFO - training.closure - iteration 1012: loss = -12.229988517725495
2023-04-09 03:00:11,686 - INFO - training.closure - iteration 1013: loss = -12.230098016035956
2023-04-09 03:00:17,876 - INFO - training.closure - iteration 1014: loss = -12.230231009413295
2023-04-09 03:00:24,082 - INFO - training.closure - iteration 1015: loss = -12.230285588234958
2023-04-09 03:00:30,198 - INFO - training.closure - iteration 1016: loss = -12.23034795592108
2023-04-09 03:00:36,297 - INFO - training.closure - iteration 1017: loss = -12.230421093833222
2023-04-09 03:00:42,484 - INFO - training.closure - iteration 1018: loss = -12.230514935693066
2023-04-09 03:00:48,573 - INFO - training.closure - iteration 1019: loss = -12.230593807434321
2023-04-09 03:00:54,747 - INFO - training.closure - iteration 1020: loss = -12.230635878769899
2023-04-09 03:01:00,920 - INFO - training.closure - iteration 1021: loss = -12.230706420504639
2023-04-09 03:01:07,037 - INFO - training.closure - iteration 1022: loss = -12.230762411775938
2023-04-09 03:01:13,192 - INFO - training.closure - iteration 1023: loss = -12.230838963916618
2023-04-09 03:01:19,326 - INFO - training.closure - iteration 1024: loss = -12.230919441254965
2023-04-09 03:01:25,433 - INFO - training.closure - iteration 1025: loss = -12.23097515176983
2023-04-09 03:01:31,817 - INFO - training.closure - iteration 1026: loss = -12.230985727550568
2023-04-09 03:01:38,027 - INFO - training.closure - iteration 1027: loss = -12.23110531558362
2023-04-09 03:01:44,278 - INFO - training.closure - iteration 1028: loss = -12.2311666382484
2023-04-09 03:01:50,405 - INFO - training.closure - iteration 1029: loss = -12.231247214705736
2023-04-09 03:01:56,497 - INFO - training.closure - iteration 1030: loss = -12.231336086545692
2023-04-09 03:02:02,669 - INFO - training.closure - iteration 1031: loss = -12.230944732949723
2023-04-09 03:02:08,801 - INFO - training.closure - iteration 1032: loss = -12.231392492355388
2023-04-09 03:02:14,912 - INFO - training.closure - iteration 1033: loss = -12.231512749664276
2023-04-09 03:02:21,130 - INFO - training.closure - iteration 1034: loss = -12.231491651295844
2023-04-09 03:02:27,204 - INFO - training.closure - iteration 1035: loss = -12.231533149116146
2023-04-09 03:02:33,491 - INFO - training.closure - iteration 1036: loss = -12.231583689076494
2023-04-09 03:02:39,579 - INFO - training.closure - iteration 1037: loss = -12.231624497587823
2023-04-09 03:02:45,667 - INFO - training.closure - iteration 1038: loss = -12.231663336601187
2023-04-09 03:02:51,844 - INFO - training.closure - iteration 1039: loss = -12.23170690887115
2023-04-09 03:02:58,022 - INFO - training.closure - iteration 1040: loss = -12.23175856897539
2023-04-09 03:03:04,101 - INFO - training.closure - iteration 1041: loss = -12.231820629623334
2023-04-09 03:03:10,277 - INFO - training.closure - iteration 1042: loss = -12.231900120699772
2023-04-09 03:03:16,364 - INFO - training.closure - iteration 1043: loss = -12.231448544805659
2023-04-09 03:03:22,656 - INFO - training.closure - iteration 1044: loss = -12.231923062958725
2023-04-09 03:03:28,754 - INFO - training.closure - iteration 1045: loss = -12.23200249747638
2023-04-09 03:03:34,836 - INFO - training.closure - iteration 1046: loss = -12.232067905724385
2023-04-09 03:03:41,012 - INFO - training.closure - iteration 1047: loss = -12.232116188508495
2023-04-09 03:03:47,106 - INFO - training.closure - iteration 1048: loss = -12.232154633341299
2023-04-09 03:03:53,213 - INFO - training.closure - iteration 1049: loss = -12.232210848345044
2023-04-09 03:03:59,376 - INFO - training.closure - iteration 1050: loss = -12.23224920964224
2023-04-09 03:04:05,452 - INFO - training.closure - iteration 1051: loss = -12.232025107919185
2023-04-09 03:04:11,627 - INFO - training.closure - iteration 1052: loss = -12.23228182700263
2023-04-09 03:04:17,681 - INFO - training.closure - iteration 1053: loss = -12.232309946252293
2023-04-09 03:04:23,857 - INFO - training.closure - iteration 1054: loss = -12.232372067847185
2023-04-09 03:04:30,061 - INFO - training.closure - iteration 1055: loss = -12.232415259256314
2023-04-09 03:04:36,232 - INFO - training.closure - iteration 1056: loss = -12.231350032311344
2023-04-09 03:04:42,537 - INFO - training.closure - iteration 1057: loss = -12.232453220357392
2023-04-09 03:04:48,732 - INFO - training.closure - iteration 1058: loss = -12.23247962714806
2023-04-09 03:04:54,878 - INFO - training.closure - iteration 1059: loss = -12.232512930735538
2023-04-09 03:05:01,080 - INFO - training.closure - iteration 1060: loss = -12.232544908781307
2023-04-09 03:05:07,261 - INFO - training.closure - iteration 1061: loss = -12.230811998809216
2023-04-09 03:05:13,394 - INFO - training.closure - iteration 1062: loss = -12.232565606916218
2023-04-09 03:05:19,623 - INFO - training.closure - iteration 1063: loss = -12.232650063631215
2023-04-09 03:05:25,715 - INFO - training.closure - iteration 1064: loss = -12.23279204601967
2023-04-09 03:05:31,892 - INFO - training.closure - iteration 1065: loss = -12.232840585589015
2023-04-09 03:05:37,949 - INFO - training.closure - iteration 1066: loss = -12.232912330596793
2023-04-09 03:05:44,063 - INFO - training.closure - iteration 1067: loss = -12.232942021900573
2023-04-09 03:05:50,333 - INFO - training.closure - iteration 1068: loss = -12.231878496721304
2023-04-09 03:05:56,696 - INFO - training.closure - iteration 1069: loss = -12.2329444012573
2023-04-09 03:06:02,772 - INFO - training.closure - iteration 1070: loss = -12.232954756231955
2023-04-09 03:06:08,936 - INFO - training.closure - iteration 1071: loss = -12.233022839491019
2023-04-09 03:06:15,019 - INFO - training.closure - iteration 1072: loss = -12.232918432545894
2023-04-09 03:06:21,117 - INFO - training.closure - iteration 1073: loss = -12.23304046818836
2023-04-09 03:06:27,289 - INFO - training.closure - iteration 1074: loss = -12.233089522870417
2023-04-09 03:06:33,463 - INFO - training.closure - iteration 1075: loss = -12.23310760703054
2023-04-09 03:06:39,647 - INFO - training.closure - iteration 1076: loss = -12.233159674226854
2023-04-09 03:06:45,743 - INFO - training.closure - iteration 1077: loss = -12.23321867119202
2023-04-09 03:06:51,895 - INFO - training.closure - iteration 1078: loss = -12.232638844704086
2023-04-09 03:06:58,074 - INFO - training.closure - iteration 1079: loss = -12.233257779652579
2023-04-09 03:07:04,172 - INFO - training.closure - iteration 1080: loss = -12.233353948424025
2023-04-09 03:07:10,395 - INFO - training.closure - iteration 1081: loss = -12.233446815158501
2023-04-09 03:07:16,559 - INFO - training.closure - iteration 1082: loss = -12.233492239004498
2023-04-09 03:07:22,629 - INFO - training.closure - iteration 1083: loss = -12.233511857570274
2023-04-09 03:07:28,811 - INFO - training.closure - iteration 1084: loss = -12.233671320348865
2023-04-09 03:07:34,891 - INFO - training.closure - iteration 1085: loss = -12.233743560246314
2023-04-09 03:07:40,981 - INFO - training.closure - iteration 1086: loss = -12.233880960756267
2023-04-09 03:07:47,153 - INFO - training.closure - iteration 1087: loss = -12.233239500197051
2023-04-09 03:07:53,241 - INFO - training.closure - iteration 1088: loss = -12.233902187090248
2023-04-09 03:07:59,333 - INFO - training.closure - iteration 1089: loss = -12.233975092052404
2023-04-09 03:08:05,512 - INFO - training.closure - iteration 1090: loss = -12.234015329433358
2023-04-09 03:08:11,590 - INFO - training.closure - iteration 1091: loss = -12.234040214324637
2023-04-09 03:08:17,750 - INFO - training.closure - iteration 1092: loss = -12.234067103185101
2023-04-09 03:08:23,852 - INFO - training.closure - iteration 1093: loss = -12.234090596134191
2023-04-09 03:08:29,944 - INFO - training.closure - iteration 1094: loss = -12.234212639667014
2023-04-09 03:08:36,112 - INFO - training.closure - iteration 1095: loss = -12.23426941596409
2023-04-09 03:08:42,175 - INFO - training.closure - iteration 1096: loss = -12.23431571284382
2023-04-09 03:08:48,247 - INFO - training.closure - iteration 1097: loss = -12.234343608186983
2023-04-09 03:08:54,415 - INFO - training.closure - iteration 1098: loss = -12.23437571571766
2023-04-09 03:09:00,592 - INFO - training.closure - iteration 1099: loss = -12.234461365415271
2023-04-09 03:09:06,765 - INFO - training.closure - iteration 1100: loss = -12.234560875966753
2023-04-09 03:09:12,854 - INFO - training.closure - iteration 1101: loss = -12.234626164171495
2023-04-09 03:09:19,009 - INFO - training.closure - iteration 1102: loss = -12.233339195529375
2023-04-09 03:09:25,052 - INFO - training.closure - iteration 1103: loss = -12.23464615316006
2023-04-09 03:09:30,986 - INFO - training.closure - iteration 1104: loss = -12.234708725671156
2023-04-09 03:09:37,025 - INFO - training.closure - iteration 1105: loss = -12.234807578142377
2023-04-09 03:09:43,125 - INFO - training.closure - iteration 1106: loss = -12.234949590377024
2023-04-09 03:09:49,211 - INFO - training.closure - iteration 1107: loss = -12.235115745320783
2023-04-09 03:09:55,395 - INFO - training.closure - iteration 1108: loss = -12.23526170558932
2023-04-09 03:10:01,569 - INFO - training.closure - iteration 1109: loss = -12.23537731047697
2023-04-09 03:10:07,650 - INFO - training.closure - iteration 1110: loss = -12.235463736554916
2023-04-09 03:10:13,829 - INFO - training.closure - iteration 1111: loss = -12.235530943334725
2023-04-09 03:10:19,988 - INFO - training.closure - iteration 1112: loss = -12.235610586890719
2023-04-09 03:10:26,071 - INFO - training.closure - iteration 1113: loss = -12.2356894432832
2023-04-09 03:10:32,247 - INFO - training.closure - iteration 1114: loss = -12.23550669420798
2023-04-09 03:10:38,575 - INFO - training.closure - iteration 1115: loss = -12.235705196464558
2023-04-09 03:10:44,941 - INFO - training.closure - iteration 1116: loss = -12.23575855817559
2023-04-09 03:10:51,084 - INFO - training.closure - iteration 1117: loss = -12.235789912936859
2023-04-09 03:10:57,182 - INFO - training.closure - iteration 1118: loss = -12.23582622878692
2023-04-09 03:11:03,368 - INFO - training.closure - iteration 1119: loss = -12.235869750858988
2023-04-09 03:11:09,561 - INFO - training.closure - iteration 1120: loss = -12.23590606716515
2023-04-09 03:11:15,648 - INFO - training.closure - iteration 1121: loss = -12.235935602245139
2023-04-09 03:11:21,815 - INFO - training.closure - iteration 1122: loss = -12.235952417733781
2023-04-09 03:11:27,996 - INFO - training.closure - iteration 1123: loss = -12.235967989662697
2023-04-09 03:11:34,251 - INFO - training.closure - iteration 1124: loss = -12.235985423279693
2023-04-09 03:11:40,347 - INFO - training.closure - iteration 1125: loss = -12.236009969316012
2023-04-09 03:11:46,476 - INFO - training.closure - iteration 1126: loss = -12.236050723095868
2023-04-09 03:11:52,776 - INFO - training.closure - iteration 1127: loss = -12.236048663977906
2023-04-09 03:11:58,841 - INFO - training.closure - iteration 1128: loss = -12.236081687548193
2023-04-09 03:12:04,922 - INFO - training.closure - iteration 1129: loss = -12.236124547211935
2023-04-09 03:12:11,078 - INFO - training.closure - iteration 1130: loss = -12.236192344864925
2023-04-09 03:12:17,209 - INFO - training.closure - iteration 1131: loss = -12.236291987467759
2023-04-09 03:12:23,389 - INFO - training.closure - iteration 1132: loss = -12.236394868045817
2023-04-09 03:12:29,463 - INFO - training.closure - iteration 1133: loss = -12.23653985479977
2023-04-09 03:12:35,529 - INFO - training.closure - iteration 1134: loss = -12.236284682611702
2023-04-09 03:12:41,749 - INFO - training.closure - iteration 1135: loss = -12.2366190387236
2023-04-09 03:12:47,812 - INFO - training.closure - iteration 1136: loss = -12.236757487422073
2023-04-09 03:12:53,993 - INFO - training.closure - iteration 1137: loss = -12.236803414048254
2023-04-09 03:13:00,062 - INFO - training.closure - iteration 1138: loss = -12.236860933695379
2023-04-09 03:13:06,129 - INFO - training.closure - iteration 1139: loss = -12.236995896367596
2023-04-09 03:13:12,381 - INFO - training.closure - iteration 1140: loss = -12.230216042434481
2023-04-09 03:13:18,492 - INFO - training.closure - iteration 1141: loss = -12.237031923233697
2023-04-09 03:13:24,557 - INFO - training.closure - iteration 1142: loss = -12.237287885339294
2023-04-09 03:13:30,901 - INFO - training.closure - iteration 1143: loss = -12.237467659966782
2023-04-09 03:13:36,992 - INFO - training.closure - iteration 1144: loss = -12.2364511544002
2023-04-09 03:13:43,258 - INFO - training.closure - iteration 1145: loss = -12.237531140760169
2023-04-09 03:13:49,319 - INFO - training.closure - iteration 1146: loss = -12.2375872007806
2023-04-09 03:13:55,388 - INFO - training.closure - iteration 1147: loss = -12.237605684120963
2023-04-09 03:14:01,643 - INFO - training.closure - iteration 1148: loss = -12.237660573613216
2023-04-09 03:14:07,781 - INFO - training.closure - iteration 1149: loss = -12.23768104414199
2023-04-09 03:14:13,848 - INFO - training.closure - iteration 1150: loss = -12.237724173415154
2023-04-09 03:14:20,006 - INFO - training.closure - iteration 1151: loss = -12.23776992487274
2023-04-09 03:14:25,978 - INFO - training.closure - iteration 1152: loss = -12.237866217882267
2023-04-09 03:14:32,191 - INFO - training.closure - iteration 1153: loss = -12.238017916588552
2023-04-09 03:14:38,257 - INFO - training.closure - iteration 1154: loss = -12.238121514516118
2023-04-09 03:14:44,356 - INFO - training.closure - iteration 1155: loss = -12.238203354553644
2023-04-09 03:14:50,571 - INFO - training.closure - iteration 1156: loss = -12.23825325570422
2023-04-09 03:14:56,648 - INFO - training.closure - iteration 1157: loss = -12.238324497388954
2023-04-09 03:15:02,706 - INFO - training.closure - iteration 1158: loss = -12.238387062154473
2023-04-09 03:15:08,884 - INFO - training.closure - iteration 1159: loss = -12.23866649632789
2023-04-09 03:15:13,917 - INFO - main.experiment - train - RMSE_b at iteration 0 = 0.09468121367400223
2023-04-09 03:15:13,917 - INFO - main.experiment - train - RMSE_b at last iteration = 0.05638861794564083
2023-04-09 03:15:13,917 - INFO - main.experiment - train - RMSE_a at iteration 0 = 0.6737714297368669
2023-04-09 03:15:13,917 - INFO - main.experiment - train - RMSE_a at last iteration = 0.04862311723262286
2023-04-09 03:15:13,917 - INFO - main.experiment - train - LOGPDF_b at iteration 0 = 86.06819439017369
2023-04-09 03:15:13,917 - INFO - main.experiment - train - LOGPDF_b at last iteration = -3.0446781661402604
2023-04-09 03:15:13,917 - INFO - main.experiment - train - LOGPDF_a at iteration 0 = 683.0056106632667
2023-04-09 03:15:13,917 - INFO - main.experiment - train - LOGPDF_a at last iteration = -3.256568143004508
2023-04-09 03:15:13,918 - INFO - main.experiment - train - LOSS at iteration 0 = 769.0738050534403
2023-04-09 03:15:13,918 - INFO - main.experiment - train - LOSS at last iteration = -6.301246309144768
2023-04-09 03:15:14,558 - INFO - main.experiment - test - RMSE_b at iteration 0 = 0.09416978178346881
2023-04-09 03:15:14,558 - INFO - main.experiment - test - RMSE_b at last iteration = 0.08024562618392286
2023-04-09 03:15:14,558 - INFO - main.experiment - test - RMSE_a at iteration 0 = 0.049057488758789104
2023-04-09 03:15:14,558 - INFO - main.experiment - test - RMSE_a at last iteration = 0.07854056967145442
2023-04-09 03:15:14,558 - INFO - main.experiment - test - LOGPDF_b at iteration 0 = 11.64194669571977
2023-04-09 03:15:14,558 - INFO - main.experiment - test - LOGPDF_b at last iteration = 224.2138056156446
2023-04-09 03:15:14,558 - INFO - main.experiment - test - LOGPDF_a at iteration 0 = -3.777827601372419
2023-04-09 03:15:14,559 - INFO - main.experiment - test - LOGPDF_a at last iteration = 2688.823005816341
2023-04-09 03:15:14,559 - INFO - main.experiment - test - LOSS at iteration 0 = 7.864119094347352
2023-04-09 03:15:14,559 - INFO - main.experiment - test - LOSS at last iteration = 2913.0368114319854
2023-04-09 03:15:14,566 - INFO - main.experiment - deep = 100 - plot = False - sigma0 = 0.01
2023-04-09 03:15:14,592 - INFO - training.pre_train_full - empirical mean of x0: 3.0000319984329904
2023-04-09 03:15:14,602 - INFO - training.pre_train_full - initial loss: 29.646962478756947
2023-04-09 03:15:14,620 - INFO - training.closure0 - iteration 0: loss = 29.646962478756947
2023-04-09 03:15:14,638 - INFO - training.closure0 - iteration 1: loss = 11.398714196079238
2023-04-09 03:15:14,654 - INFO - training.closure0 - iteration 2: loss = 8.739990639860741
2023-04-09 03:15:14,669 - INFO - training.closure0 - iteration 3: loss = 6.115606439248072
2023-04-09 03:15:14,685 - INFO - training.closure0 - iteration 4: loss = 4.913746823902767
2023-04-09 03:15:14,700 - INFO - training.closure0 - iteration 5: loss = 4.1285799725328225
2023-04-09 03:15:14,715 - INFO - training.closure0 - iteration 6: loss = 3.447443301834518
2023-04-09 03:15:14,730 - INFO - training.closure0 - iteration 7: loss = 2.7327724590636335
2023-04-09 03:15:14,745 - INFO - training.closure0 - iteration 8: loss = 3.091074814255957
2023-04-09 03:15:14,760 - INFO - training.closure0 - iteration 9: loss = 2.372055518879715
2023-04-09 03:15:14,779 - INFO - training.closure0 - iteration 10: loss = 1.9259133532553383
2023-04-09 03:15:14,798 - INFO - training.closure0 - iteration 11: loss = 5627.618523950736
2023-04-09 03:15:14,817 - INFO - training.closure0 - iteration 12: loss = 35.4347416258257
2023-04-09 03:15:14,838 - INFO - training.closure0 - iteration 13: loss = 1.6253471464895146
2023-04-09 03:15:14,860 - INFO - training.closure0 - iteration 14: loss = 1.3150198252735024
2023-04-09 03:15:14,880 - INFO - training.closure0 - iteration 15: loss = 3833.220192789379
2023-04-09 03:15:14,896 - INFO - training.closure0 - iteration 16: loss = 31.330493268393255
2023-04-09 03:15:14,915 - INFO - training.closure0 - iteration 17: loss = 1.4660138979474555
2023-04-09 03:15:14,931 - INFO - training.closure0 - iteration 18: loss = 1.0426742748682267
2023-04-09 03:15:14,950 - INFO - training.closure0 - iteration 19: loss = 191317.58518294862
2023-04-09 03:15:14,968 - INFO - training.closure0 - iteration 20: loss = 504.0520366376942
2023-04-09 03:15:14,986 - INFO - training.closure0 - iteration 21: loss = 2.916125873720909
2023-04-09 03:15:15,004 - INFO - training.closure0 - iteration 22: loss = 0.4257588323988637
2023-04-09 03:15:15,024 - INFO - training.closure0 - iteration 23: loss = 27050173.593733117
2023-04-09 03:15:15,043 - INFO - training.closure0 - iteration 24: loss = 21716.229203151506
2023-04-09 03:15:15,062 - INFO - training.closure0 - iteration 25: loss = 54.23271736947885
2023-04-09 03:15:15,081 - INFO - training.closure0 - iteration 26: loss = 0.5276613617309903
2023-04-09 03:15:15,098 - INFO - training.closure0 - iteration 27: loss = 0.10995613491370476
2023-04-09 03:15:15,115 - INFO - training.closure0 - iteration 28: loss = 12.968076305954238
2023-04-09 03:15:15,131 - INFO - training.closure0 - iteration 29: loss = 0.44830039454349
2023-04-09 03:15:15,146 - INFO - training.closure0 - iteration 30: loss = 0.09985471114921543
2023-04-09 03:15:15,162 - INFO - training.closure0 - iteration 31: loss = 0.09526476446519405
2023-04-09 03:15:15,178 - INFO - training.closure0 - iteration 32: loss = 0.08470198534063103
2023-04-09 03:15:15,194 - INFO - training.closure0 - iteration 33: loss = 0.0488744156356008
2023-04-09 03:15:15,210 - INFO - training.closure0 - iteration 34: loss = 0.04554687806374933
2023-04-09 03:15:15,226 - INFO - training.closure0 - iteration 35: loss = 0.7085355215618055
2023-04-09 03:15:15,243 - INFO - training.closure0 - iteration 36: loss = 0.03739791964453166
2023-04-09 03:15:15,259 - INFO - training.closure0 - iteration 37: loss = 0.031989312622663496
2023-04-09 03:15:15,276 - INFO - training.closure0 - iteration 38: loss = 2.2740891577773663
2023-04-09 03:15:15,292 - INFO - training.closure0 - iteration 39: loss = 0.04266158293043364
2023-04-09 03:15:15,308 - INFO - training.closure0 - iteration 40: loss = 0.005945752807427644
2023-04-09 03:15:15,324 - INFO - training.closure0 - iteration 41: loss = 0.0006155144312370679
2023-04-09 03:15:15,341 - INFO - training.closure0 - iteration 42: loss = -0.005655116305980576
2023-04-09 03:15:15,357 - INFO - training.closure0 - iteration 43: loss = -0.017591254582642074
2023-04-09 03:15:15,373 - INFO - training.closure0 - iteration 44: loss = -0.04421687670390539
2023-04-09 03:15:15,389 - INFO - training.closure0 - iteration 45: loss = 0.16979709983834088
2023-04-09 03:15:15,405 - INFO - training.closure0 - iteration 46: loss = -0.0613678984498145
2023-04-09 03:15:15,422 - INFO - training.closure0 - iteration 47: loss = -0.09137702284813504
2023-04-09 03:15:15,438 - INFO - training.closure0 - iteration 48: loss = -0.11243047404909921
2023-04-09 03:15:15,455 - INFO - training.closure0 - iteration 49: loss = -0.20085309318904304
2023-04-09 03:15:15,473 - INFO - training.closure0 - iteration 50: loss = -0.2319784384085081
2023-04-09 03:15:15,489 - INFO - training.closure0 - iteration 51: loss = 705.1496005325953
2023-04-09 03:15:15,506 - INFO - training.closure0 - iteration 52: loss = 1.0242124369373973
2023-04-09 03:15:15,521 - INFO - training.closure0 - iteration 53: loss = -0.24701243072554818
2023-04-09 03:15:15,538 - INFO - training.closure0 - iteration 54: loss = -0.28286584732359377
2023-04-09 03:15:15,554 - INFO - training.closure0 - iteration 55: loss = -0.312112764228453
2023-04-09 03:15:15,571 - INFO - training.closure0 - iteration 56: loss = -0.34323954001470036
2023-04-09 03:15:15,587 - INFO - training.closure0 - iteration 57: loss = -0.30684810930078893
2023-04-09 03:15:15,602 - INFO - training.closure0 - iteration 58: loss = -0.357836680263666
2023-04-09 03:15:15,619 - INFO - training.closure0 - iteration 59: loss = -0.3750504981231432
2023-04-09 03:15:15,635 - INFO - training.closure0 - iteration 60: loss = -0.38895000307089234
2023-04-09 03:15:15,653 - INFO - training.closure0 - iteration 61: loss = -0.13106255287029303
2023-04-09 03:15:15,669 - INFO - training.closure0 - iteration 62: loss = -0.4006569664224583
2023-04-09 03:15:15,685 - INFO - training.closure0 - iteration 63: loss = -0.40485528879123267
2023-04-09 03:15:15,704 - INFO - training.closure0 - iteration 64: loss = -0.413047781062475
2023-04-09 03:15:15,721 - INFO - training.closure0 - iteration 65: loss = -0.4107032322456297
2023-04-09 03:15:15,737 - INFO - training.closure0 - iteration 66: loss = -0.4171063238623577
2023-04-09 03:15:15,753 - INFO - training.closure0 - iteration 67: loss = -0.4237639898111947
2023-04-09 03:15:15,769 - INFO - training.closure0 - iteration 68: loss = -0.4285952642072455
2023-04-09 03:15:15,786 - INFO - training.closure0 - iteration 69: loss = -0.43606847611194416
2023-04-09 03:15:15,802 - INFO - training.closure0 - iteration 70: loss = 1.0129385859380335
2023-04-09 03:15:15,818 - INFO - training.closure0 - iteration 71: loss = -0.43624053471777535
2023-04-09 03:15:15,834 - INFO - training.closure0 - iteration 72: loss = -0.4435202082240673
2023-04-09 03:15:15,851 - INFO - training.closure0 - iteration 73: loss = -0.44471821607141054
2023-04-09 03:15:15,867 - INFO - training.closure0 - iteration 74: loss = -0.4454967135125605
2023-04-09 03:15:15,884 - INFO - training.closure0 - iteration 75: loss = -0.4540338665694865
2023-04-09 03:15:15,902 - INFO - training.closure0 - iteration 76: loss = -0.47137900610001043
2023-04-09 03:15:15,919 - INFO - training.closure0 - iteration 77: loss = -0.5242461658184221
2023-04-09 03:15:15,935 - INFO - training.closure0 - iteration 78: loss = -0.7290822058993616
2023-04-09 03:15:15,952 - INFO - training.closure0 - iteration 79: loss = 199590599.32361063
2023-04-09 03:15:15,967 - INFO - training.closure0 - iteration 80: loss = 520652.78179013653
2023-04-09 03:15:15,982 - INFO - training.closure0 - iteration 81: loss = 2032.0036312858624
2023-04-09 03:15:15,998 - INFO - training.closure0 - iteration 82: loss = -1.9721464199454188
2023-04-09 03:15:16,014 - INFO - training.closure0 - iteration 83: loss = 1.0091427777490996
2023-04-09 03:15:16,030 - INFO - training.closure0 - iteration 84: loss = -2.1049861682490514
2023-04-09 03:15:16,047 - INFO - training.closure0 - iteration 85: loss = -2.139186883915972
2023-04-09 03:15:16,063 - INFO - training.closure0 - iteration 86: loss = 5620.410145909469
2023-04-09 03:15:16,081 - INFO - training.closure0 - iteration 87: loss = 185.06786056108865
2023-04-09 03:15:16,097 - INFO - training.closure0 - iteration 88: loss = 2.901639864089648
2023-04-09 03:15:16,114 - INFO - training.closure0 - iteration 89: loss = -2.13986460163177
2023-04-09 03:15:16,131 - INFO - training.closure0 - iteration 90: loss = -2.7241134311134187
2023-04-09 03:15:16,148 - INFO - training.closure0 - iteration 91: loss = 185289449370.3223
2023-04-09 03:15:16,164 - INFO - training.closure0 - iteration 92: loss = 1429783838.2875779
2023-04-09 03:15:16,180 - INFO - training.closure0 - iteration 93: loss = 19349021.008180797
2023-04-09 03:15:16,195 - INFO - training.closure0 - iteration 94: loss = 131647.01790007064
2023-04-09 03:15:16,211 - INFO - training.closure0 - iteration 95: loss = 311.5085318558546
2023-04-09 03:15:16,239 - INFO - training.closure0 - iteration 96: loss = 3.296479462841376
2023-04-09 03:15:16,259 - INFO - training.closure0 - iteration 97: loss = -2.774439846999415
2023-04-09 03:15:16,279 - INFO - training.closure0 - iteration 98: loss = -2.8620186478509977
2023-04-09 03:15:16,299 - INFO - training.closure0 - iteration 99: loss = 32.67806539269725
2023-04-09 03:15:16,316 - INFO - training.closure0 - iteration 100: loss = -2.2500787732303222
2023-04-09 03:15:16,332 - INFO - training.closure0 - iteration 101: loss = -3.0214956485354647
2023-04-09 03:15:16,348 - INFO - training.closure0 - iteration 102: loss = -3.01223008658587
2023-04-09 03:15:16,363 - INFO - training.closure0 - iteration 103: loss = -3.0264777463227146
2023-04-09 03:15:16,379 - INFO - training.closure0 - iteration 104: loss = -3.0349819849055883
2023-04-09 03:15:16,396 - INFO - training.closure0 - iteration 105: loss = -3.046545443887421
2023-04-09 03:15:16,414 - INFO - training.closure0 - iteration 106: loss = -3.1119360069292994
2023-04-09 03:15:16,432 - INFO - training.closure0 - iteration 107: loss = 80.37365205374199
2023-04-09 03:15:16,448 - INFO - training.closure0 - iteration 108: loss = -1.9691006144648147
2023-04-09 03:15:16,463 - INFO - training.closure0 - iteration 109: loss = -3.18546801966997
2023-04-09 03:15:16,479 - INFO - training.closure0 - iteration 110: loss = -3.4307469358368117
2023-04-09 03:15:16,499 - INFO - training.closure0 - iteration 111: loss = 3.28735042035852
2023-04-09 03:15:16,518 - INFO - training.closure0 - iteration 112: loss = -3.3795874651572406
2023-04-09 03:15:16,536 - INFO - training.closure0 - iteration 113: loss = -3.4336415534362503
2023-04-09 03:15:16,552 - INFO - training.closure0 - iteration 114: loss = -3.368907869144243
2023-04-09 03:15:16,568 - INFO - training.closure0 - iteration 115: loss = -3.4465904519295973
2023-04-09 03:15:16,584 - INFO - training.closure0 - iteration 116: loss = -3.4457644139343806
2023-04-09 03:15:16,599 - INFO - training.closure0 - iteration 117: loss = -3.44827497721998
2023-04-09 03:15:16,616 - INFO - training.closure0 - iteration 118: loss = -3.4489629791246186
2023-04-09 03:15:16,634 - INFO - training.closure0 - iteration 119: loss = -3.454837773596687
2023-04-09 03:15:16,650 - INFO - training.closure0 - iteration 120: loss = -3.528354748443024
2023-04-09 03:15:16,669 - INFO - training.closure0 - iteration 121: loss = -3.6162266753528627
2023-04-09 03:15:16,686 - INFO - training.closure0 - iteration 122: loss = 58.264295881361164
2023-04-09 03:15:16,703 - INFO - training.closure0 - iteration 123: loss = -2.3331922962646745
2023-04-09 03:15:16,721 - INFO - training.closure0 - iteration 124: loss = -3.748273783682346
2023-04-09 03:15:16,741 - INFO - training.closure0 - iteration 125: loss = -3.364356059168961
2023-04-09 03:15:16,758 - INFO - training.closure0 - iteration 126: loss = -3.971033117276094
2023-04-09 03:15:16,774 - INFO - training.closure0 - iteration 127: loss = -1.5196859868973154
2023-04-09 03:15:16,789 - INFO - training.closure0 - iteration 128: loss = -3.9766507958213246
2023-04-09 03:15:16,805 - INFO - training.closure0 - iteration 129: loss = -4.0075228173597
2023-04-09 03:15:16,820 - INFO - training.closure0 - iteration 130: loss = -4.300383990553378
2023-04-09 03:15:16,837 - INFO - training.closure0 - iteration 131: loss = -4.245623643416074
2023-04-09 03:15:16,855 - INFO - training.closure0 - iteration 132: loss = -4.320989319446035
2023-04-09 03:15:16,873 - INFO - training.closure0 - iteration 133: loss = -4.332388469002501
2023-04-09 03:15:16,889 - INFO - training.closure0 - iteration 134: loss = -4.402298002297342
2023-04-09 03:15:16,908 - INFO - training.closure0 - iteration 135: loss = -4.48094875502196
2023-04-09 03:15:16,924 - INFO - training.closure0 - iteration 136: loss = -4.579644851065267
2023-04-09 03:15:16,941 - INFO - training.closure0 - iteration 137: loss = -4.53489852544916
2023-04-09 03:15:16,958 - INFO - training.closure0 - iteration 138: loss = -4.591437089641758
2023-04-09 03:15:16,975 - INFO - training.closure0 - iteration 139: loss = -4.584137681703309
2023-04-09 03:15:16,991 - INFO - training.closure0 - iteration 140: loss = -4.604169677586883
2023-04-09 03:15:17,007 - INFO - training.closure0 - iteration 141: loss = -4.613749393047759
2023-04-09 03:15:17,023 - INFO - training.closure0 - iteration 142: loss = -4.700534634599593
2023-04-09 03:15:17,039 - INFO - training.closure0 - iteration 143: loss = -5.450071753794969
2023-04-09 03:15:17,055 - INFO - training.closure0 - iteration 144: loss = -5.463545436469827
2023-04-09 03:15:17,071 - INFO - training.closure0 - iteration 145: loss = 1263111.899305163
2023-04-09 03:15:17,087 - INFO - training.closure0 - iteration 146: loss = -5.463550963447432
2023-04-09 03:15:17,104 - INFO - training.closure0 - iteration 147: loss = 0.8598292216608941
2023-04-09 03:15:17,120 - INFO - training.closure0 - iteration 148: loss = -5.45452187949951
2023-04-09 03:15:17,136 - INFO - training.closure0 - iteration 149: loss = -5.519827753374907
2023-04-09 03:15:17,153 - INFO - training.closure0 - iteration 150: loss = -5.603991244668517
2023-04-09 03:15:17,170 - INFO - training.closure0 - iteration 151: loss = -5.932218311252516
2023-04-09 03:15:17,187 - INFO - training.closure0 - iteration 152: loss = -6.074285223773571
2023-04-09 03:15:17,204 - INFO - training.closure0 - iteration 153: loss = -4.706584166518955
2023-04-09 03:15:17,219 - INFO - training.closure0 - iteration 154: loss = -6.188145997380825
2023-04-09 03:15:17,236 - INFO - training.closure0 - iteration 155: loss = -5.807883017136931
2023-04-09 03:15:17,251 - INFO - training.closure0 - iteration 156: loss = -6.216520201103423
2023-04-09 03:15:17,267 - INFO - training.closure0 - iteration 157: loss = -6.091683198218801
2023-04-09 03:15:17,282 - INFO - training.closure0 - iteration 158: loss = -6.266968017942954
2023-04-09 03:15:17,299 - INFO - training.closure0 - iteration 159: loss = -6.2905671872677456
2023-04-09 03:15:17,316 - INFO - training.closure0 - iteration 160: loss = -6.382579338254271
2023-04-09 03:15:17,333 - INFO - training.closure0 - iteration 161: loss = -6.4433174061451535
2023-04-09 03:15:17,351 - INFO - training.closure0 - iteration 162: loss = -6.454285086687718
2023-04-09 03:15:17,367 - INFO - training.closure0 - iteration 163: loss = -6.4568284505440925
2023-04-09 03:15:17,386 - INFO - training.closure0 - iteration 164: loss = -6.456882490206767
2023-04-09 03:15:17,402 - INFO - training.closure0 - iteration 165: loss = -6.456909771435135
2023-04-09 03:15:17,419 - INFO - training.closure0 - iteration 166: loss = -6.456911661689381
2023-04-09 03:15:17,435 - INFO - training.closure0 - iteration 167: loss = -6.456911675864372
2023-04-09 03:15:17,452 - INFO - training.closure0 - iteration 168: loss = -6.456911675911261
2023-04-09 03:15:17,467 - INFO - training.closure0 - iteration 169: loss = -6.456911675911369
2023-04-09 03:15:17,483 - INFO - training.closure0 - iteration 170: loss = -6.456911675911376
2023-04-09 03:15:17,491 - INFO - training.pre_train_full - a0 mean: [2.99944253 3.00062146]
2023-04-09 03:15:17,492 - INFO - training.pre_train_full - a0 var: [1.00409418e-04 8.44174449e-05]
2023-04-09 03:15:17,494 - INFO - training.pre_train_full - a0 covar: [[0.00010040941754015528, -5.508686337777873e-06], [-5.508686337777873e-06, 8.441744487900625e-05]]
2023-04-09 03:15:39,682 - INFO - training.closure - iteration 0: loss = 253283.44839863433
2023-04-09 03:16:05,794 - INFO - training.closure - iteration 1: loss = 3766688.729040721
2023-04-09 03:16:30,724 - INFO - training.closure - iteration 2: loss = 165269.63266598817
2023-04-09 03:16:55,489 - INFO - training.closure - iteration 3: loss = 129932.15515450023
2023-04-09 03:17:19,981 - INFO - training.closure - iteration 4: loss = 56934.76305923972
2023-04-09 03:17:44,165 - INFO - training.closure - iteration 5: loss = 36829.00663053065
2023-04-09 03:18:08,490 - INFO - training.closure - iteration 6: loss = 23791.507454479623
2023-04-09 03:18:33,014 - INFO - training.closure - iteration 7: loss = 14426.779687603743
2023-04-09 03:18:57,254 - INFO - training.closure - iteration 8: loss = 5235.504053579255
2023-04-09 03:19:21,469 - INFO - training.closure - iteration 9: loss = 2423.035876836492
2023-04-09 03:19:45,733 - INFO - training.closure - iteration 10: loss = 1755.636591873952
2023-04-09 03:20:10,137 - INFO - training.closure - iteration 11: loss = 669.7598765636841
2023-04-09 03:20:34,762 - INFO - training.closure - iteration 12: loss = 485.3818596767704
2023-04-09 03:20:59,395 - INFO - training.closure - iteration 13: loss = 336.79497660751883
2023-04-09 03:21:23,835 - INFO - training.closure - iteration 14: loss = 214.30181733915137
2023-04-09 03:21:48,699 - INFO - training.closure - iteration 15: loss = 102.78988337404982
2023-04-09 03:22:13,223 - INFO - training.closure - iteration 16: loss = 51.42418676972745
2023-04-09 03:22:37,585 - INFO - training.closure - iteration 17: loss = 20.62336784179294
2023-04-09 03:23:01,930 - INFO - training.closure - iteration 18: loss = 12.316544559803832
2023-04-09 03:23:26,207 - INFO - training.closure - iteration 19: loss = 9.372389771626777
2023-04-09 03:23:50,676 - INFO - training.closure - iteration 20: loss = 7.984319597626314
2023-04-09 03:24:15,263 - INFO - training.closure - iteration 21: loss = 7.742857860192094
2023-04-09 03:24:39,676 - INFO - training.closure - iteration 22: loss = 6.806815229306606
2023-04-09 03:25:03,942 - INFO - training.closure - iteration 23: loss = 6.659466361695717
2023-04-09 03:25:28,588 - INFO - training.closure - iteration 24: loss = 6.490972717240367
2023-04-09 03:25:52,909 - INFO - training.closure - iteration 25: loss = 6.05318347412172
2023-04-09 03:26:17,493 - INFO - training.closure - iteration 26: loss = 5.236367144367627
2023-04-09 03:26:41,956 - INFO - training.closure - iteration 27: loss = 4.389371767830829
2023-04-09 03:27:06,809 - INFO - training.closure - iteration 28: loss = 3.8653020619934217
2023-04-09 03:27:31,180 - INFO - training.closure - iteration 29: loss = 3.711104469212371
2023-04-09 03:27:55,772 - INFO - training.closure - iteration 30: loss = 3.1844723201967877
2023-04-09 03:28:20,383 - INFO - training.closure - iteration 31: loss = 2.6016544284975742
2023-04-09 03:28:44,888 - INFO - training.closure - iteration 32: loss = 1.6417927156593173
2023-04-09 03:29:09,334 - INFO - training.closure - iteration 33: loss = 0.8947073554973004
2023-04-09 03:29:33,619 - INFO - training.closure - iteration 34: loss = 0.2810620201860168
2023-04-09 03:29:58,036 - INFO - training.closure - iteration 35: loss = 0.051747637940054325
2023-04-09 03:30:22,699 - INFO - training.closure - iteration 36: loss = -0.13103859559557218
2023-04-09 03:30:47,259 - INFO - training.closure - iteration 37: loss = -0.29955676409922916
2023-04-09 03:31:11,800 - INFO - training.closure - iteration 38: loss = -0.5380572180103291
2023-04-09 03:31:36,796 - INFO - training.closure - iteration 39: loss = -0.8190878116779476
2023-04-09 03:32:01,438 - INFO - training.closure - iteration 40: loss = -0.9764386775651639
2023-04-09 03:32:25,849 - INFO - training.closure - iteration 41: loss = -1.2782193898786582
2023-04-09 03:32:50,282 - INFO - training.closure - iteration 42: loss = -1.479100926215981
2023-04-09 03:33:14,612 - INFO - training.closure - iteration 43: loss = -1.6869841446891973
2023-04-09 03:33:38,972 - INFO - training.closure - iteration 44: loss = -1.9127827164051805
2023-04-09 03:34:03,176 - INFO - training.closure - iteration 45: loss = -2.0438268705559564
2023-04-09 03:34:27,678 - INFO - training.closure - iteration 46: loss = -2.4283847786056025
2023-04-09 03:34:52,273 - INFO - training.closure - iteration 47: loss = -2.591599498147256
2023-04-09 03:35:16,679 - INFO - training.closure - iteration 48: loss = -2.5567100021840163
2023-04-09 03:35:41,143 - INFO - training.closure - iteration 49: loss = -2.646586839661851
2023-04-09 03:36:05,546 - INFO - training.closure - iteration 50: loss = -2.6564570392849793
2023-04-09 03:36:29,883 - INFO - training.closure - iteration 51: loss = -2.732207444910956
2023-04-09 03:36:54,418 - INFO - training.closure - iteration 52: loss = -2.8200205994974894
2023-04-09 03:37:18,724 - INFO - training.closure - iteration 53: loss = -2.9519000265115434
2023-04-09 03:37:43,265 - INFO - training.closure - iteration 54: loss = -3.078662225139589
2023-04-09 03:38:08,241 - INFO - training.closure - iteration 55: loss = -3.2526480830386233
2023-04-09 03:38:32,747 - INFO - training.closure - iteration 56: loss = -3.342482154645178
2023-04-09 03:38:56,997 - INFO - training.closure - iteration 57: loss = -3.3905903222870117
2023-04-09 03:39:21,382 - INFO - training.closure - iteration 58: loss = -3.485381630362051
2023-04-09 03:39:45,858 - INFO - training.closure - iteration 59: loss = -3.531757177595337
2023-04-09 03:40:10,335 - INFO - training.closure - iteration 60: loss = -3.6266001716176386
2023-04-09 03:40:34,942 - INFO - training.closure - iteration 61: loss = -3.693311458635761
2023-04-09 03:40:59,285 - INFO - training.closure - iteration 62: loss = -3.77227807088827
2023-04-09 03:41:23,704 - INFO - training.closure - iteration 63: loss = -3.892431509862959
2023-04-09 03:41:48,358 - INFO - training.closure - iteration 64: loss = -3.9846511956857245
2023-04-09 03:42:12,876 - INFO - training.closure - iteration 65: loss = -4.088747716920835
2023-04-09 03:42:37,335 - INFO - training.closure - iteration 66: loss = -4.169771062465671
2023-04-09 03:43:01,940 - INFO - training.closure - iteration 67: loss = -4.160999611282319
2023-04-09 03:43:26,491 - INFO - training.closure - iteration 68: loss = -4.210147960130504
2023-04-09 03:43:50,807 - INFO - training.closure - iteration 69: loss = -4.231590629446657
2023-04-09 03:44:14,983 - INFO - training.closure - iteration 70: loss = -4.254207726984711
2023-04-09 03:44:39,532 - INFO - training.closure - iteration 71: loss = -4.272939028529618
2023-04-09 03:45:04,080 - INFO - training.closure - iteration 72: loss = -4.290711928264239
2023-04-09 03:45:28,387 - INFO - training.closure - iteration 73: loss = -4.323949318881297
2023-04-09 03:45:52,836 - INFO - training.closure - iteration 74: loss = -4.3839435440898065
2023-04-09 03:46:17,336 - INFO - training.closure - iteration 75: loss = -4.426060855917165
2023-04-09 03:46:41,989 - INFO - training.closure - iteration 76: loss = -4.454707971584885
2023-04-09 03:47:06,571 - INFO - training.closure - iteration 77: loss = -4.466414028147646
2023-04-09 03:47:30,867 - INFO - training.closure - iteration 78: loss = -4.50450572590434
2023-04-09 03:47:55,572 - INFO - training.closure - iteration 79: loss = -4.560863911441794
2023-04-09 03:48:19,983 - INFO - training.closure - iteration 80: loss = -4.60481987722265
2023-04-09 03:48:44,493 - INFO - training.closure - iteration 81: loss = -4.657630666735372
2023-04-09 03:49:09,016 - INFO - training.closure - iteration 82: loss = -4.707171052067887
2023-04-09 03:49:33,385 - INFO - training.closure - iteration 83: loss = -4.757564229536152
2023-04-09 03:49:57,767 - INFO - training.closure - iteration 84: loss = -4.881489176630324
2023-04-09 03:50:22,222 - INFO - training.closure - iteration 85: loss = 2.0069721830331746
2023-04-09 03:50:46,935 - INFO - training.closure - iteration 86: loss = -4.866600634630458
2023-04-09 03:51:11,439 - INFO - training.closure - iteration 87: loss = -5.0821072882204295
2023-04-09 03:51:35,818 - INFO - training.closure - iteration 88: loss = 36.088690191238214
2023-04-09 03:52:00,295 - INFO - training.closure - iteration 89: loss = -3.795062472669833
2023-04-09 03:52:24,730 - INFO - training.closure - iteration 90: loss = -5.186805084705922
2023-04-09 03:52:49,104 - INFO - training.closure - iteration 91: loss = -3.8459517115528654
2023-04-09 03:53:13,564 - INFO - training.closure - iteration 92: loss = -5.5128713493497905
2023-04-09 03:53:37,947 - INFO - training.closure - iteration 93: loss = 209.2796616269446
2023-04-09 03:54:02,364 - INFO - training.closure - iteration 94: loss = -5.680215997208452
2023-04-09 03:54:26,951 - INFO - training.closure - iteration 95: loss = -5.781374667748729
2023-04-09 03:54:51,462 - INFO - training.closure - iteration 96: loss = -5.957098864525499
2023-04-09 03:55:16,073 - INFO - training.closure - iteration 97: loss = -0.5397842592702773
2023-04-09 03:55:40,776 - INFO - training.closure - iteration 98: loss = -6.139543656807692
2023-04-09 03:56:05,231 - INFO - training.closure - iteration 99: loss = -6.186745027142187
2023-04-09 03:56:29,692 - INFO - training.closure - iteration 100: loss = -6.312646266535185
2023-04-09 03:56:54,166 - INFO - training.closure - iteration 101: loss = -6.394662716957354
2023-04-09 03:57:18,662 - INFO - training.closure - iteration 102: loss = -6.475726156636711
2023-04-09 03:57:43,326 - INFO - training.closure - iteration 103: loss = -6.527558435276466
2023-04-09 03:58:07,564 - INFO - training.closure - iteration 104: loss = -6.600985632411679
2023-04-09 03:58:31,945 - INFO - training.closure - iteration 105: loss = -6.668348425975638
2023-04-09 03:58:56,305 - INFO - training.closure - iteration 106: loss = -6.6742432137759184
2023-04-09 03:59:20,924 - INFO - training.closure - iteration 107: loss = -6.704783534417813
2023-04-09 03:59:45,434 - INFO - training.closure - iteration 108: loss = -6.738027346237159
2023-04-09 04:00:10,033 - INFO - training.closure - iteration 109: loss = -6.780960053845506
2023-04-09 04:00:34,573 - INFO - training.closure - iteration 110: loss = -6.730732445883815
2023-04-09 04:00:58,966 - INFO - training.closure - iteration 111: loss = -6.8014919247155925
2023-04-09 04:01:23,559 - INFO - training.closure - iteration 112: loss = -6.738703039741007
2023-04-09 04:01:48,231 - INFO - training.closure - iteration 113: loss = -6.8196983096172605
2023-04-09 04:02:12,664 - INFO - training.closure - iteration 114: loss = -6.831179313070619
2023-04-09 04:02:37,386 - INFO - training.closure - iteration 115: loss = -6.848493615437324
2023-04-09 04:03:02,318 - INFO - training.closure - iteration 116: loss = -6.864272584782379
2023-04-09 04:03:26,661 - INFO - training.closure - iteration 117: loss = -6.887022486795478
2023-04-09 04:03:51,027 - INFO - training.closure - iteration 118: loss = -6.917979119712946
2023-04-09 04:04:15,524 - INFO - training.closure - iteration 119: loss = -6.949815166290508
2023-04-09 04:04:40,368 - INFO - training.closure - iteration 120: loss = -6.96649008832772
2023-04-09 04:05:04,839 - INFO - training.closure - iteration 121: loss = -6.974133017799652
2023-04-09 04:05:29,245 - INFO - training.closure - iteration 122: loss = -6.9764597103649475
2023-04-09 04:05:54,027 - INFO - training.closure - iteration 123: loss = -6.983276392099575
2023-04-09 04:06:18,408 - INFO - training.closure - iteration 124: loss = -6.993328024816476
2023-04-09 04:06:42,681 - INFO - training.closure - iteration 125: loss = -7.00952405515757
2023-04-09 04:07:07,259 - INFO - training.closure - iteration 126: loss = -7.037636266457212
2023-04-09 04:07:32,348 - INFO - training.closure - iteration 127: loss = -7.089303160092425
2023-04-09 04:07:56,878 - INFO - training.closure - iteration 128: loss = -7.127115571385721
2023-04-09 04:08:21,271 - INFO - training.closure - iteration 129: loss = -7.122273050596587
2023-04-09 04:08:45,664 - INFO - training.closure - iteration 130: loss = -7.168169040000242
2023-04-09 04:09:09,961 - INFO - training.closure - iteration 131: loss = -7.200800673529657
2023-04-09 04:09:34,482 - INFO - training.closure - iteration 132: loss = -7.226904526091447
2023-04-09 04:09:59,053 - INFO - training.closure - iteration 133: loss = -7.244421546957707
2023-04-09 04:10:23,421 - INFO - training.closure - iteration 134: loss = -7.2550675973018475
2023-04-09 04:10:48,113 - INFO - training.closure - iteration 135: loss = -7.267649814659952
2023-04-09 04:11:12,518 - INFO - training.closure - iteration 136: loss = -7.2703714450767345
2023-04-09 04:11:36,942 - INFO - training.closure - iteration 137: loss = -7.277743775864643
2023-04-09 04:12:01,314 - INFO - training.closure - iteration 138: loss = -7.285331865574445
2023-04-09 04:12:25,683 - INFO - training.closure - iteration 139: loss = -7.291790330903163
2023-04-09 04:12:50,059 - INFO - training.closure - iteration 140: loss = -7.300538608962547
2023-04-09 04:13:14,351 - INFO - training.closure - iteration 141: loss = -7.326460027938988
2023-04-09 04:13:38,859 - INFO - training.closure - iteration 142: loss = -7.371306722565125
2023-04-09 04:14:03,606 - INFO - training.closure - iteration 143: loss = -7.417110238250431
2023-04-09 04:14:28,211 - INFO - training.closure - iteration 144: loss = -7.479070775685963
2023-04-09 04:14:52,661 - INFO - training.closure - iteration 145: loss = -7.523863739603113
2023-04-09 04:15:17,171 - INFO - training.closure - iteration 146: loss = -7.564007885650786
2023-04-09 04:15:41,625 - INFO - training.closure - iteration 147: loss = -7.570472462637431
2023-04-09 04:16:06,179 - INFO - training.closure - iteration 148: loss = -7.5887438160142
2023-04-09 04:16:30,610 - INFO - training.closure - iteration 149: loss = -7.601672610558979
2023-04-09 04:16:55,499 - INFO - training.closure - iteration 150: loss = -7.620190926070769
2023-04-09 04:17:20,071 - INFO - training.closure - iteration 151: loss = -7.63113366186031
2023-04-09 04:17:44,592 - INFO - training.closure - iteration 152: loss = -7.646771697189688
2023-04-09 04:18:08,995 - INFO - training.closure - iteration 153: loss = -7.662655710669017
2023-04-09 04:18:33,506 - INFO - training.closure - iteration 154: loss = -7.662974777444046
2023-04-09 04:18:58,064 - INFO - training.closure - iteration 155: loss = -7.6731520063807155
2023-04-09 04:19:22,522 - INFO - training.closure - iteration 156: loss = -7.6844303348855485
2023-04-09 04:19:47,024 - INFO - training.closure - iteration 157: loss = -7.695506775293253
2023-04-09 04:20:11,576 - INFO - training.closure - iteration 158: loss = -7.713270585814375
2023-04-09 04:20:36,333 - INFO - training.closure - iteration 159: loss = -7.722249264316773
2023-04-09 04:21:01,037 - INFO - training.closure - iteration 160: loss = -7.7277958481164895
2023-04-09 04:21:25,570 - INFO - training.closure - iteration 161: loss = -7.733198379297494
2023-04-09 04:21:50,164 - INFO - training.closure - iteration 162: loss = -7.743700029869353
2023-04-09 04:22:14,678 - INFO - training.closure - iteration 163: loss = -7.76155708334523
2023-04-09 04:22:39,201 - INFO - training.closure - iteration 164: loss = -7.778721291109668
2023-04-09 04:23:03,490 - INFO - training.closure - iteration 165: loss = -7.793056826580587
2023-04-09 04:23:28,014 - INFO - training.closure - iteration 166: loss = -7.818027843921646
2023-04-09 04:23:52,365 - INFO - training.closure - iteration 167: loss = -7.8477271157939725
2023-04-09 04:24:16,806 - INFO - training.closure - iteration 168: loss = -7.895893619137263
2023-04-09 04:24:41,026 - INFO - training.closure - iteration 169: loss = -7.612435686215665
2023-04-09 04:25:05,296 - INFO - training.closure - iteration 170: loss = -7.92285290218918
2023-04-09 04:25:29,929 - INFO - training.closure - iteration 171: loss = -6.892614612129172
2023-04-09 04:25:54,244 - INFO - training.closure - iteration 172: loss = -7.961914117711488
2023-04-09 04:26:18,610 - INFO - training.closure - iteration 173: loss = -6.584992154907761
2023-04-09 04:26:42,909 - INFO - training.closure - iteration 174: loss = -7.970288043581034
2023-04-09 04:27:07,325 - INFO - training.closure - iteration 175: loss = 705.0703550738373
2023-04-09 04:27:31,675 - INFO - training.closure - iteration 176: loss = 4.6805336020435035
2023-04-09 04:27:55,936 - INFO - training.closure - iteration 177: loss = -7.754892079111967
2023-04-09 04:28:20,388 - INFO - training.closure - iteration 178: loss = -8.002660426928957
2023-04-09 04:28:44,730 - INFO - training.closure - iteration 179: loss = -8.04035479222826
2023-04-09 04:29:09,160 - INFO - training.closure - iteration 180: loss = -8.06994424863912
2023-04-09 04:29:33,301 - INFO - training.closure - iteration 181: loss = -8.084804152907168
2023-04-09 04:29:57,794 - INFO - training.closure - iteration 182: loss = -8.100800253198896
2023-04-09 04:30:22,157 - INFO - training.closure - iteration 183: loss = -8.117668202384893
2023-04-09 04:30:46,508 - INFO - training.closure - iteration 184: loss = -8.127899411567718
2023-04-09 04:31:10,862 - INFO - training.closure - iteration 185: loss = -8.139575405544267
2023-04-09 04:31:35,383 - INFO - training.closure - iteration 186: loss = -8.15245393563809
2023-04-09 04:32:00,071 - INFO - training.closure - iteration 187: loss = -8.152544961240046
2023-04-09 04:32:24,555 - INFO - training.closure - iteration 188: loss = -8.162582295014966
2023-04-09 04:32:48,880 - INFO - training.closure - iteration 189: loss = -8.176132962754288
2023-04-09 04:33:13,240 - INFO - training.closure - iteration 190: loss = -8.194001070180697
2023-04-09 04:33:37,781 - INFO - training.closure - iteration 191: loss = -8.20218581263499
2023-04-09 04:34:02,085 - INFO - training.closure - iteration 192: loss = -8.208780027323803
2023-04-09 04:34:26,427 - INFO - training.closure - iteration 193: loss = -8.218024599893763
2023-04-09 04:34:50,877 - INFO - training.closure - iteration 194: loss = -8.238803070341838
2023-04-09 04:35:15,267 - INFO - training.closure - iteration 195: loss = -8.268617794463724
2023-04-09 04:35:39,971 - INFO - training.closure - iteration 196: loss = -8.316046516059437
2023-04-09 04:36:04,354 - INFO - training.closure - iteration 197: loss = -8.375363489778724
2023-04-09 04:36:28,586 - INFO - training.closure - iteration 198: loss = -8.433465625232628
2023-04-09 04:36:53,189 - INFO - training.closure - iteration 199: loss = -8.459650001605187
2023-04-09 04:37:17,578 - INFO - training.closure - iteration 200: loss = -8.452313557966507
2023-04-09 04:37:41,944 - INFO - training.closure - iteration 201: loss = -8.468411053088973
2023-04-09 04:38:06,536 - INFO - training.closure - iteration 202: loss = -8.473277076138256
2023-04-09 04:38:31,021 - INFO - training.closure - iteration 203: loss = -8.476347162600888
2023-04-09 04:38:55,414 - INFO - training.closure - iteration 204: loss = -8.479872550096614
2023-04-09 04:39:19,797 - INFO - training.closure - iteration 205: loss = -8.488115512563981
2023-04-09 04:39:44,407 - INFO - training.closure - iteration 206: loss = -8.511448302366912
2023-04-09 04:40:08,781 - INFO - training.closure - iteration 207: loss = -8.48535178583581
2023-04-09 04:40:33,340 - INFO - training.closure - iteration 208: loss = -8.524862117670963
2023-04-09 04:40:58,102 - INFO - training.closure - iteration 209: loss = -8.546744672240424
2023-04-09 04:41:22,397 - INFO - training.closure - iteration 210: loss = -8.557516970846265
2023-04-09 04:41:47,170 - INFO - training.closure - iteration 211: loss = -8.563760035757301
2023-04-09 04:42:11,691 - INFO - training.closure - iteration 212: loss = -8.570249244691107
2023-04-09 04:42:36,009 - INFO - training.closure - iteration 213: loss = -8.58318939147533
2023-04-09 04:43:00,450 - INFO - training.closure - iteration 214: loss = -8.586669456022854
2023-04-09 04:43:24,443 - INFO - training.closure - iteration 215: loss = -8.589840349167865
2023-04-09 04:43:48,960 - INFO - training.closure - iteration 216: loss = -8.594974167253998
2023-04-09 04:44:13,154 - INFO - training.closure - iteration 217: loss = -8.602249120812077
2023-04-09 04:44:37,575 - INFO - training.closure - iteration 218: loss = -8.611401411274585
2023-04-09 04:45:01,848 - INFO - training.closure - iteration 219: loss = -8.621396091543716
2023-04-09 04:45:26,661 - INFO - training.closure - iteration 220: loss = -8.631657100087555
2023-04-09 04:45:51,028 - INFO - training.closure - iteration 221: loss = -8.641382811897737
2023-04-09 04:46:15,493 - INFO - training.closure - iteration 222: loss = -8.66251630781163
2023-04-09 04:46:39,926 - INFO - training.closure - iteration 223: loss = -8.699427396668064
2023-04-09 04:47:04,612 - INFO - training.closure - iteration 224: loss = -8.709960461279433
2023-04-09 04:47:28,837 - INFO - training.closure - iteration 225: loss = -8.731024298488839
2023-04-09 04:47:53,239 - INFO - training.closure - iteration 226: loss = -8.744404976700835
2023-04-09 04:48:17,328 - INFO - training.closure - iteration 227: loss = -8.759574775898024
2023-04-09 04:48:41,900 - INFO - training.closure - iteration 228: loss = -8.845107815708756
2023-04-09 04:49:06,146 - INFO - training.closure - iteration 229: loss = -8.893615504618005
2023-04-09 04:49:30,633 - INFO - training.closure - iteration 230: loss = -8.904444143543213
2023-04-09 04:49:55,178 - INFO - training.closure - iteration 231: loss = -8.940400316001256
2023-04-09 04:50:19,633 - INFO - training.closure - iteration 232: loss = -8.964556798849406
2023-04-09 04:50:44,042 - INFO - training.closure - iteration 233: loss = -8.984963287881856
2023-04-09 04:51:08,602 - INFO - training.closure - iteration 234: loss = -8.933177397956403
2023-04-09 04:51:33,222 - INFO - training.closure - iteration 235: loss = -9.010493138187043
2023-04-09 04:51:57,814 - INFO - training.closure - iteration 236: loss = -9.029539842704574
2023-04-09 04:52:22,242 - INFO - training.closure - iteration 237: loss = -9.055306153836762
2023-04-09 04:52:46,330 - INFO - training.closure - iteration 238: loss = -9.081930800026313
2023-04-09 04:53:10,756 - INFO - training.closure - iteration 239: loss = -9.091503633426528
2023-04-09 04:53:35,353 - INFO - training.closure - iteration 240: loss = -8.955301313568139
2023-04-09 04:53:59,670 - INFO - training.closure - iteration 241: loss = -9.097156417173624
2023-04-09 04:54:24,021 - INFO - training.closure - iteration 242: loss = -9.106152822078009
2023-04-09 04:54:49,222 - INFO - training.closure - iteration 243: loss = -9.110944989410708
2023-04-09 04:55:13,892 - INFO - training.closure - iteration 244: loss = -9.12089787981229
2023-04-09 04:55:38,672 - INFO - training.closure - iteration 245: loss = -9.14010240415054
2023-04-09 04:56:03,130 - INFO - training.closure - iteration 246: loss = -9.161521962435792
2023-04-09 04:56:27,637 - INFO - training.closure - iteration 247: loss = -9.187007175111082
2023-04-09 04:56:52,014 - INFO - training.closure - iteration 248: loss = -9.25395995308756
2023-04-09 04:57:16,590 - INFO - training.closure - iteration 249: loss = -9.30762075319263
2023-04-09 04:57:40,962 - INFO - training.closure - iteration 250: loss = -8.654213288355157
2023-04-09 04:58:05,388 - INFO - training.closure - iteration 251: loss = -9.389349141290822
2023-04-09 04:58:30,095 - INFO - training.closure - iteration 252: loss = -9.432811795443968
2023-04-09 04:58:54,422 - INFO - training.closure - iteration 253: loss = -9.50835983376832
2023-04-09 04:59:18,791 - INFO - training.closure - iteration 254: loss = -9.53539892150335
2023-04-09 04:59:43,107 - INFO - training.closure - iteration 255: loss = -9.537638750642767
2023-04-09 05:00:07,546 - INFO - training.closure - iteration 256: loss = -9.560022225056738
2023-04-09 05:00:32,122 - INFO - training.closure - iteration 257: loss = -9.590927114512361
2023-04-09 05:00:57,563 - INFO - training.closure - iteration 258: loss = -9.613069150201317
2023-04-09 05:01:22,888 - INFO - training.closure - iteration 259: loss = -9.631078055134989
2023-04-09 05:01:47,774 - INFO - training.closure - iteration 260: loss = -9.652129221948845
2023-04-09 05:02:12,496 - INFO - training.closure - iteration 261: loss = -9.68782920490871
2023-04-09 05:02:37,003 - INFO - training.closure - iteration 262: loss = -9.775097735756674
2023-04-09 05:03:01,497 - INFO - training.closure - iteration 263: loss = -9.85713017479757
2023-04-09 05:03:26,131 - INFO - training.closure - iteration 264: loss = -9.936614255944537
2023-04-09 05:03:50,480 - INFO - training.closure - iteration 265: loss = -10.073490517410114
2023-04-09 05:04:14,932 - INFO - training.closure - iteration 266: loss = -9.8684686303788
2023-04-09 05:04:39,868 - INFO - training.closure - iteration 267: loss = -10.27978171108564
2023-04-09 05:05:04,355 - INFO - training.closure - iteration 268: loss = -10.156094179168807
2023-04-09 05:05:29,261 - INFO - training.closure - iteration 269: loss = -10.36216249520864
2023-04-09 05:05:53,813 - INFO - training.closure - iteration 270: loss = -10.437836813721395
2023-04-09 05:06:18,334 - INFO - training.closure - iteration 271: loss = -10.087527945509457
2023-04-09 05:06:42,832 - INFO - training.closure - iteration 272: loss = -10.46324827427114
2023-04-09 05:07:07,161 - INFO - training.closure - iteration 273: loss = -10.532641077499672
2023-04-09 05:07:31,422 - INFO - training.closure - iteration 274: loss = -10.579569606644514
2023-04-09 05:07:55,889 - INFO - training.closure - iteration 275: loss = -10.277705284474266
2023-04-09 05:08:20,344 - INFO - training.closure - iteration 276: loss = -10.594485080796574
2023-04-09 05:08:44,753 - INFO - training.closure - iteration 277: loss = -10.60851809404853
2023-04-09 05:09:09,132 - INFO - training.closure - iteration 278: loss = -10.634641470592994
2023-04-09 05:09:33,374 - INFO - training.closure - iteration 279: loss = -10.639348993021851
2023-04-09 05:09:57,946 - INFO - training.closure - iteration 280: loss = -10.643932452013768
2023-04-09 05:10:22,334 - INFO - training.closure - iteration 281: loss = -10.647233366810472
2023-04-09 05:10:46,687 - INFO - training.closure - iteration 282: loss = -10.647854226680767
2023-04-09 05:11:11,270 - INFO - training.closure - iteration 283: loss = -10.643030748923401
2023-04-09 05:11:35,610 - INFO - training.closure - iteration 284: loss = -10.648459401441524
2023-04-09 05:12:00,012 - INFO - training.closure - iteration 285: loss = -10.649771607721156
2023-04-09 05:12:24,520 - INFO - training.closure - iteration 286: loss = -10.653899889920737
2023-04-09 05:12:48,935 - INFO - training.closure - iteration 287: loss = -10.659098958373553
2023-04-09 05:13:13,265 - INFO - training.closure - iteration 288: loss = -10.668445007911338
2023-04-09 05:13:37,770 - INFO - training.closure - iteration 289: loss = -10.647191616047994
2023-04-09 05:14:02,063 - INFO - training.closure - iteration 290: loss = -10.673821077161026
2023-04-09 05:14:26,288 - INFO - training.closure - iteration 291: loss = -10.693932109400565
2023-04-09 05:14:50,620 - INFO - training.closure - iteration 292: loss = -10.746498339167303
2023-04-09 05:15:15,045 - INFO - training.closure - iteration 293: loss = -10.766140938341238
2023-04-09 05:15:39,583 - INFO - training.closure - iteration 294: loss = -10.777232498913893
2023-04-09 05:16:04,318 - INFO - training.closure - iteration 295: loss = -10.784158050202226
2023-04-09 05:16:29,037 - INFO - training.closure - iteration 296: loss = -10.789674582457314
2023-04-09 05:16:53,971 - INFO - training.closure - iteration 297: loss = -10.798048956188664
2023-04-09 05:17:18,509 - INFO - training.closure - iteration 298: loss = -10.80083969521457
2023-04-09 05:17:42,957 - INFO - training.closure - iteration 299: loss = -10.807949622317288
2023-04-09 05:18:07,274 - INFO - training.closure - iteration 300: loss = -10.820290045448633
2023-04-09 05:18:31,629 - INFO - training.closure - iteration 301: loss = -10.851052526535298
2023-04-09 05:18:56,173 - INFO - training.closure - iteration 302: loss = -10.894619414806048
2023-04-09 05:19:20,476 - INFO - training.closure - iteration 303: loss = -10.145085067971701
2023-04-09 05:19:44,835 - INFO - training.closure - iteration 304: loss = -10.933116857499854
2023-04-09 05:20:09,527 - INFO - training.closure - iteration 305: loss = -11.035785395288444
2023-04-09 05:20:33,965 - INFO - training.closure - iteration 306: loss = -10.8477396858652
2023-04-09 05:20:58,405 - INFO - training.closure - iteration 307: loss = -11.07365391005931
2023-04-09 05:21:22,854 - INFO - training.closure - iteration 308: loss = -10.58549670671345
2023-04-09 05:21:47,454 - INFO - training.closure - iteration 309: loss = -11.174317702857635
2023-04-09 05:22:11,965 - INFO - training.closure - iteration 310: loss = -11.205505287172072
2023-04-09 05:22:36,557 - INFO - training.closure - iteration 311: loss = -11.040529863526737
2023-04-09 05:23:01,454 - INFO - training.closure - iteration 312: loss = -11.245853980749366
2023-04-09 05:23:26,030 - INFO - training.closure - iteration 313: loss = -11.057054771404772
2023-04-09 05:23:50,591 - INFO - training.closure - iteration 314: loss = -11.28855434045563
2023-04-09 05:24:15,057 - INFO - training.closure - iteration 315: loss = -11.333190772207757
2023-04-09 05:24:39,817 - INFO - training.closure - iteration 316: loss = -11.38577814881064
2023-04-09 05:25:04,211 - INFO - training.closure - iteration 317: loss = -11.350608822961018
2023-04-09 05:25:28,652 - INFO - training.closure - iteration 318: loss = -11.400562672094319
2023-04-09 05:25:53,089 - INFO - training.closure - iteration 319: loss = -11.420338653625688
2023-04-09 05:26:17,432 - INFO - training.closure - iteration 320: loss = -11.347002762913814
2023-04-09 05:26:41,775 - INFO - training.closure - iteration 321: loss = -11.429226870596068
2023-04-09 05:27:06,180 - INFO - training.closure - iteration 322: loss = -11.441826753308646
2023-04-09 05:27:30,559 - INFO - training.closure - iteration 323: loss = -11.454945701098659
2023-04-09 05:27:55,456 - INFO - training.closure - iteration 324: loss = -11.480201358718073
2023-04-09 05:28:19,941 - INFO - training.closure - iteration 325: loss = -11.50201800429506
2023-04-09 05:28:44,224 - INFO - training.closure - iteration 326: loss = -11.524623829579468
2023-04-09 05:29:08,968 - INFO - training.closure - iteration 327: loss = -11.544305116232476
2023-04-09 05:29:33,566 - INFO - training.closure - iteration 328: loss = -11.55592118333102
2023-04-09 05:29:58,024 - INFO - training.closure - iteration 329: loss = -11.563843361492395
2023-04-09 05:30:22,633 - INFO - training.closure - iteration 330: loss = -11.583587492103913
2023-04-09 05:30:47,097 - INFO - training.closure - iteration 331: loss = -11.600588725449589
2023-04-09 05:31:11,537 - INFO - training.closure - iteration 332: loss = -11.608206869945501
2023-04-09 05:31:35,968 - INFO - training.closure - iteration 333: loss = -11.610189087911813
2023-04-09 05:32:00,718 - INFO - training.closure - iteration 334: loss = -11.610849955282369
2023-04-09 05:32:25,426 - INFO - training.closure - iteration 335: loss = -11.612168622883566
2023-04-09 05:32:50,116 - INFO - training.closure - iteration 336: loss = -11.600466904026321
2023-04-09 05:33:14,660 - INFO - training.closure - iteration 337: loss = -11.614242153620939
2023-04-09 05:33:39,227 - INFO - training.closure - iteration 338: loss = -11.60855402228183
2023-04-09 05:34:03,734 - INFO - training.closure - iteration 339: loss = -11.616200104254432
2023-04-09 05:34:28,361 - INFO - training.closure - iteration 340: loss = -11.621070000246375
2023-04-09 05:34:53,418 - INFO - training.closure - iteration 341: loss = -11.626251438100564
2023-04-09 05:35:17,821 - INFO - training.closure - iteration 342: loss = -11.62659833847799
2023-04-09 05:35:42,545 - INFO - training.closure - iteration 343: loss = -11.627081868649237
2023-04-09 05:36:07,398 - INFO - training.closure - iteration 344: loss = -11.627822706698009
2023-04-09 05:36:31,984 - INFO - training.closure - iteration 345: loss = -11.62837609220939
2023-04-09 05:36:56,583 - INFO - training.closure - iteration 346: loss = -11.629055370829285
2023-04-09 05:37:20,903 - INFO - training.closure - iteration 347: loss = -11.629897000917051
2023-04-09 05:37:45,350 - INFO - training.closure - iteration 348: loss = -11.630964736043381
2023-04-09 05:38:09,816 - INFO - training.closure - iteration 349: loss = -11.632082635755264
2023-04-09 05:38:34,108 - INFO - training.closure - iteration 350: loss = -11.633165121569613
2023-04-09 05:38:58,568 - INFO - training.closure - iteration 351: loss = -11.63360801432254
2023-04-09 05:39:23,028 - INFO - training.closure - iteration 352: loss = -11.634066953253162
2023-04-09 05:39:47,717 - INFO - training.closure - iteration 353: loss = -11.625480515481142
2023-04-09 05:40:12,044 - INFO - training.closure - iteration 354: loss = -11.634213318807943
2023-04-09 05:40:36,428 - INFO - training.closure - iteration 355: loss = -11.634847587921886
2023-04-09 05:41:01,241 - INFO - training.closure - iteration 356: loss = -11.635503575317834
2023-04-09 05:41:25,628 - INFO - training.closure - iteration 357: loss = -11.636276400195854
2023-04-09 05:41:49,960 - INFO - training.closure - iteration 358: loss = -11.637616413481068
2023-04-09 05:42:14,381 - INFO - training.closure - iteration 359: loss = -11.612469295310792
2023-04-09 05:42:38,807 - INFO - training.closure - iteration 360: loss = -11.63810499451645
2023-04-09 05:43:03,500 - INFO - training.closure - iteration 361: loss = -11.640440083309382
2023-04-09 05:43:27,958 - INFO - training.closure - iteration 362: loss = -11.644109153496238
2023-04-09 05:43:52,456 - INFO - training.closure - iteration 363: loss = -11.646587806311551
2023-04-09 05:44:17,158 - INFO - training.closure - iteration 364: loss = -11.65059573740606
2023-04-09 05:44:41,756 - INFO - training.closure - iteration 365: loss = -11.652678747531501
2023-04-09 05:45:06,236 - INFO - training.closure - iteration 366: loss = -11.653727751866805
2023-04-09 05:45:30,645 - INFO - training.closure - iteration 367: loss = -11.65533741707338
2023-04-09 05:45:55,284 - INFO - training.closure - iteration 368: loss = -11.65772743233339
2023-04-09 05:46:19,786 - INFO - training.closure - iteration 369: loss = -11.662273302267396
2023-04-09 05:46:44,109 - INFO - training.closure - iteration 370: loss = -11.669744703254956
2023-04-09 05:47:08,847 - INFO - training.closure - iteration 371: loss = -11.676895847153329
2023-04-09 05:47:33,508 - INFO - training.closure - iteration 372: loss = -11.683822616774803
2023-04-09 05:47:58,062 - INFO - training.closure - iteration 373: loss = -11.6852536907763
2023-04-09 05:48:22,665 - INFO - training.closure - iteration 374: loss = -11.688836244927002
2023-04-09 05:48:47,253 - INFO - training.closure - iteration 375: loss = -11.694473325610716
2023-04-09 05:49:11,816 - INFO - training.closure - iteration 376: loss = -11.700088312028251
2023-04-09 05:49:36,615 - INFO - training.closure - iteration 377: loss = -11.719357459558339
2023-04-09 05:50:01,087 - INFO - training.closure - iteration 378: loss = -11.729072175162338
2023-04-09 05:50:25,637 - INFO - training.closure - iteration 379: loss = -11.734170765775993
2023-04-09 05:50:50,391 - INFO - training.closure - iteration 380: loss = -11.742081927539395
2023-04-09 05:51:15,592 - INFO - training.closure - iteration 381: loss = -11.74440166615868
2023-04-09 05:51:40,279 - INFO - training.closure - iteration 382: loss = -11.748266361471574
2023-04-09 05:52:04,792 - INFO - training.closure - iteration 383: loss = -11.750296229130598
2023-04-09 05:52:29,471 - INFO - training.closure - iteration 384: loss = -11.752170623238008
2023-04-09 05:52:53,896 - INFO - training.closure - iteration 385: loss = -11.753625254144197
2023-04-09 05:53:18,514 - INFO - training.closure - iteration 386: loss = -11.75500492088587
2023-04-09 05:53:43,175 - INFO - training.closure - iteration 387: loss = -11.762748006751401
2023-04-09 05:54:07,721 - INFO - training.closure - iteration 388: loss = -11.756415604117212
2023-04-09 05:54:32,469 - INFO - training.closure - iteration 389: loss = -11.76465040827237
2023-04-09 05:54:57,073 - INFO - training.closure - iteration 390: loss = -11.769061937840764
2023-04-09 05:55:21,545 - INFO - training.closure - iteration 391: loss = -11.775074386071697
2023-04-09 05:55:46,237 - INFO - training.closure - iteration 392: loss = -11.779746782828623
2023-04-09 05:56:10,758 - INFO - training.closure - iteration 393: loss = -11.790600199348589
2023-04-09 05:56:35,199 - INFO - training.closure - iteration 394: loss = -11.802333172029233
2023-04-09 05:56:59,576 - INFO - training.closure - iteration 395: loss = -11.63526804419292
2023-04-09 05:57:24,153 - INFO - training.closure - iteration 396: loss = -11.82532178245506
2023-04-09 05:57:48,636 - INFO - training.closure - iteration 397: loss = -11.840408985125826
2023-04-09 05:58:12,994 - INFO - training.closure - iteration 398: loss = -11.85436783192996
2023-04-09 05:58:37,583 - INFO - training.closure - iteration 399: loss = -11.862008979989259
2023-04-09 05:59:02,143 - INFO - training.closure - iteration 400: loss = -11.870585781290675
2023-04-09 05:59:26,733 - INFO - training.closure - iteration 401: loss = -11.876105866565904
2023-04-09 05:59:51,183 - INFO - training.closure - iteration 402: loss = -11.880616135757945
2023-04-09 06:00:15,842 - INFO - training.closure - iteration 403: loss = -11.883583450392916
2023-04-09 06:00:40,462 - INFO - training.closure - iteration 404: loss = -11.889519276188448
2023-04-09 06:01:05,096 - INFO - training.closure - iteration 405: loss = -11.897030394144178
2023-04-09 06:01:29,583 - INFO - training.closure - iteration 406: loss = -11.90102132938214
2023-04-09 06:01:54,368 - INFO - training.closure - iteration 407: loss = -11.907681041404802
2023-04-09 06:02:19,424 - INFO - training.closure - iteration 408: loss = -11.91240088275931
2023-04-09 06:02:43,980 - INFO - training.closure - iteration 409: loss = -11.915706104704764
2023-04-09 06:03:08,405 - INFO - training.closure - iteration 410: loss = -11.918399919506452
2023-04-09 06:03:32,815 - INFO - training.closure - iteration 411: loss = -11.9201006247656
2023-04-09 06:03:57,223 - INFO - training.closure - iteration 412: loss = -11.92159152255978
2023-04-09 06:04:21,699 - INFO - training.closure - iteration 413: loss = -11.739280623667398
2023-04-09 06:04:46,968 - INFO - training.closure - iteration 414: loss = -11.923373634723909
2023-04-09 06:05:11,299 - INFO - training.closure - iteration 415: loss = -11.927556318863505
2023-04-09 06:05:35,716 - INFO - training.closure - iteration 416: loss = -11.931564832477054
2023-04-09 06:06:00,191 - INFO - training.closure - iteration 417: loss = -11.934303538174031
2023-04-09 06:06:24,541 - INFO - training.closure - iteration 418: loss = -11.939768226580345
2023-04-09 06:06:48,857 - INFO - training.closure - iteration 419: loss = -11.947274194031852
2023-04-09 06:07:13,709 - INFO - training.closure - iteration 420: loss = -11.942159437240747
2023-04-09 06:07:38,019 - INFO - training.closure - iteration 421: loss = -11.949344798962375
2023-04-09 06:08:02,604 - INFO - training.closure - iteration 422: loss = -11.952008367060944
2023-04-09 06:08:27,342 - INFO - training.closure - iteration 423: loss = -11.954341564625034
2023-04-09 06:08:51,974 - INFO - training.closure - iteration 424: loss = -11.956299057054165
2023-04-09 06:09:16,364 - INFO - training.closure - iteration 425: loss = -11.959586908482425
2023-04-09 06:09:40,740 - INFO - training.closure - iteration 426: loss = -11.949986525877314
2023-04-09 06:10:05,298 - INFO - training.closure - iteration 427: loss = -11.960637932365998
2023-04-09 06:10:29,735 - INFO - training.closure - iteration 428: loss = -11.96267904953037
2023-04-09 06:10:54,471 - INFO - training.closure - iteration 429: loss = -11.959866295467181
2023-04-09 06:11:18,922 - INFO - training.closure - iteration 430: loss = -11.963274856867352
2023-04-09 06:11:43,164 - INFO - training.closure - iteration 431: loss = -11.964309888761052
2023-04-09 06:12:07,895 - INFO - training.closure - iteration 432: loss = -11.965133009199562
2023-04-09 06:12:32,349 - INFO - training.closure - iteration 433: loss = -11.963787944793303
2023-04-09 06:12:56,714 - INFO - training.closure - iteration 434: loss = -11.965809830335767
2023-04-09 06:13:21,360 - INFO - training.closure - iteration 435: loss = -11.967280954274297
2023-04-09 06:13:45,748 - INFO - training.closure - iteration 436: loss = -11.968179838224616
2023-04-09 06:14:10,265 - INFO - training.closure - iteration 437: loss = -11.968779088087416
2023-04-09 06:14:34,922 - INFO - training.closure - iteration 438: loss = -11.96896208689542
2023-04-09 06:14:59,379 - INFO - training.closure - iteration 439: loss = -11.969284566629772
2023-04-09 06:15:24,000 - INFO - training.closure - iteration 440: loss = -11.969467252913699
2023-04-09 06:15:48,612 - INFO - training.closure - iteration 441: loss = -11.969626270482525
2023-04-09 06:16:13,196 - INFO - training.closure - iteration 442: loss = -11.969728681089686
2023-04-09 06:16:37,951 - INFO - training.closure - iteration 443: loss = -11.969778260502741
2023-04-09 06:17:02,849 - INFO - training.closure - iteration 444: loss = -11.96986173139199
2023-04-09 06:17:27,753 - INFO - training.closure - iteration 445: loss = -11.969464010097436
2023-04-09 06:17:52,083 - INFO - training.closure - iteration 446: loss = -11.96988200108127
2023-04-09 06:18:16,518 - INFO - training.closure - iteration 447: loss = -11.96994899687379
2023-04-09 06:18:40,877 - INFO - training.closure - iteration 448: loss = -11.970028432622806
2023-04-09 06:19:05,360 - INFO - training.closure - iteration 449: loss = -11.97016973368516
2023-04-09 06:19:30,041 - INFO - training.closure - iteration 450: loss = -11.970500631473136
2023-04-09 06:19:54,367 - INFO - training.closure - iteration 451: loss = -11.970930200659708
2023-04-09 06:20:18,739 - INFO - training.closure - iteration 452: loss = -11.971445325011064
2023-04-09 06:20:43,227 - INFO - training.closure - iteration 453: loss = -11.968952004441439
2023-04-09 06:21:07,546 - INFO - training.closure - iteration 454: loss = -11.97181065343529
2023-04-09 06:21:32,148 - INFO - training.closure - iteration 455: loss = -11.972886360924905
2023-04-09 06:21:56,735 - INFO - training.closure - iteration 456: loss = -11.973708666664864
2023-04-09 06:22:21,349 - INFO - training.closure - iteration 457: loss = -11.974306302446966
2023-04-09 06:22:45,760 - INFO - training.closure - iteration 458: loss = -11.974657059884969
2023-04-09 06:23:10,289 - INFO - training.closure - iteration 459: loss = -11.97485957749383
2023-04-09 06:23:34,814 - INFO - training.closure - iteration 460: loss = -11.976248748055047
2023-04-09 06:23:59,363 - INFO - training.closure - iteration 461: loss = -11.97686431155152
2023-04-09 06:24:23,886 - INFO - training.closure - iteration 462: loss = -11.97787527274225
2023-04-09 06:24:48,225 - INFO - training.closure - iteration 463: loss = -11.978996035920828
2023-04-09 06:25:12,764 - INFO - training.closure - iteration 464: loss = -11.979757357694702
2023-04-09 06:25:37,489 - INFO - training.closure - iteration 465: loss = -11.980756649779465
2023-04-09 06:26:02,148 - INFO - training.closure - iteration 466: loss = -11.981126992571628
2023-04-09 06:26:26,387 - INFO - training.closure - iteration 467: loss = -11.981975296504178
2023-04-09 06:26:50,987 - INFO - training.closure - iteration 468: loss = -11.982700920537367
2023-04-09 06:27:15,309 - INFO - training.closure - iteration 469: loss = -11.984496882286567
2023-04-09 06:27:39,642 - INFO - training.closure - iteration 470: loss = -11.985025007273205
2023-04-09 06:28:04,173 - INFO - training.closure - iteration 471: loss = -11.98793266126344
2023-04-09 06:28:28,487 - INFO - training.closure - iteration 472: loss = -11.989029572531173
2023-04-09 06:28:52,912 - INFO - training.closure - iteration 473: loss = -11.99108783723605
2023-04-09 06:29:17,338 - INFO - training.closure - iteration 474: loss = -11.993034082961309
2023-04-09 06:29:41,901 - INFO - training.closure - iteration 475: loss = -11.99414774399009
2023-04-09 06:30:06,319 - INFO - training.closure - iteration 476: loss = -11.996407890025711
2023-04-09 06:30:30,826 - INFO - training.closure - iteration 477: loss = -11.99785234924219
2023-04-09 06:30:55,320 - INFO - training.closure - iteration 478: loss = -11.999514002246148
2023-04-09 06:31:19,612 - INFO - training.closure - iteration 479: loss = -12.001734792881749
2023-04-09 06:31:44,282 - INFO - training.closure - iteration 480: loss = -12.003490826154874
2023-04-09 06:32:08,983 - INFO - training.closure - iteration 481: loss = -11.990175100172754
2023-04-09 06:32:33,533 - INFO - training.closure - iteration 482: loss = -12.004019286948843
2023-04-09 06:32:57,966 - INFO - training.closure - iteration 483: loss = -12.005975004707075
2023-04-09 06:33:22,132 - INFO - training.closure - iteration 484: loss = -12.007288347874695
2023-04-09 06:33:46,678 - INFO - training.closure - iteration 485: loss = -12.00921455036654
2023-04-09 06:34:11,249 - INFO - training.closure - iteration 486: loss = -12.011329282044546
2023-04-09 06:34:35,641 - INFO - training.closure - iteration 487: loss = -12.013761214403502
2023-04-09 06:35:00,584 - INFO - training.closure - iteration 488: loss = -12.017607714056652
2023-04-09 06:35:24,825 - INFO - training.closure - iteration 489: loss = -12.005350024046965
2023-04-09 06:35:49,485 - INFO - training.closure - iteration 490: loss = -12.019847058279332
2023-04-09 06:36:13,836 - INFO - training.closure - iteration 491: loss = -12.026126292297853
2023-04-09 06:36:38,753 - INFO - training.closure - iteration 492: loss = -12.02951329728264
2023-04-09 06:37:03,177 - INFO - training.closure - iteration 493: loss = -12.033331272328175
2023-04-09 06:37:27,741 - INFO - training.closure - iteration 494: loss = -12.035506363462286
2023-04-09 06:37:52,513 - INFO - training.closure - iteration 495: loss = -12.037706679726874
2023-04-09 06:38:16,932 - INFO - training.closure - iteration 496: loss = -12.040162592264963
2023-04-09 06:38:41,464 - INFO - training.closure - iteration 497: loss = -12.042819019640472
2023-04-09 06:39:06,059 - INFO - training.closure - iteration 498: loss = -12.04412933283093
2023-04-09 06:39:30,797 - INFO - training.closure - iteration 499: loss = -12.046842188511924
2023-04-09 06:39:55,373 - INFO - training.closure - iteration 500: loss = -12.049413210787218
2023-04-09 06:40:19,835 - INFO - training.closure - iteration 501: loss = -12.050938641000673
2023-04-09 06:40:44,573 - INFO - training.closure - iteration 502: loss = -12.0523555914583
2023-04-09 06:41:09,092 - INFO - training.closure - iteration 503: loss = -12.054014175509803
2023-04-09 06:41:33,421 - INFO - training.closure - iteration 504: loss = -12.05786387341914
2023-04-09 06:41:57,876 - INFO - training.closure - iteration 505: loss = -12.060560530103324
2023-04-09 06:42:22,399 - INFO - training.closure - iteration 506: loss = -12.061874022091548
2023-04-09 06:42:46,783 - INFO - training.closure - iteration 507: loss = -12.062331143691711
2023-04-09 06:43:11,026 - INFO - training.closure - iteration 508: loss = -12.063167532612159
2023-04-09 06:43:35,809 - INFO - training.closure - iteration 509: loss = -12.061983982893807
2023-04-09 06:44:00,163 - INFO - training.closure - iteration 510: loss = -12.065232817962311
2023-04-09 06:44:24,427 - INFO - training.closure - iteration 511: loss = -12.06843760301545
2023-04-09 06:44:49,101 - INFO - training.closure - iteration 512: loss = -12.070760536187514
2023-04-09 06:45:13,308 - INFO - training.closure - iteration 513: loss = -12.07178994826117
2023-04-09 06:45:37,596 - INFO - training.closure - iteration 514: loss = -12.072669021904172
2023-04-09 06:46:01,926 - INFO - training.closure - iteration 515: loss = -12.07361931132112
2023-04-09 06:46:26,379 - INFO - training.closure - iteration 516: loss = -12.075511936227269
2023-04-09 06:46:50,631 - INFO - training.closure - iteration 517: loss = -12.06954438676516
2023-04-09 06:47:15,543 - INFO - training.closure - iteration 518: loss = -12.076222592591888
2023-04-09 06:47:40,138 - INFO - training.closure - iteration 519: loss = -12.078166963684575
2023-04-09 06:48:04,672 - INFO - training.closure - iteration 520: loss = -12.079421485333329
2023-04-09 06:48:29,159 - INFO - training.closure - iteration 521: loss = -12.080162832520152
2023-04-09 06:48:53,551 - INFO - training.closure - iteration 522: loss = -12.080610910563852
2023-04-09 06:49:18,140 - INFO - training.closure - iteration 523: loss = -12.081233564160783
2023-04-09 06:49:42,512 - INFO - training.closure - iteration 524: loss = -12.08164021584875
2023-04-09 06:50:06,884 - INFO - training.closure - iteration 525: loss = -12.082071808728621
2023-04-09 06:50:31,496 - INFO - training.closure - iteration 526: loss = -12.082441987859488
2023-04-09 06:50:55,894 - INFO - training.closure - iteration 527: loss = -12.082758036784227
2023-04-09 06:51:20,320 - INFO - training.closure - iteration 528: loss = -12.083074284267756
2023-04-09 06:51:44,817 - INFO - training.closure - iteration 529: loss = -12.083313229390932
2023-04-09 06:52:09,078 - INFO - training.closure - iteration 530: loss = -12.083522550726999
2023-04-09 06:52:33,529 - INFO - training.closure - iteration 531: loss = -12.083602741868305
2023-04-09 06:52:58,024 - INFO - training.closure - iteration 532: loss = -12.083898460983207
2023-04-09 06:53:22,433 - INFO - training.closure - iteration 533: loss = -12.084234105953328
2023-04-09 06:53:46,745 - INFO - training.closure - iteration 534: loss = -12.084788875528167
2023-04-09 06:54:11,061 - INFO - training.closure - iteration 535: loss = -12.085383987351031
2023-04-09 06:54:35,419 - INFO - training.closure - iteration 536: loss = -12.085819057245713
2023-04-09 06:54:59,892 - INFO - training.closure - iteration 537: loss = -12.086347153642382
2023-04-09 06:55:24,174 - INFO - training.closure - iteration 538: loss = -12.086756687879873
2023-04-09 06:55:48,712 - INFO - training.closure - iteration 539: loss = -12.087077118620616
2023-04-09 06:56:13,247 - INFO - training.closure - iteration 540: loss = -12.087417587528094
2023-04-09 06:56:37,651 - INFO - training.closure - iteration 541: loss = -12.087750115582512
2023-04-09 06:57:02,340 - INFO - training.closure - iteration 542: loss = -12.08835418791973
2023-04-09 06:57:26,681 - INFO - training.closure - iteration 543: loss = -12.089021093522643
2023-04-09 06:57:51,102 - INFO - training.closure - iteration 544: loss = -12.089454619278326
2023-04-09 06:58:15,574 - INFO - training.closure - iteration 545: loss = -12.08977495319204
2023-04-09 06:58:40,089 - INFO - training.closure - iteration 546: loss = -12.088920977734976
2023-04-09 06:59:04,536 - INFO - training.closure - iteration 547: loss = -12.089874574886867
2023-04-09 06:59:28,794 - INFO - training.closure - iteration 548: loss = -12.08996786390557
2023-04-09 06:59:53,269 - INFO - training.closure - iteration 549: loss = -12.090110502365313
2023-04-09 07:00:17,750 - INFO - training.closure - iteration 550: loss = -12.09018143137284
2023-04-09 07:00:42,047 - INFO - training.closure - iteration 551: loss = -12.090287967406875
2023-04-09 07:01:06,918 - INFO - training.closure - iteration 552: loss = -12.090416835300744
2023-04-09 07:01:31,097 - INFO - training.closure - iteration 553: loss = -12.090611260307213
2023-04-09 07:01:55,517 - INFO - training.closure - iteration 554: loss = -12.090900831943891
2023-04-09 07:02:20,151 - INFO - training.closure - iteration 555: loss = -12.090309164507339
2023-04-09 07:02:44,589 - INFO - training.closure - iteration 556: loss = -12.091018008110265
2023-04-09 07:03:09,271 - INFO - training.closure - iteration 557: loss = -12.091281819858196
2023-04-09 07:03:33,859 - INFO - training.closure - iteration 558: loss = -12.091350443775092
2023-04-09 07:03:58,147 - INFO - training.closure - iteration 559: loss = -12.091401145840612
2023-04-09 07:04:22,477 - INFO - training.closure - iteration 560: loss = -12.091488414554044
2023-04-09 07:04:47,140 - INFO - training.closure - iteration 561: loss = -12.09161159465702
2023-04-09 07:05:11,459 - INFO - training.closure - iteration 562: loss = -12.091491189331869
2023-04-09 07:05:35,878 - INFO - training.closure - iteration 563: loss = -12.091664645149379
2023-04-09 07:06:00,510 - INFO - training.closure - iteration 564: loss = -12.091778786394803
2023-04-09 07:06:25,149 - INFO - training.closure - iteration 565: loss = -12.091906699405724
2023-04-09 07:06:49,574 - INFO - training.closure - iteration 566: loss = -12.092027785920743
2023-04-09 07:07:13,814 - INFO - training.closure - iteration 567: loss = -12.092154505118668
2023-04-09 07:07:38,125 - INFO - training.closure - iteration 568: loss = -12.092305693051113
2023-04-09 07:08:02,477 - INFO - training.closure - iteration 569: loss = -12.092464367716621
2023-04-09 07:08:27,023 - INFO - training.closure - iteration 570: loss = -12.092622042917789
2023-04-09 07:08:51,494 - INFO - training.closure - iteration 571: loss = -12.092843553384029
2023-04-09 07:09:15,802 - INFO - training.closure - iteration 572: loss = -12.093019453595423
2023-04-09 07:09:40,253 - INFO - training.closure - iteration 573: loss = -12.09312752468258
2023-04-09 07:10:04,774 - INFO - training.closure - iteration 574: loss = -12.093192439982811
2023-04-09 07:10:29,075 - INFO - training.closure - iteration 575: loss = -12.093266029559675
2023-04-09 07:10:54,180 - INFO - training.closure - iteration 576: loss = -12.093363584201885
2023-04-09 07:11:18,697 - INFO - training.closure - iteration 577: loss = -12.093509107276814
2023-04-09 07:11:42,882 - INFO - training.closure - iteration 578: loss = -12.093092090763957
2023-04-09 07:12:07,243 - INFO - training.closure - iteration 579: loss = -12.09354014895014
2023-04-09 07:12:31,891 - INFO - training.closure - iteration 580: loss = -12.093618455456912
2023-04-09 07:12:56,279 - INFO - training.closure - iteration 581: loss = -12.093661362402553
2023-04-09 07:13:20,692 - INFO - training.closure - iteration 582: loss = -12.093695952515496
2023-04-09 07:13:45,104 - INFO - training.closure - iteration 583: loss = -12.09371000273201
2023-04-09 07:14:09,399 - INFO - training.closure - iteration 584: loss = -12.093753557780467
2023-04-09 07:14:33,970 - INFO - training.closure - iteration 585: loss = -12.093914533740161
2023-04-09 07:14:58,439 - INFO - training.closure - iteration 586: loss = -12.094156375862529
2023-04-09 07:15:22,817 - INFO - training.closure - iteration 587: loss = -12.094399292881892
2023-04-09 07:15:47,266 - INFO - training.closure - iteration 588: loss = -12.094559339749516
2023-04-09 07:16:11,868 - INFO - training.closure - iteration 589: loss = -12.094328685760413
2023-04-09 07:16:36,417 - INFO - training.closure - iteration 590: loss = -12.094642800565072
2023-04-09 07:17:00,902 - INFO - training.closure - iteration 591: loss = -12.094752785025616
2023-04-09 07:17:25,860 - INFO - training.closure - iteration 592: loss = -12.094875462927597
2023-04-09 07:17:50,487 - INFO - training.closure - iteration 593: loss = -12.095028223470614
2023-04-09 07:18:15,114 - INFO - training.closure - iteration 594: loss = -12.095095976808105
2023-04-09 07:18:39,781 - INFO - training.closure - iteration 595: loss = -12.094747068937085
2023-04-09 07:19:04,260 - INFO - training.closure - iteration 596: loss = -12.09514505116303
2023-04-09 07:19:29,035 - INFO - training.closure - iteration 597: loss = -12.095185578241043
2023-04-09 07:19:53,681 - INFO - training.closure - iteration 598: loss = -12.095277488602221
2023-04-09 07:20:18,043 - INFO - training.closure - iteration 599: loss = -12.095580961896307
2023-04-09 07:20:42,609 - INFO - training.closure - iteration 600: loss = -12.09599651613693
2023-04-09 07:21:06,999 - INFO - training.closure - iteration 601: loss = -12.096622981992436
2023-04-09 07:21:31,576 - INFO - training.closure - iteration 602: loss = -12.096924597626916
2023-04-09 07:21:56,064 - INFO - training.closure - iteration 603: loss = -12.096892014770225
2023-04-09 07:22:20,532 - INFO - training.closure - iteration 604: loss = -12.097251093073973
2023-04-09 07:22:45,214 - INFO - training.closure - iteration 605: loss = -12.097413261417506
2023-04-09 07:23:09,635 - INFO - training.closure - iteration 606: loss = -12.097534379387284
2023-04-09 07:23:33,830 - INFO - training.closure - iteration 607: loss = -12.097625276380928
2023-04-09 07:23:58,060 - INFO - training.closure - iteration 608: loss = -12.097735132778428
2023-04-09 07:24:22,509 - INFO - training.closure - iteration 609: loss = -12.097961627617437
2023-04-09 07:24:47,005 - INFO - training.closure - iteration 610: loss = -12.098263283661183
2023-04-09 07:25:11,317 - INFO - training.closure - iteration 611: loss = -12.06199852334565
2023-04-09 07:25:35,707 - INFO - training.closure - iteration 612: loss = -12.098545697006767
2023-04-09 07:26:00,209 - INFO - training.closure - iteration 613: loss = -12.099459566993616
2023-04-09 07:26:24,601 - INFO - training.closure - iteration 614: loss = -12.100286123497996
2023-04-09 07:26:49,013 - INFO - training.closure - iteration 615: loss = -12.101482428099805
2023-04-09 07:27:13,360 - INFO - training.closure - iteration 616: loss = -12.102972481876243
2023-04-09 07:27:37,855 - INFO - training.closure - iteration 617: loss = -12.105730383398756
2023-04-09 07:28:02,312 - INFO - training.closure - iteration 618: loss = -12.106710282521224
2023-04-09 07:28:26,939 - INFO - training.closure - iteration 619: loss = -12.108956555251325
2023-04-09 07:28:51,474 - INFO - training.closure - iteration 620: loss = -12.109527272337617
2023-04-09 07:29:15,731 - INFO - training.closure - iteration 621: loss = -12.110318613115297
2023-04-09 07:29:40,231 - INFO - training.closure - iteration 622: loss = -12.111136360680838
2023-04-09 07:30:04,671 - INFO - training.closure - iteration 623: loss = -12.111693034672363
2023-04-09 07:30:29,138 - INFO - training.closure - iteration 624: loss = -12.105108888621347
2023-04-09 07:30:53,740 - INFO - training.closure - iteration 625: loss = -12.111967864879563
2023-04-09 07:31:18,280 - INFO - training.closure - iteration 626: loss = -12.112265941848797
2023-04-09 07:31:42,743 - INFO - training.closure - iteration 627: loss = -12.112480789400465
2023-04-09 07:32:07,275 - INFO - training.closure - iteration 628: loss = -12.112738421876706
2023-04-09 07:32:32,146 - INFO - training.closure - iteration 629: loss = -12.113067654692605
2023-04-09 07:32:56,595 - INFO - training.closure - iteration 630: loss = -12.113654009810947
2023-04-09 07:33:21,052 - INFO - training.closure - iteration 631: loss = -12.114247026500943
2023-04-09 07:33:45,394 - INFO - training.closure - iteration 632: loss = -12.114911791728904
2023-04-09 07:34:10,011 - INFO - training.closure - iteration 633: loss = -12.115427107772689
2023-04-09 07:34:34,442 - INFO - training.closure - iteration 634: loss = -12.116411293293215
2023-04-09 07:34:59,004 - INFO - training.closure - iteration 635: loss = -12.116645070887333
2023-04-09 07:35:23,499 - INFO - training.closure - iteration 636: loss = -12.116776946392397
2023-04-09 07:35:47,843 - INFO - training.closure - iteration 637: loss = -12.116761070121758
2023-04-09 07:36:12,511 - INFO - training.closure - iteration 638: loss = -12.117009740728614
2023-04-09 07:36:36,945 - INFO - training.closure - iteration 639: loss = -12.117334121057759
2023-04-09 07:37:01,217 - INFO - training.closure - iteration 640: loss = -12.11781908260726
2023-04-09 07:37:26,070 - INFO - training.closure - iteration 641: loss = -12.118369396455954
2023-04-09 07:37:50,259 - INFO - training.closure - iteration 642: loss = -12.11890317951066
2023-04-09 07:38:14,784 - INFO - training.closure - iteration 643: loss = -12.117842918169739
2023-04-09 07:38:39,043 - INFO - training.closure - iteration 644: loss = -12.119061666419682
2023-04-09 07:39:03,623 - INFO - training.closure - iteration 645: loss = -12.119318405791548
2023-04-09 07:39:28,037 - INFO - training.closure - iteration 646: loss = -12.119541247515595
2023-04-09 07:39:52,515 - INFO - training.closure - iteration 647: loss = -12.11991065641385
2023-04-09 07:40:16,515 - INFO - training.closure - iteration 648: loss = -12.120238718715893
2023-04-09 07:40:40,999 - INFO - training.closure - iteration 649: loss = -12.120489384459056
2023-04-09 07:41:05,595 - INFO - training.closure - iteration 650: loss = -12.120695942006817
2023-04-09 07:41:30,044 - INFO - training.closure - iteration 651: loss = -12.1208505965491
2023-04-09 07:41:54,504 - INFO - training.closure - iteration 652: loss = -12.121072506610485
2023-04-09 07:42:19,068 - INFO - training.closure - iteration 653: loss = -12.121377451356908
2023-04-09 07:42:43,562 - INFO - training.closure - iteration 654: loss = -12.121613849224685
2023-04-09 07:43:07,920 - INFO - training.closure - iteration 655: loss = -12.122353932793725
2023-04-09 07:43:32,383 - INFO - training.closure - iteration 656: loss = -12.12331475133338
2023-04-09 07:43:56,735 - INFO - training.closure - iteration 657: loss = -12.124494140597788
2023-04-09 07:44:20,973 - INFO - training.closure - iteration 658: loss = -12.125842661006619
2023-04-09 07:44:45,290 - INFO - training.closure - iteration 659: loss = -12.127057378189106
2023-04-09 07:45:09,490 - INFO - training.closure - iteration 660: loss = -12.127719419860824
2023-04-09 07:45:34,035 - INFO - training.closure - iteration 661: loss = -12.128358442245542
2023-04-09 07:45:58,457 - INFO - training.closure - iteration 662: loss = -12.128648670053943
2023-04-09 07:46:22,866 - INFO - training.closure - iteration 663: loss = -12.129522497234596
2023-04-09 07:46:47,120 - INFO - training.closure - iteration 664: loss = -12.12969739105759
2023-04-09 07:47:12,008 - INFO - training.closure - iteration 665: loss = -12.129824564129859
2023-04-09 07:47:36,669 - INFO - training.closure - iteration 666: loss = -12.129871660933642
2023-04-09 07:48:01,161 - INFO - training.closure - iteration 667: loss = -12.129959293846774
2023-04-09 07:48:25,717 - INFO - training.closure - iteration 668: loss = -12.13002291131021
2023-04-09 07:48:50,153 - INFO - training.closure - iteration 669: loss = -12.130104788627744
2023-04-09 07:49:14,738 - INFO - training.closure - iteration 670: loss = -12.130266295759235
2023-04-09 07:49:38,988 - INFO - training.closure - iteration 671: loss = -12.130560475958625
2023-04-09 07:50:03,519 - INFO - training.closure - iteration 672: loss = -12.130898458843031
2023-04-09 07:50:28,331 - INFO - training.closure - iteration 673: loss = -12.130555550175448
2023-04-09 07:50:53,194 - INFO - training.closure - iteration 674: loss = -12.131171627926516
2023-04-09 07:51:17,578 - INFO - training.closure - iteration 675: loss = -12.131407926329938
2023-04-09 07:51:42,182 - INFO - training.closure - iteration 676: loss = -12.131560902031474
2023-04-09 07:52:06,626 - INFO - training.closure - iteration 677: loss = -12.131660727712784
2023-04-09 07:52:31,216 - INFO - training.closure - iteration 678: loss = -12.131950020580765
2023-04-09 07:52:55,681 - INFO - training.closure - iteration 679: loss = -12.132358692305495
2023-04-09 07:53:20,308 - INFO - training.closure - iteration 680: loss = -12.132782304516777
2023-04-09 07:53:44,986 - INFO - training.closure - iteration 681: loss = -12.132999840758654
2023-04-09 07:54:09,319 - INFO - training.closure - iteration 682: loss = -12.133112123983704
2023-04-09 07:54:33,748 - INFO - training.closure - iteration 683: loss = -12.133224094452105
2023-04-09 07:54:58,293 - INFO - training.closure - iteration 684: loss = -12.133414301794035
2023-04-09 07:55:22,658 - INFO - training.closure - iteration 685: loss = -12.13399216602476
2023-04-09 07:55:47,287 - INFO - training.closure - iteration 686: loss = -12.13416635695948
2023-04-09 07:56:11,780 - INFO - training.closure - iteration 687: loss = -12.134481562657417
2023-04-09 07:56:36,341 - INFO - training.closure - iteration 688: loss = -12.13473694905624
2023-04-09 07:57:00,874 - INFO - training.closure - iteration 689: loss = -12.135281912222442
2023-04-09 07:57:25,173 - INFO - training.closure - iteration 690: loss = -12.135713165621357
2023-04-09 07:57:49,546 - INFO - training.closure - iteration 691: loss = -12.13657406643861
2023-04-09 07:58:13,822 - INFO - training.closure - iteration 692: loss = -12.13708382105299
2023-04-09 07:58:38,642 - INFO - training.closure - iteration 693: loss = -12.137455732239925
2023-04-09 07:59:03,164 - INFO - training.closure - iteration 694: loss = -12.137862832469386
2023-04-09 07:59:27,768 - INFO - training.closure - iteration 695: loss = -12.138135322517645
2023-04-09 07:59:52,194 - INFO - training.closure - iteration 696: loss = -12.138228381161056
2023-04-09 08:00:16,592 - INFO - training.closure - iteration 697: loss = -12.138445785989429
2023-04-09 08:00:41,194 - INFO - training.closure - iteration 698: loss = -12.138509474829707
2023-04-09 08:01:06,053 - INFO - training.closure - iteration 699: loss = -12.138623188340267
2023-04-09 08:01:32,602 - INFO - training.closure - iteration 700: loss = -12.13887118706835
2023-04-09 08:01:58,115 - INFO - training.closure - iteration 701: loss = -12.139380741406997
2023-04-09 08:02:22,793 - INFO - training.closure - iteration 702: loss = -12.127895288020259
2023-04-09 08:02:47,418 - INFO - training.closure - iteration 703: loss = -12.13954303333248
2023-04-09 08:03:11,850 - INFO - training.closure - iteration 704: loss = -12.13989821553851
2023-04-09 08:03:36,094 - INFO - training.closure - iteration 705: loss = -12.132739836398713
2023-04-09 08:04:00,701 - INFO - training.closure - iteration 706: loss = -12.140044896658697
2023-04-09 08:04:24,996 - INFO - training.closure - iteration 707: loss = -12.14025997767284
2023-04-09 08:04:49,929 - INFO - training.closure - iteration 708: loss = -12.14000139125757
2023-04-09 08:05:14,437 - INFO - training.closure - iteration 709: loss = -12.140500077458153
2023-04-09 08:05:39,056 - INFO - training.closure - iteration 710: loss = -12.14076641654009
2023-04-09 08:06:03,327 - INFO - training.closure - iteration 711: loss = -12.140866878431325
2023-04-09 08:06:27,684 - INFO - training.closure - iteration 712: loss = -12.140942900900058
2023-04-09 08:06:52,210 - INFO - training.closure - iteration 713: loss = -12.141038736420374
2023-04-09 08:07:16,538 - INFO - training.closure - iteration 714: loss = -12.141173175012927
2023-04-09 08:07:40,854 - INFO - training.closure - iteration 715: loss = -12.141570069682661
2023-04-09 08:08:05,244 - INFO - training.closure - iteration 716: loss = -12.141673053723965
2023-04-09 08:08:29,595 - INFO - training.closure - iteration 717: loss = -12.139850021008826
2023-04-09 08:08:53,928 - INFO - training.closure - iteration 718: loss = -12.141724190679076
2023-04-09 08:09:18,765 - INFO - training.closure - iteration 719: loss = -12.141869703355987
2023-04-09 08:09:43,340 - INFO - training.closure - iteration 720: loss = -12.142111074180994
2023-04-09 08:10:07,873 - INFO - training.closure - iteration 721: loss = -12.142437031219742
2023-04-09 08:10:32,142 - INFO - training.closure - iteration 722: loss = -12.142497979624768
2023-04-09 08:10:56,982 - INFO - training.closure - iteration 723: loss = -12.142884171061873
2023-04-09 08:11:21,560 - INFO - training.closure - iteration 724: loss = -12.143036327729174
2023-04-09 08:11:46,193 - INFO - training.closure - iteration 725: loss = -12.143331437791069
2023-04-09 08:12:10,576 - INFO - training.closure - iteration 726: loss = -12.143534466320366
2023-04-09 08:12:35,005 - INFO - training.closure - iteration 727: loss = -12.144022331696291
2023-04-09 08:12:59,636 - INFO - training.closure - iteration 728: loss = -12.144768768080972
2023-04-09 08:13:24,114 - INFO - training.closure - iteration 729: loss = -12.145662089925896
2023-04-09 08:13:48,778 - INFO - training.closure - iteration 730: loss = -12.146119542793343
2023-04-09 08:14:13,377 - INFO - training.closure - iteration 731: loss = -12.1468120028072
2023-04-09 08:14:37,761 - INFO - training.closure - iteration 732: loss = -12.147255965087451
2023-04-09 08:15:02,475 - INFO - training.closure - iteration 733: loss = -12.14791063680218
2023-04-09 08:15:27,077 - INFO - training.closure - iteration 734: loss = -12.148566882246561
2023-04-09 08:15:51,517 - INFO - training.closure - iteration 735: loss = -12.149450158822813
2023-04-09 08:16:16,002 - INFO - training.closure - iteration 736: loss = -12.149978958092325
2023-04-09 08:16:40,364 - INFO - training.closure - iteration 737: loss = -12.150105358435326
2023-04-09 08:17:04,682 - INFO - training.closure - iteration 738: loss = -12.1502273695655
2023-04-09 08:17:29,310 - INFO - training.closure - iteration 739: loss = -12.150380934161904
2023-04-09 08:17:53,904 - INFO - training.closure - iteration 740: loss = -12.15062556683884
2023-04-09 08:18:18,463 - INFO - training.closure - iteration 741: loss = -12.149124335708535
2023-04-09 08:18:42,711 - INFO - training.closure - iteration 742: loss = -12.15073728130633
2023-04-09 08:19:07,101 - INFO - training.closure - iteration 743: loss = -12.1511212660343
2023-04-09 08:19:31,492 - INFO - training.closure - iteration 744: loss = -12.151439278384508
2023-04-09 08:19:55,942 - INFO - training.closure - iteration 745: loss = -12.15166488966861
2023-04-09 08:20:20,615 - INFO - training.closure - iteration 746: loss = -12.15184496841892
2023-04-09 08:20:45,133 - INFO - training.closure - iteration 747: loss = -12.151997364766672
2023-04-09 08:21:09,429 - INFO - training.closure - iteration 748: loss = -12.152329818674277
2023-04-09 08:21:34,422 - INFO - training.closure - iteration 749: loss = -12.152596270225304
2023-04-09 08:21:59,111 - INFO - training.closure - iteration 750: loss = -12.153046113625898
2023-04-09 08:22:23,898 - INFO - training.closure - iteration 751: loss = -12.153864610746545
2023-04-09 08:22:48,354 - INFO - training.closure - iteration 752: loss = -12.140231947559126
2023-04-09 08:23:12,479 - INFO - training.closure - iteration 753: loss = -12.15401819949133
2023-04-09 08:23:37,085 - INFO - training.closure - iteration 754: loss = -12.154798425696441
2023-04-09 08:24:01,620 - INFO - training.closure - iteration 755: loss = -12.143830069468157
2023-04-09 08:24:26,100 - INFO - training.closure - iteration 756: loss = -12.155652044743654
2023-04-09 08:24:50,526 - INFO - training.closure - iteration 757: loss = -12.156839353741288
2023-04-09 08:25:14,879 - INFO - training.closure - iteration 758: loss = -12.159302375916567
2023-04-09 08:25:39,382 - INFO - training.closure - iteration 759: loss = -12.160257801671634
2023-04-09 08:26:03,973 - INFO - training.closure - iteration 760: loss = -12.16077639157095
2023-04-09 08:26:28,609 - INFO - training.closure - iteration 761: loss = -12.161631508951837
2023-04-09 08:26:52,971 - INFO - training.closure - iteration 762: loss = -12.161871815643915
2023-04-09 08:27:17,446 - INFO - training.closure - iteration 763: loss = -12.162396267643672
2023-04-09 08:27:41,781 - INFO - training.closure - iteration 764: loss = -12.162878578979626
2023-04-09 08:28:06,093 - INFO - training.closure - iteration 765: loss = -12.163465172640592
2023-04-09 08:28:31,136 - INFO - training.closure - iteration 766: loss = -12.163070650798785
2023-04-09 08:28:55,654 - INFO - training.closure - iteration 767: loss = -12.163703593608195
2023-04-09 08:29:20,387 - INFO - training.closure - iteration 768: loss = -12.163859930912148
2023-04-09 08:29:45,173 - INFO - training.closure - iteration 769: loss = -12.164094586106675
2023-04-09 08:30:09,470 - INFO - training.closure - iteration 770: loss = -12.164227656235665
2023-04-09 08:30:33,996 - INFO - training.closure - iteration 771: loss = -12.164554355589921
2023-04-09 08:30:58,483 - INFO - training.closure - iteration 772: loss = -12.165018346722814
2023-04-09 08:31:22,953 - INFO - training.closure - iteration 773: loss = -12.165598404280214
2023-04-09 08:31:47,355 - INFO - training.closure - iteration 774: loss = -12.166346588239142
2023-04-09 08:32:11,743 - INFO - training.closure - iteration 775: loss = -12.16694160608359
2023-04-09 08:32:36,599 - INFO - training.closure - iteration 776: loss = -12.167416821978648
2023-04-09 08:33:01,157 - INFO - training.closure - iteration 777: loss = -12.167725006942327
2023-04-09 08:33:25,391 - INFO - training.closure - iteration 778: loss = -12.16803634544187
2023-04-09 08:33:49,878 - INFO - training.closure - iteration 779: loss = -12.168374514302947
2023-04-09 08:34:14,315 - INFO - training.closure - iteration 780: loss = -12.16889169473902
2023-04-09 08:34:38,494 - INFO - training.closure - iteration 781: loss = -12.169810567718848
2023-04-09 08:35:03,145 - INFO - training.closure - iteration 782: loss = -12.163302041566464
2023-04-09 08:35:27,847 - INFO - training.closure - iteration 783: loss = -12.16999886192621
2023-04-09 08:35:52,323 - INFO - training.closure - iteration 784: loss = -12.170873045449813
2023-04-09 08:36:16,856 - INFO - training.closure - iteration 785: loss = -12.171578993138173
2023-04-09 08:36:41,627 - INFO - training.closure - iteration 786: loss = -12.172145422523378
2023-04-09 08:37:06,164 - INFO - training.closure - iteration 787: loss = -12.172442662473891
2023-04-09 08:37:30,358 - INFO - training.closure - iteration 788: loss = -12.172786925336002
2023-04-09 08:37:55,098 - INFO - training.closure - iteration 789: loss = -12.173300699729158
2023-04-09 08:38:19,815 - INFO - training.closure - iteration 790: loss = -12.172595665114363
2023-04-09 08:38:44,108 - INFO - training.closure - iteration 791: loss = -12.173455924595658
2023-04-09 08:39:08,579 - INFO - training.closure - iteration 792: loss = -12.1736543743872
2023-04-09 08:39:33,595 - INFO - training.closure - iteration 793: loss = -12.174190350816012
2023-04-09 08:39:58,235 - INFO - training.closure - iteration 794: loss = -12.174604741889564
2023-04-09 08:40:22,515 - INFO - training.closure - iteration 795: loss = -12.175140499005956
2023-04-09 08:40:47,115 - INFO - training.closure - iteration 796: loss = -12.175530000863697
2023-04-09 08:41:11,650 - INFO - training.closure - iteration 797: loss = -12.175809062350165
2023-04-09 08:41:36,202 - INFO - training.closure - iteration 798: loss = -12.176303412486615
2023-04-09 08:42:00,718 - INFO - training.closure - iteration 799: loss = -12.17669382191845
2023-04-09 08:42:25,459 - INFO - training.closure - iteration 800: loss = -12.177381374853894
2023-04-09 08:42:50,064 - INFO - training.closure - iteration 801: loss = -12.178311928537642
2023-04-09 08:43:14,499 - INFO - training.closure - iteration 802: loss = -12.172028223630814
2023-04-09 08:43:38,969 - INFO - training.closure - iteration 803: loss = -12.178933522321959
2023-04-09 08:44:03,462 - INFO - training.closure - iteration 804: loss = -12.179766157364831
2023-04-09 08:44:27,706 - INFO - training.closure - iteration 805: loss = -12.18166566849109
2023-04-09 08:44:52,450 - INFO - training.closure - iteration 806: loss = -12.18257646379319
2023-04-09 08:45:16,908 - INFO - training.closure - iteration 807: loss = -12.184694468529358
2023-04-09 08:45:41,281 - INFO - training.closure - iteration 808: loss = -12.186713804931973
2023-04-09 08:46:06,006 - INFO - training.closure - iteration 809: loss = -12.188916788214943
2023-04-09 08:46:30,707 - INFO - training.closure - iteration 810: loss = -12.190156075129469
2023-04-09 08:46:55,275 - INFO - training.closure - iteration 811: loss = -12.191871534419366
2023-04-09 08:47:20,055 - INFO - training.closure - iteration 812: loss = -12.192761306186334
2023-04-09 08:47:44,799 - INFO - training.closure - iteration 813: loss = -12.19357437048555
2023-04-09 08:48:09,342 - INFO - training.closure - iteration 814: loss = -12.194657023947023
2023-04-09 08:48:33,817 - INFO - training.closure - iteration 815: loss = -12.195397182940216
2023-04-09 08:48:58,297 - INFO - training.closure - iteration 816: loss = -12.195687194089931
2023-04-09 08:49:22,698 - INFO - training.closure - iteration 817: loss = -12.195895306411057
2023-04-09 08:49:47,524 - INFO - training.closure - iteration 818: loss = -12.195895835381247
2023-04-09 08:50:11,901 - INFO - training.closure - iteration 819: loss = -12.196284029348547
2023-04-09 08:50:36,525 - INFO - training.closure - iteration 820: loss = -12.196811686657226
2023-04-09 08:51:01,092 - INFO - training.closure - iteration 821: loss = -12.19640993846187
2023-04-09 08:51:25,271 - INFO - training.closure - iteration 822: loss = -12.197090641547682
2023-04-09 08:51:49,725 - INFO - training.closure - iteration 823: loss = -12.197434428759315
2023-04-09 08:52:14,311 - INFO - training.closure - iteration 824: loss = -12.197779712307502
2023-04-09 08:52:38,741 - INFO - training.closure - iteration 825: loss = -12.198089517751143
2023-04-09 08:53:03,194 - INFO - training.closure - iteration 826: loss = -12.197969299208179
2023-04-09 08:53:27,583 - INFO - training.closure - iteration 827: loss = -12.198515568343058
2023-04-09 08:53:52,059 - INFO - training.closure - iteration 828: loss = -12.199499056061864
2023-04-09 08:54:16,518 - INFO - training.closure - iteration 829: loss = -12.200839423547986
2023-04-09 08:54:41,034 - INFO - training.closure - iteration 830: loss = -12.201204828467958
2023-04-09 08:55:05,551 - INFO - training.closure - iteration 831: loss = -12.20146184174842
2023-04-09 08:55:29,845 - INFO - training.closure - iteration 832: loss = -12.201587766347203
2023-04-09 08:55:54,241 - INFO - training.closure - iteration 833: loss = -12.201856784494023
2023-04-09 08:56:18,985 - INFO - training.closure - iteration 834: loss = -12.2022059265245
2023-04-09 08:56:43,608 - INFO - training.closure - iteration 835: loss = -12.202419215242186
2023-04-09 08:57:08,023 - INFO - training.closure - iteration 836: loss = -12.201831892783948
2023-04-09 08:57:32,411 - INFO - training.closure - iteration 837: loss = -12.202453595731455
2023-04-09 08:57:57,129 - INFO - training.closure - iteration 838: loss = -12.202546304561949
2023-04-09 08:58:21,393 - INFO - training.closure - iteration 839: loss = -12.20269874173987
2023-04-09 08:58:46,036 - INFO - training.closure - iteration 840: loss = -12.202907609317716
2023-04-09 08:59:10,612 - INFO - training.closure - iteration 841: loss = -12.203096000413574
2023-04-09 08:59:34,915 - INFO - training.closure - iteration 842: loss = -12.202267705923392
2023-04-09 08:59:59,723 - INFO - training.closure - iteration 843: loss = -12.203161214362467
2023-04-09 09:00:24,311 - INFO - training.closure - iteration 844: loss = -12.203353786519134
2023-04-09 09:00:48,764 - INFO - training.closure - iteration 845: loss = -12.20349922853228
2023-04-09 09:01:13,509 - INFO - training.closure - iteration 846: loss = -12.203585718219276
2023-04-09 09:01:37,967 - INFO - training.closure - iteration 847: loss = -12.20365034829274
2023-04-09 09:02:02,531 - INFO - training.closure - iteration 848: loss = -12.203751513758244
2023-04-09 09:02:27,146 - INFO - training.closure - iteration 849: loss = -12.203879293736748
2023-04-09 09:02:51,775 - INFO - training.closure - iteration 850: loss = -12.204009907710713
2023-04-09 09:03:16,367 - INFO - training.closure - iteration 851: loss = -12.204230242821842
2023-04-09 09:03:41,176 - INFO - training.closure - iteration 852: loss = -12.204346466690918
2023-04-09 09:04:05,789 - INFO - training.closure - iteration 853: loss = -12.204682467829244
2023-04-09 09:04:30,600 - INFO - training.closure - iteration 854: loss = -12.205079403159509
2023-04-09 09:04:55,360 - INFO - training.closure - iteration 855: loss = -12.199017927069011
2023-04-09 09:05:19,800 - INFO - training.closure - iteration 856: loss = -12.20524418990769
2023-04-09 09:05:44,326 - INFO - training.closure - iteration 857: loss = -12.205658022268707
2023-04-09 09:06:07,661 - INFO - training.closure - iteration 858: loss = -12.206274974379312
2023-04-09 09:06:32,241 - INFO - training.closure - iteration 859: loss = -12.206814429118822
2023-04-09 09:06:56,722 - INFO - training.closure - iteration 860: loss = -12.207227488474032
2023-04-09 09:07:21,316 - INFO - training.closure - iteration 861: loss = -12.207382407412084
2023-04-09 09:07:45,634 - INFO - training.closure - iteration 862: loss = -12.207463196069176
2023-04-09 09:08:09,948 - INFO - training.closure - iteration 863: loss = -12.207522477770908
2023-04-09 09:08:34,459 - INFO - training.closure - iteration 864: loss = -12.207612272162034
2023-04-09 09:08:59,032 - INFO - training.closure - iteration 865: loss = -12.207752670811075
2023-04-09 09:09:23,625 - INFO - training.closure - iteration 866: loss = -12.205903577682772
2023-04-09 09:09:48,038 - INFO - training.closure - iteration 867: loss = -12.20777701537932
2023-04-09 09:10:13,013 - INFO - training.closure - iteration 868: loss = -12.207878326076766
2023-04-09 09:10:37,678 - INFO - training.closure - iteration 869: loss = -12.20795907036506
2023-04-09 09:11:02,020 - INFO - training.closure - iteration 870: loss = -12.207884460852043
2023-04-09 09:11:26,671 - INFO - training.closure - iteration 871: loss = -12.207995111177745
2023-04-09 09:11:51,005 - INFO - training.closure - iteration 872: loss = -12.208049856937137
2023-04-09 09:12:15,511 - INFO - training.closure - iteration 873: loss = -12.20808834574423
2023-04-09 09:12:40,051 - INFO - training.closure - iteration 874: loss = -12.208151745191515
2023-04-09 09:13:04,585 - INFO - training.closure - iteration 875: loss = -12.208224448299582
2023-04-09 09:13:29,098 - INFO - training.closure - iteration 876: loss = -12.208271833288876
2023-04-09 09:13:53,543 - INFO - training.closure - iteration 877: loss = -12.20832037080647
2023-04-09 09:14:17,948 - INFO - training.closure - iteration 878: loss = -12.208359504467587
2023-04-09 09:14:42,484 - INFO - training.closure - iteration 879: loss = -12.208516531962868
2023-04-09 09:15:07,237 - INFO - training.closure - iteration 880: loss = -12.20860337517529
2023-04-09 09:15:31,614 - INFO - training.closure - iteration 881: loss = -12.20886340677734
2023-04-09 09:15:56,192 - INFO - training.closure - iteration 882: loss = -12.209289918803089
2023-04-09 09:16:20,885 - INFO - training.closure - iteration 883: loss = -12.209607717188353
2023-04-09 09:16:45,556 - INFO - training.closure - iteration 884: loss = -12.209870541547302
2023-04-09 09:17:10,134 - INFO - training.closure - iteration 885: loss = -12.209693908086727
2023-04-09 09:17:35,105 - INFO - training.closure - iteration 886: loss = -12.21009812838781
2023-04-09 09:17:59,667 - INFO - training.closure - iteration 887: loss = -12.210479593648543
2023-04-09 09:18:24,246 - INFO - training.closure - iteration 888: loss = -12.211222211001242
2023-04-09 09:18:48,839 - INFO - training.closure - iteration 889: loss = -12.21031362602611
2023-04-09 09:19:13,408 - INFO - training.closure - iteration 890: loss = -12.211375236593279
2023-04-09 09:19:37,964 - INFO - training.closure - iteration 891: loss = -12.211586595760295
2023-04-09 09:20:02,517 - INFO - training.closure - iteration 892: loss = -12.211686156587376
2023-04-09 09:20:27,767 - INFO - training.closure - iteration 893: loss = -12.211775701491025
2023-04-09 09:20:52,829 - INFO - training.closure - iteration 894: loss = -12.211788503292553
2023-04-09 09:21:17,170 - INFO - training.closure - iteration 895: loss = -12.21183046901131
2023-04-09 09:21:41,663 - INFO - training.closure - iteration 896: loss = -12.211904709454302
2023-04-09 09:22:06,194 - INFO - training.closure - iteration 897: loss = -12.21203284866058
2023-04-09 09:22:30,880 - INFO - training.closure - iteration 898: loss = -12.212201072913837
2023-04-09 09:22:55,593 - INFO - training.closure - iteration 899: loss = -12.212133052275947
2023-04-09 09:23:20,095 - INFO - training.closure - iteration 900: loss = -12.212268859388903
2023-04-09 09:23:44,454 - INFO - training.closure - iteration 901: loss = -12.21238250806997
2023-04-09 09:24:08,970 - INFO - training.closure - iteration 902: loss = -12.21246089422959
2023-04-09 09:24:33,250 - INFO - training.closure - iteration 903: loss = -12.212515014973455
2023-04-09 09:24:57,680 - INFO - training.closure - iteration 904: loss = -12.21268839362397
2023-04-09 09:25:22,055 - INFO - training.closure - iteration 905: loss = -12.212877456772103
2023-04-09 09:25:46,328 - INFO - training.closure - iteration 906: loss = -12.213061108107858
2023-04-09 09:26:10,942 - INFO - training.closure - iteration 907: loss = -12.213467507441965
2023-04-09 09:26:35,523 - INFO - training.closure - iteration 908: loss = -12.213625579225546
2023-04-09 09:26:59,924 - INFO - training.closure - iteration 909: loss = -12.213810067816407
2023-04-09 09:27:24,594 - INFO - training.closure - iteration 910: loss = -12.213832570452773
2023-04-09 09:27:49,128 - INFO - training.closure - iteration 911: loss = -12.213970148379396
2023-04-09 09:28:13,936 - INFO - training.closure - iteration 912: loss = -12.214046726198784
2023-04-09 09:28:38,767 - INFO - training.closure - iteration 913: loss = -12.21416422174951
2023-04-09 09:29:03,439 - INFO - training.closure - iteration 914: loss = -12.21427658393821
2023-04-09 09:29:27,783 - INFO - training.closure - iteration 915: loss = -12.2143022490716
2023-04-09 09:29:52,423 - INFO - training.closure - iteration 916: loss = -12.21448047550765
2023-04-09 09:30:17,081 - INFO - training.closure - iteration 917: loss = -12.214533675778464
2023-04-09 09:30:41,615 - INFO - training.closure - iteration 918: loss = -12.214581417664247
2023-04-09 09:31:06,452 - INFO - training.closure - iteration 919: loss = -12.214608491163016
2023-04-09 09:31:30,840 - INFO - training.closure - iteration 920: loss = -12.214658340922076
2023-04-09 09:31:55,313 - INFO - training.closure - iteration 921: loss = -12.214722706088946
2023-04-09 09:32:19,922 - INFO - training.closure - iteration 922: loss = -12.214774921610669
2023-04-09 09:32:44,860 - INFO - training.closure - iteration 923: loss = -12.214812551308288
2023-04-09 09:33:09,446 - INFO - training.closure - iteration 924: loss = -12.214862994069522
2023-04-09 09:33:33,873 - INFO - training.closure - iteration 925: loss = -12.214914456027483
2023-04-09 09:33:58,566 - INFO - training.closure - iteration 926: loss = -12.215069377410614
2023-04-09 09:34:23,426 - INFO - training.closure - iteration 927: loss = -12.215226864680496
2023-04-09 09:34:47,843 - INFO - training.closure - iteration 928: loss = -12.215359733196763
2023-04-09 09:35:12,369 - INFO - training.closure - iteration 929: loss = -12.215652878825008
2023-04-09 09:35:37,027 - INFO - training.closure - iteration 930: loss = -12.215874875270924
2023-04-09 09:36:01,629 - INFO - training.closure - iteration 931: loss = -12.216294916080919
2023-04-09 09:36:26,118 - INFO - training.closure - iteration 932: loss = -12.217546746337776
2023-04-09 09:36:50,624 - INFO - training.closure - iteration 933: loss = -12.217861359616194
2023-04-09 09:37:15,327 - INFO - training.closure - iteration 934: loss = -12.21782774730691
2023-04-09 09:37:39,911 - INFO - training.closure - iteration 935: loss = -12.218021332504193
2023-04-09 09:38:04,608 - INFO - training.closure - iteration 936: loss = -12.218156830832829
2023-04-09 09:38:29,382 - INFO - training.closure - iteration 937: loss = -12.218399445270933
2023-04-09 09:38:53,994 - INFO - training.closure - iteration 938: loss = -12.213240859480596
2023-04-09 09:39:18,933 - INFO - training.closure - iteration 939: loss = -12.218472015631251
2023-04-09 09:39:43,664 - INFO - training.closure - iteration 940: loss = -12.218799313649075
2023-04-09 09:40:08,284 - INFO - training.closure - iteration 941: loss = -12.219193969761797
2023-04-09 09:40:32,880 - INFO - training.closure - iteration 942: loss = -12.219087413746328
2023-04-09 09:40:57,594 - INFO - training.closure - iteration 943: loss = -12.219370150756598
2023-04-09 09:41:22,206 - INFO - training.closure - iteration 944: loss = -12.219697235494337
2023-04-09 09:41:47,094 - INFO - training.closure - iteration 945: loss = -12.220168248837194
2023-04-09 09:42:11,505 - INFO - training.closure - iteration 946: loss = -12.220618023438593
2023-04-09 09:42:35,961 - INFO - training.closure - iteration 947: loss = -12.215216306810003
2023-04-09 09:43:00,516 - INFO - training.closure - iteration 948: loss = -12.220710398985808
2023-04-09 09:43:25,056 - INFO - training.closure - iteration 949: loss = -12.220885768898249
2023-04-09 09:43:49,648 - INFO - training.closure - iteration 950: loss = -12.22101397451047
2023-04-09 09:44:14,301 - INFO - training.closure - iteration 951: loss = -12.221135864476622
2023-04-09 09:44:38,871 - INFO - training.closure - iteration 952: loss = -12.221435589428477
2023-04-09 09:45:03,519 - INFO - training.closure - iteration 953: loss = -12.221906898405619
2023-04-09 09:45:27,944 - INFO - training.closure - iteration 954: loss = -12.200014957640441
2023-04-09 09:45:52,411 - INFO - training.closure - iteration 955: loss = -12.222061472491358
2023-04-09 09:46:16,721 - INFO - training.closure - iteration 956: loss = -12.222585409750067
2023-04-09 09:46:40,919 - INFO - training.closure - iteration 957: loss = -12.223405974779036
2023-04-09 09:47:05,487 - INFO - training.closure - iteration 958: loss = -12.224219831376285
2023-04-09 09:47:30,476 - INFO - training.closure - iteration 959: loss = -12.224862423353125
2023-04-09 09:47:55,008 - INFO - training.closure - iteration 960: loss = -12.225205082850628
2023-04-09 09:48:19,476 - INFO - training.closure - iteration 961: loss = -12.225558327979
2023-04-09 09:48:44,115 - INFO - training.closure - iteration 962: loss = -12.225764575457536
2023-04-09 09:49:08,788 - INFO - training.closure - iteration 963: loss = -12.225809260687623
2023-04-09 09:49:33,407 - INFO - training.closure - iteration 964: loss = -12.225916525821615
2023-04-09 09:49:59,277 - INFO - training.closure - iteration 965: loss = -12.22562868675659
2023-04-09 09:50:24,166 - INFO - training.closure - iteration 966: loss = -12.226023770912725
2023-04-09 09:50:48,735 - INFO - training.closure - iteration 967: loss = -12.225465894766858
2023-04-09 09:51:13,544 - INFO - training.closure - iteration 968: loss = -12.226133058077874
2023-04-09 09:51:37,985 - INFO - training.closure - iteration 969: loss = -12.226395799711838
2023-04-09 09:52:03,178 - INFO - training.closure - iteration 970: loss = -12.227402703596084
2023-04-09 09:52:28,192 - INFO - training.closure - iteration 971: loss = -12.227964047842155
2023-04-09 09:52:52,755 - INFO - training.closure - iteration 972: loss = -12.22764157613165
2023-04-09 09:53:17,228 - INFO - training.closure - iteration 973: loss = -12.228253165349473
2023-04-09 09:53:41,720 - INFO - training.closure - iteration 974: loss = -12.228409520032017
2023-04-09 09:54:06,719 - INFO - training.closure - iteration 975: loss = -12.228488646581141
2023-04-09 09:54:31,177 - INFO - training.closure - iteration 976: loss = -12.228585037415266
2023-04-09 09:54:57,243 - INFO - training.closure - iteration 977: loss = -12.228669373257567
2023-04-09 09:55:24,739 - INFO - training.closure - iteration 978: loss = -12.228888530588394
2023-04-09 09:55:48,925 - INFO - training.closure - iteration 979: loss = -12.229074806663828
2023-04-09 09:56:13,811 - INFO - training.closure - iteration 980: loss = -12.229272402281207
2023-04-09 09:56:38,254 - INFO - training.closure - iteration 981: loss = -12.22736172880765
2023-04-09 09:57:02,928 - INFO - training.closure - iteration 982: loss = -12.22946317331811
2023-04-09 09:57:27,088 - INFO - training.closure - iteration 983: loss = -12.229825362801868
2023-04-09 09:57:51,491 - INFO - training.closure - iteration 984: loss = -12.230962367624857
2023-04-09 09:58:15,911 - INFO - training.closure - iteration 985: loss = -12.231384612473715
2023-04-09 09:58:40,101 - INFO - training.closure - iteration 986: loss = -12.231674144178086
2023-04-09 09:59:04,476 - INFO - training.closure - iteration 987: loss = -12.231735858288564
2023-04-09 09:59:28,958 - INFO - training.closure - iteration 988: loss = -12.231844656267484
2023-04-09 09:59:53,414 - INFO - training.closure - iteration 989: loss = -12.231906920198025
2023-04-09 10:00:17,891 - INFO - training.closure - iteration 990: loss = -12.232027244026717
2023-04-09 10:00:42,207 - INFO - training.closure - iteration 991: loss = -12.23219884081993
2023-04-09 10:01:06,488 - INFO - training.closure - iteration 992: loss = -12.232575058457842
2023-04-09 10:01:30,969 - INFO - training.closure - iteration 993: loss = -12.22364644354697
2023-04-09 10:01:55,233 - INFO - training.closure - iteration 994: loss = -12.232672204550418
2023-04-09 10:02:19,694 - INFO - training.closure - iteration 995: loss = -12.233150468024128
2023-04-09 10:02:44,550 - INFO - training.closure - iteration 996: loss = -12.23358254459548
2023-04-09 10:03:08,806 - INFO - training.closure - iteration 997: loss = -12.233846388813843
2023-04-09 10:03:33,287 - INFO - training.closure - iteration 998: loss = -12.234048622040003
2023-04-09 10:03:57,556 - INFO - training.closure - iteration 999: loss = -12.233966417297907
2023-04-09 10:04:21,888 - INFO - training.closure - iteration 1000: loss = -12.234177017563486
2023-04-09 10:04:46,632 - INFO - training.closure - iteration 1001: loss = -12.234513397679352
2023-04-09 10:05:10,735 - INFO - training.closure - iteration 1002: loss = -12.235352244932539
2023-04-09 10:05:35,296 - INFO - training.closure - iteration 1003: loss = -12.236334192233157
2023-04-09 10:05:59,846 - INFO - training.closure - iteration 1004: loss = -12.237327200657862
2023-04-09 10:06:24,078 - INFO - training.closure - iteration 1005: loss = -12.237773324747355
2023-04-09 10:06:48,678 - INFO - training.closure - iteration 1006: loss = -12.238228714376476
2023-04-09 10:07:13,070 - INFO - training.closure - iteration 1007: loss = -12.23855582267474
2023-04-09 10:07:37,294 - INFO - training.closure - iteration 1008: loss = -12.239246135703748
2023-04-09 10:08:01,677 - INFO - training.closure - iteration 1009: loss = -12.239782394565987
2023-04-09 10:08:26,338 - INFO - training.closure - iteration 1010: loss = -12.235117277105878
2023-04-09 10:08:50,451 - INFO - training.closure - iteration 1011: loss = -12.239998629456874
2023-04-09 10:09:14,739 - INFO - training.closure - iteration 1012: loss = -12.240303120066622
2023-04-09 10:09:38,887 - INFO - training.closure - iteration 1013: loss = -12.240516932904763
2023-04-09 10:10:03,185 - INFO - training.closure - iteration 1014: loss = -12.240640709534286
2023-04-09 10:10:27,455 - INFO - training.closure - iteration 1015: loss = -12.240915049514154
2023-04-09 10:10:51,695 - INFO - training.closure - iteration 1016: loss = -12.241198376213013
2023-04-09 10:11:16,016 - INFO - training.closure - iteration 1017: loss = -12.241551476916666
2023-04-09 10:11:40,335 - INFO - training.closure - iteration 1018: loss = -12.24181113165589
2023-04-09 10:12:04,604 - INFO - training.closure - iteration 1019: loss = -12.241923068322443
2023-04-09 10:12:28,998 - INFO - training.closure - iteration 1020: loss = -12.242120770323147
2023-04-09 10:12:53,163 - INFO - training.closure - iteration 1021: loss = -12.24216349731197
2023-04-09 10:13:17,360 - INFO - training.closure - iteration 1022: loss = -12.242253965178897
2023-04-09 10:13:42,015 - INFO - training.closure - iteration 1023: loss = -12.242321431956517
2023-04-09 10:14:06,471 - INFO - training.closure - iteration 1024: loss = -12.24241816889824
2023-04-09 10:14:30,794 - INFO - training.closure - iteration 1025: loss = -12.242568501895231
2023-04-09 10:14:55,197 - INFO - training.closure - iteration 1026: loss = -12.242820490238248
2023-04-09 10:15:19,467 - INFO - training.closure - iteration 1027: loss = -12.24296544860007
2023-04-09 10:15:43,825 - INFO - training.closure - iteration 1028: loss = -12.243165281675196
2023-04-09 10:16:08,723 - INFO - training.closure - iteration 1029: loss = -12.243218031415442
2023-04-09 10:16:33,274 - INFO - training.closure - iteration 1030: loss = -12.243261860814478
2023-04-09 10:16:57,947 - INFO - training.closure - iteration 1031: loss = -12.243306207093578
2023-04-09 10:17:22,304 - INFO - training.closure - iteration 1032: loss = -12.243369834435459
2023-04-09 10:17:47,186 - INFO - training.closure - iteration 1033: loss = -12.243394592163753
2023-04-09 10:18:11,660 - INFO - training.closure - iteration 1034: loss = -12.243452380029037
2023-04-09 10:18:36,172 - INFO - training.closure - iteration 1035: loss = -12.24349874353154
2023-04-09 10:19:00,632 - INFO - training.closure - iteration 1036: loss = -12.243625517033742
2023-04-09 10:19:24,898 - INFO - training.closure - iteration 1037: loss = -12.243832803556309
2023-04-09 10:19:49,148 - INFO - training.closure - iteration 1038: loss = -12.244358187934878
2023-04-09 10:20:13,425 - INFO - training.closure - iteration 1039: loss = -12.245187107482508
2023-04-09 10:20:38,044 - INFO - training.closure - iteration 1040: loss = -12.24274355721861
2023-04-09 10:21:02,508 - INFO - training.closure - iteration 1041: loss = -12.24561167275249
2023-04-09 10:21:27,424 - INFO - training.closure - iteration 1042: loss = -12.246316331611107
2023-04-09 10:21:51,920 - INFO - training.closure - iteration 1043: loss = -12.246803374997729
2023-04-09 10:22:16,202 - INFO - training.closure - iteration 1044: loss = -12.247081175427677
2023-04-09 10:22:40,652 - INFO - training.closure - iteration 1045: loss = -12.247628370396242
2023-04-09 10:23:05,034 - INFO - training.closure - iteration 1046: loss = -12.248002585520961
2023-04-09 10:23:29,350 - INFO - training.closure - iteration 1047: loss = -12.248426379078046
2023-04-09 10:23:53,867 - INFO - training.closure - iteration 1048: loss = -12.24871738558433
2023-04-09 10:24:18,131 - INFO - training.closure - iteration 1049: loss = -12.248984329033203
2023-04-09 10:24:42,386 - INFO - training.closure - iteration 1050: loss = -12.249182528328411
2023-04-09 10:25:06,848 - INFO - training.closure - iteration 1051: loss = -12.249350958817004
2023-04-09 10:25:31,418 - INFO - training.closure - iteration 1052: loss = -12.249494313852004
2023-04-09 10:25:56,123 - INFO - training.closure - iteration 1053: loss = -12.24960899586539
2023-04-09 10:26:21,396 - INFO - training.closure - iteration 1054: loss = -12.249715021138506
2023-04-09 10:26:45,736 - INFO - training.closure - iteration 1055: loss = -12.24976229611497
2023-04-09 10:27:10,244 - INFO - training.closure - iteration 1056: loss = -12.250140692825486
2023-04-09 10:27:34,530 - INFO - training.closure - iteration 1057: loss = -12.249968980964375
2023-04-09 10:27:58,895 - INFO - training.closure - iteration 1058: loss = -12.250293563784588
2023-04-09 10:28:23,216 - INFO - training.closure - iteration 1059: loss = -12.25057339631205
2023-04-09 10:28:47,560 - INFO - training.closure - iteration 1060: loss = -12.251352599870028
2023-04-09 10:29:11,972 - INFO - training.closure - iteration 1061: loss = -12.251642832424647
2023-04-09 10:29:36,425 - INFO - training.closure - iteration 1062: loss = -12.252279402987622
2023-04-09 10:30:00,929 - INFO - training.closure - iteration 1063: loss = -12.25220346056235
2023-04-09 10:30:25,211 - INFO - training.closure - iteration 1064: loss = -12.252448543447306
2023-04-09 10:30:51,557 - INFO - training.closure - iteration 1065: loss = -12.252753540659107
2023-04-09 10:31:17,197 - INFO - training.closure - iteration 1066: loss = -12.253134054716298
2023-04-09 10:31:41,430 - INFO - training.closure - iteration 1067: loss = -12.253541206561858
2023-04-09 10:32:05,957 - INFO - training.closure - iteration 1068: loss = -12.254007919795448
2023-04-09 10:32:30,203 - INFO - training.closure - iteration 1069: loss = -12.254321876160663
2023-04-09 10:32:54,925 - INFO - training.closure - iteration 1070: loss = -12.254641936705314
2023-04-09 10:33:19,411 - INFO - training.closure - iteration 1071: loss = -12.25472966816319
2023-04-09 10:33:43,808 - INFO - training.closure - iteration 1072: loss = -12.254829698547198
2023-04-09 10:34:08,136 - INFO - training.closure - iteration 1073: loss = -12.255121533967806
2023-04-09 10:34:32,464 - INFO - training.closure - iteration 1074: loss = -12.255448534201573
2023-04-09 10:34:56,786 - INFO - training.closure - iteration 1075: loss = -12.255081903036242
2023-04-09 10:35:21,066 - INFO - training.closure - iteration 1076: loss = -12.255697976717471
2023-04-09 10:35:45,487 - INFO - training.closure - iteration 1077: loss = -12.25621811267594
2023-04-09 10:36:10,236 - INFO - training.closure - iteration 1078: loss = -12.256685892280931
2023-04-09 10:36:34,877 - INFO - training.closure - iteration 1079: loss = -12.256871383814762
2023-04-09 10:36:59,318 - INFO - training.closure - iteration 1080: loss = -12.257048093450354
2023-04-09 10:37:23,656 - INFO - training.closure - iteration 1081: loss = -12.254567637690808
2023-04-09 10:37:48,119 - INFO - training.closure - iteration 1082: loss = -12.257070383506765
2023-04-09 10:38:12,806 - INFO - training.closure - iteration 1083: loss = -12.257179579658885
2023-04-09 10:38:37,087 - INFO - training.closure - iteration 1084: loss = -12.25722922738321
2023-04-09 10:39:01,230 - INFO - training.closure - iteration 1085: loss = -12.257263423577928
2023-04-09 10:39:25,266 - INFO - training.closure - iteration 1086: loss = -12.257301636221406
2023-04-09 10:39:49,666 - INFO - training.closure - iteration 1087: loss = -12.2573707300641
2023-04-09 10:40:13,893 - INFO - training.closure - iteration 1088: loss = -12.257427815030724
2023-04-09 10:40:38,466 - INFO - training.closure - iteration 1089: loss = -12.257585154216011
2023-04-09 10:41:03,064 - INFO - training.closure - iteration 1090: loss = -12.257693451765807
2023-04-09 10:41:27,273 - INFO - training.closure - iteration 1091: loss = -12.257742814987694
2023-04-09 10:41:51,859 - INFO - training.closure - iteration 1092: loss = -12.257602192938466
2023-04-09 10:42:16,221 - INFO - training.closure - iteration 1093: loss = -12.257762896366046
2023-04-09 10:42:40,503 - INFO - training.closure - iteration 1094: loss = -12.257792363306997
2023-04-09 10:43:04,948 - INFO - training.closure - iteration 1095: loss = -12.25781128855506
2023-04-09 10:43:29,265 - INFO - training.closure - iteration 1096: loss = -12.257829108646906
2023-04-09 10:43:53,854 - INFO - training.closure - iteration 1097: loss = -12.257884004811372
2023-04-09 10:44:18,113 - INFO - training.closure - iteration 1098: loss = -12.257931894497624
2023-04-09 10:44:42,501 - INFO - training.closure - iteration 1099: loss = -12.25797618987256
2023-04-09 10:45:06,739 - INFO - training.closure - iteration 1100: loss = -12.257949571852468
2023-04-09 10:45:31,056 - INFO - training.closure - iteration 1101: loss = -12.257995998334042
2023-04-09 10:45:55,393 - INFO - training.closure - iteration 1102: loss = -12.258015755122747
2023-04-09 10:46:19,800 - INFO - training.closure - iteration 1103: loss = -12.258135259640143
2023-04-09 10:46:44,144 - INFO - training.closure - iteration 1104: loss = -12.258226682308631
2023-04-09 10:47:08,746 - INFO - training.closure - iteration 1105: loss = -12.25840379931146
2023-04-09 10:47:33,047 - INFO - training.closure - iteration 1106: loss = -12.258525604698011
2023-04-09 10:47:58,216 - INFO - training.closure - iteration 1107: loss = -12.258625389183418
2023-04-09 10:48:22,670 - INFO - training.closure - iteration 1108: loss = -12.258658387413616
2023-04-09 10:48:47,126 - INFO - training.closure - iteration 1109: loss = -12.25868209118985
2023-04-09 10:49:11,583 - INFO - training.closure - iteration 1110: loss = -12.258689815804452
2023-04-09 10:49:35,864 - INFO - training.closure - iteration 1111: loss = -12.258704588245207
2023-04-09 10:50:00,288 - INFO - training.closure - iteration 1112: loss = -12.258739718854992
2023-04-09 10:50:24,958 - INFO - training.closure - iteration 1113: loss = -12.25825974136903
2023-04-09 10:50:49,649 - INFO - training.closure - iteration 1114: loss = -12.2587541233526
2023-04-09 10:51:14,130 - INFO - training.closure - iteration 1115: loss = -12.258790268241258
2023-04-09 10:51:38,505 - INFO - training.closure - iteration 1116: loss = -12.258777335467727
2023-04-09 10:52:02,763 - INFO - training.closure - iteration 1117: loss = -12.258815416772915
2023-04-09 10:52:27,580 - INFO - training.closure - iteration 1118: loss = -12.258830700911739
2023-04-09 10:52:51,782 - INFO - training.closure - iteration 1119: loss = -12.258860785241014
2023-04-09 10:53:16,131 - INFO - training.closure - iteration 1120: loss = -12.258868773963663
2023-04-09 10:53:40,388 - INFO - training.closure - iteration 1121: loss = -12.258881487859655
2023-04-09 10:53:59,240 - INFO - main.experiment - train - RMSE_b at iteration 0 = 0.09468121343497633
2023-04-09 10:53:59,241 - INFO - main.experiment - train - RMSE_b at last iteration = 0.05652221039222123
2023-04-09 10:53:59,241 - INFO - main.experiment - train - RMSE_a at iteration 0 = 1.4721875659414054
2023-04-09 10:53:59,241 - INFO - main.experiment - train - RMSE_a at last iteration = 0.04866308605030585
2023-04-09 10:53:59,241 - INFO - main.experiment - train - LOGPDF_b at iteration 0 = 86.06819679335625
2023-04-09 10:53:59,241 - INFO - main.experiment - train - LOGPDF_b at last iteration = -3.059458989160188
2023-04-09 10:53:59,241 - INFO - main.experiment - train - LOGPDF_a at iteration 0 = 102112.04702931983
2023-04-09 10:53:59,241 - INFO - main.experiment - train - LOGPDF_a at last iteration = -3.239381896399963
2023-04-09 10:53:59,241 - INFO - main.experiment - train - LOSS at iteration 0 = 102198.1152261132
2023-04-09 10:53:59,241 - INFO - main.experiment - train - LOSS at last iteration = -6.298840885560152
2023-04-09 10:54:00,828 - INFO - main.experiment - test - RMSE_b at iteration 0 = 0.09394814064145388
2023-04-09 10:54:00,828 - INFO - main.experiment - test - RMSE_b at last iteration = 0.0777174509552564
2023-04-09 10:54:00,828 - INFO - main.experiment - test - RMSE_a at iteration 0 = 0.03893450817377227
2023-04-09 10:54:00,828 - INFO - main.experiment - test - RMSE_a at last iteration = 0.0685910543947485
2023-04-09 10:54:00,828 - INFO - main.experiment - test - LOGPDF_b at iteration 0 = 13.894646843948609
2023-04-09 10:54:00,828 - INFO - main.experiment - test - LOGPDF_b at last iteration = 580.6541520497076
2023-04-09 10:54:00,828 - INFO - main.experiment - test - LOGPDF_a at iteration 0 = -3.9276934794504923
2023-04-09 10:54:00,828 - INFO - main.experiment - test - LOGPDF_a at last iteration = 5302.535997491718
2023-04-09 10:54:00,828 - INFO - main.experiment - test - LOSS at iteration 0 = 9.966953364498117
2023-04-09 10:54:00,829 - INFO - main.experiment - test - LOSS at last iteration = 5883.190149541426